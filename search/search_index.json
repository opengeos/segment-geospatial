{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to samgeo","text":"<p>A Python package for segmenting geospatial data with the Segment Anything Model (SAM) \ud83d\uddfa\ufe0f</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The segment-geospatial package draws its inspiration from segment-anything-eo repository authored by Aliaksandr Hancharenka. To facilitate the use of the Segment Anything Model (SAM) for geospatial data, I have developed the segment-anything-py and segment-geospatial Python packages, which are now available on PyPI and conda-forge. My primary objective is to simplify the process of leveraging SAM for geospatial data analysis by enabling users to achieve this with minimal coding effort. I have adapted the source code of segment-geospatial from the segment-anything-eo repository, and credit for its original version goes to Aliaksandr Hancharenka.</p> <ul> <li>\ud83c\udd93 Free software: MIT license</li> <li>\ud83d\udcd6 Documentation: https://samgeo.gishub.org</li> </ul>"},{"location":"#citations","title":"Citations","text":"<ul> <li>Wu, Q., &amp; Osco, L. (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). Journal of Open Source Software, 8(89), 5663. https://doi.org/10.21105/joss.05663</li> <li>Osco, L. P., Wu, Q., de Lemos, E. L., Gon\u00e7alves, W. N., Ramos, A. P. M., Li, J., &amp; Junior, J. M. (2023). The Segment Anything Model (SAM) for remote sensing applications: From zero to one shot. International Journal of Applied Earth Observation and Geoinformation, 124, 103540. https://doi.org/10.1016/j.jag.2023.103540</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Download map tiles from Tile Map Service (TMS) servers and create GeoTIFF files</li> <li>Segment GeoTIFF files using the Segment Anything Model (SAM) and HQ-SAM</li> <li>Segment remote sensing imagery with text prompts</li> <li>Create foreground and background markers interactively</li> <li>Load existing markers from vector datasets</li> <li>Save segmentation results as common vector formats (GeoPackage, Shapefile, GeoJSON)</li> <li>Save input prompts as GeoJSON files</li> <li>Visualize segmentation results on interactive maps</li> <li>Segment objects from timeseries remote sensing imagery</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Segmenting remote sensing imagery</li> <li>Automatically generating object masks</li> <li>Segmenting remote sensing imagery with input prompts</li> <li>Segmenting remote sensing imagery with box prompts</li> <li>Segmenting remote sensing imagery with text prompts</li> <li>Batch segmentation with text prompts</li> <li>Using segment-geospatial with ArcGIS Pro</li> <li>Segmenting swimming pools with text prompts</li> <li>Segmenting satellite imagery from the Maxar Open Data Program</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Automatic mask generator</li> </ul> <ul> <li>Interactive segmentation with input prompts</li> </ul> <ul> <li>Input prompts from existing files</li> </ul> <ul> <li>Interactive segmentation with text prompts</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Video tutorials are available on my YouTube Channel.</p> <ul> <li>Automatic mask generation</li> </ul> <p></p> <ul> <li>Using SAM with ArcGIS Pro</li> </ul> <p></p> <ul> <li>Interactive segmentation with text prompts</li> </ul> <p></p>"},{"location":"#using-sam-with-desktop-gis","title":"Using SAM with Desktop GIS","text":"<ul> <li>QGIS: Check out the Geometric Attributes plugin for QGIS. Credit goes to Bjorn Nyberg.</li> <li>ArcGIS: Check out the Segment Anything Model (SAM) Toolbox for ArcGIS and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</li> </ul>"},{"location":"#computing-resources","title":"Computing Resources","text":"<p>The Segment Anything Model is computationally intensive, and a powerful GPU is recommended to process large datasets. It is recommended to have a GPU with at least 8 GB of GPU memory. You can utilize the free GPU resources provided by Google Colab. Alternatively, you can apply for AWS Cloud Credit for Research, which offers cloud credits to support academic research. If you are in the Greater China region, apply for the AWS Cloud Credit here.</p>"},{"location":"#legal-notice","title":"Legal Notice","text":"<p>This repository and its content are provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is based upon work partially supported by the National Aeronautics and Space Administration (NASA) under Grant No. 80NSSC22K1742 issued through the Open Source Tools, Frameworks, and Libraries 2020 Program.</p> <p>This project is also supported by Amazon Web Services (AWS). In addition, this package was made possible by the following open source projects. Credit goes to the developers of these projects.</p> <ul> <li>segment-anything \ud83d\udcbb</li> <li>segment-anything-eo \ud83d\udef0\ufe0f</li> <li>tms2geotiff \ud83d\udcf7</li> <li>GroundingDINO \ud83e\udd96</li> <li>lang-segment-anything \ud83d\udcdd</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0102-nov-7-2023","title":"v0.10.2 - Nov 7, 2023","text":"<p>What's Changed</p> <ul> <li>Add JOSS paper by @giswqs in #197</li> <li>Add notebook for using Maxar Open Data by @giswqs in #198</li> <li>Add checkpoint to textsam.LangSAM() by @forestbat in #204</li> <li>Add workshop notebook by @giswqs in #209</li> </ul> <p>New Contributors</p> <ul> <li>@forestbat made their first contribution in #204</li> </ul> <p>Full Changelog: v0.10.1...v0.10.2</p>"},{"location":"changelog/#v0101-sep-1-2023","title":"v0.10.1 - Sep 1, 2023","text":"<p>What's Changed</p> <ul> <li>Fix basemap issue by @giswqs in #190</li> </ul> <p>Full Changelog: v0.10.0...v0.10.1)</p>"},{"location":"changelog/#v0100-aug-24-2023","title":"v0.10.0 - Aug 24, 2023","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>Added fastsam module by @giswqs #167</li> <li>Update optional dependencies by @giswqs in #68</li> <li>Improve contributing guidelines by @giswqs in #169</li> <li>[FIX] Added missing conversions from BGR to RGB by @lbferreira in #171</li> <li>Address JOSS review comments by @giswqs in #175</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<ul> <li>@lbferreira made their first contribution in #171</li> </ul>"},{"location":"changelog/#v091-aug-14-2023","title":"v0.9.1 - Aug 14, 2023","text":"<p>New Features</p> <ul> <li>Added support for HQ-SAM (#161)</li> <li>Added HQ-SAM notebooks (#162)</li> </ul>"},{"location":"changelog/#v090-aug-6-2023","title":"v0.9.0 - Aug 6, 2023","text":"<p>New Features</p> <ul> <li>Added support for multiple input boxes (#159)</li> </ul> <p>Improvements</p> <ul> <li>UpdateD groundingdino installation (#147)</li> <li>Updated README (#152)</li> </ul>"},{"location":"changelog/#v085-jul-19-2023","title":"v0.8.5 - Jul 19, 2023","text":"<p>Improvements</p> <ul> <li>Updated installation docs (#146)</li> <li>Updated leafmap and localtileserver to dependencies (#146)</li> <li>Added info about libgl1 dependency install on Linux systems (#141)</li> <li>Fixed save_masks bug without source image (#139)</li> </ul>"},{"location":"changelog/#v084-jul-5-2023","title":"v0.8.4 - Jul 5, 2023","text":"<p>Improvements</p> <ul> <li>Fixed model download bug (#136)</li> <li>Added legal notice (#133)</li> <li>Fixed image source bug for show_anns (#131)</li> <li>Improved exception handling for LangSAM GUI (#130)</li> <li>Added to return pixel coordinates of masks (#129)</li> <li>Added text_sam to docs (#123)</li> <li>Fixed file deletion error on Windows (#122)</li> <li>Fixed mask bug in text_sam/predict when the input is PIL image (#117)</li> </ul>"},{"location":"changelog/#v083-jun-20-2023","title":"v0.8.3 - Jun 20, 2023","text":"<p>New Features</p> <ul> <li>Added support for batch segmentation (#116)</li> <li>Added swimming pools example (#106)</li> </ul> <p>Improvements</p> <ul> <li>Removed 'flag' and 'param' arguments (#112)</li> <li>Used sorted function instead of if statements (#109)</li> </ul>"},{"location":"changelog/#v082-jun-14-2023","title":"v0.8.2 - Jun 14, 2023","text":"<p>New Features</p> <ul> <li>Added regularized option for vector output (#104)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Added more deep learning resources (#90)</li> <li>Use the force_filename parameter with hf_hub_download() (#93)</li> <li>Fixed typo (#94)</li> </ul>"},{"location":"changelog/#v081-may-24-2023","title":"v0.8.1 - May 24, 2023","text":"<p>Improvements</p> <ul> <li>Added huggingface_hub and remove onnx (#87)</li> <li>Added more demos to docs (#82)</li> </ul>"},{"location":"changelog/#v080-may-24-2023","title":"v0.8.0 - May 24, 2023","text":"<p>New Features</p> <ul> <li>Added support for using text prompts with SAM (#73)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Improved text prompt notebook (#79)</li> <li>Fixed notebook typos (#78)</li> <li>Added ArcGIS tutorial to docs (#72)</li> </ul>"},{"location":"changelog/#v070-may-20-2023","title":"v0.7.0 - May 20, 2023","text":"<p>New Features</p> <ul> <li>Added unittest (#58)</li> <li>Added JOSS paper draft (#61)</li> <li>Added ArcGIS notebook example (#63)</li> <li>Added text prompting segmentation (#65)</li> <li>Added support for segmenting non-georeferenced imagery (#66)</li> </ul> <p>Improvements</p> <ul> <li>Added blend option for show_anns method (#59)</li> <li>Updated ArcGIS installation instructions (#68, #70)</li> </ul> <p>Contributors</p> <p>@p-vdp @LucasOsco</p>"},{"location":"changelog/#v062-may-17-2023","title":"v0.6.2 - May 17, 2023","text":"<p>Improvements</p> <ul> <li>Added jupyter-server-proxy to Dockerfile for supporting add_raster (#57)</li> </ul>"},{"location":"changelog/#v061-may-16-2023","title":"v0.6.1 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added Dockerfile (#51)</li> </ul>"},{"location":"changelog/#v060-may-16-2023","title":"v0.6.0 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added interactive GUI for creating foreground and background markers (#44)</li> <li>Added support for custom projection bbox (#39)</li> </ul> <p>Improvements</p> <ul> <li>Fixed Colab Marker AwesomeIcon bug (#50)</li> <li>Added info about using SAM with Desktop GIS (#48)</li> <li>Use proper extension in the usage documentation (#43)</li> </ul> <p>Demos</p> <ul> <li>Interactive segmentation with input prompts</li> </ul> <p></p> <ul> <li>Input prompts from existing files</li> </ul> <p></p>"},{"location":"changelog/#v050-may-10-2023","title":"v0.5.0 - May 10, 2023","text":"<p>New Features</p> <ul> <li>Added support for input prompts (#30)</li> </ul> <p>Improvements</p> <ul> <li>Fixed the batch processing bug (#29)</li> </ul> <p>Demos</p> <p></p>"},{"location":"changelog/#v040-may-6-2023","title":"v0.4.0 - May 6, 2023","text":"<p>New Features</p> <ul> <li>Added new methods to <code>SamGeo</code> class, including <code>show_masks</code>, <code>save_masks</code>, <code>show_anns</code>, making it much easier to save segmentation results in GeoTIFF and vector formats.</li> <li>Added new functions to <code>common</code> module, including <code>array_to_image</code>, <code>show_image</code>, <code>download_file</code>, <code>overlay_images</code>, <code>blend_images</code>, and <code>update_package</code></li> <li>Added tow more notebooks, including automatic_mask_generator and satellite-predictor</li> <li>Added <code>SamGeoPredictor</code> class</li> </ul> <p>Improvements</p> <ul> <li>Improved <code>SamGeo.generate()</code> method</li> <li>Improved docstrings and API reference</li> <li>Added demos to docs</li> </ul> <p>Demos</p> <ul> <li>Automatic mask generator</li> </ul> <p></p> <p>Contributors</p> <p>@darrenwiens</p>"},{"location":"changelog/#v030-apr-26-2023","title":"v0.3.0 - Apr 26, 2023","text":"<p>New Features</p> <ul> <li>Added several new functions, including <code>get_basemaps</code>, <code>reproject</code>, <code>tiff_to_shp</code>, and <code>tiff_to_geojson</code></li> <li>Added hundereds of new basemaps through xyzservices</li> </ul> <p>Improvement</p> <ul> <li>Fixed <code>tiff_to_vector</code> crs bug #12</li> <li>Add <code>crs</code> parameter to <code>tms_to_geotiff</code></li> </ul>"},{"location":"changelog/#v020-apr-21-2023","title":"v0.2.0 - Apr 21, 2023","text":"<p>New Features</p> <ul> <li>Added notebook example</li> <li>Added <code>SamGeo.generate</code> method</li> <li>Added <code>SamGeo.tiff_to_vector</code> method</li> </ul>"},{"location":"changelog/#v010-apr-19-2023","title":"v0.1.0 - Apr 19, 2023","text":"<p>New Features</p> <ul> <li>Added <code>SamGeo</code> class</li> <li>Added GitHub Actions</li> <li>Added notebook example</li> </ul>"},{"location":"changelog/#v001-apr-18-2023","title":"v0.0.1 - Apr 18, 2023","text":"<p>Initial release</p>"},{"location":"changelog_update/","title":"Changelog update","text":"In\u00a0[\u00a0]: Copied! <pre>import re\n</pre> import re In\u00a0[\u00a0]: Copied! <pre># Copy the release notes from the GitHub release page\nmarkdown_text = \"\"\"\n## What's Changed\n* Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197\n* Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198\n* Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204\n* Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209\n\n## New Contributors\n* @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204\n\n**Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2\n\"\"\"\n</pre> # Copy the release notes from the GitHub release page markdown_text = \"\"\" ## What's Changed * Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197 * Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198 * Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204 * Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209  ## New Contributors * @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204  **Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2 \"\"\" In\u00a0[\u00a0]: Copied! <pre># Regular expression pattern to match the Markdown hyperlinks\npattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\"\n</pre> # Regular expression pattern to match the Markdown hyperlinks pattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\" In\u00a0[\u00a0]: Copied! <pre># Function to replace matched URLs with the desired format\ndef replace_url(match):\n    pr_number = match.group(1)\n    return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\"\n</pre> # Function to replace matched URLs with the desired format def replace_url(match):     pr_number = match.group(1)     return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\" In\u00a0[\u00a0]: Copied! <pre># Use re.sub to replace URLs with the desired format\nformatted_text = re.sub(pattern, replace_url, markdown_text)\n</pre> # Use re.sub to replace URLs with the desired format formatted_text = re.sub(pattern, replace_url, markdown_text) In\u00a0[\u00a0]: Copied! <pre>for line in formatted_text.splitlines():\n    if \"Full Changelog\" in line:\n        prefix = line.split(\": \")[0]\n        link = line.split(\": \")[1]\n        version = line.split(\"/\")[-1]\n        formatted_text = (\n            formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")\n            .replace(\"## What's Changed\", \"**What's Changed**\")\n            .replace(\"## New Contributors\", \"**New Contributors**\")\n        )\n</pre> for line in formatted_text.splitlines():     if \"Full Changelog\" in line:         prefix = line.split(\": \")[0]         link = line.split(\": \")[1]         version = line.split(\"/\")[-1]         formatted_text = (             formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")             .replace(\"## What's Changed\", \"**What's Changed**\")             .replace(\"## New Contributors\", \"**New Contributors**\")         ) In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/changelog_update.md\", \"w\") as f:\n    f.write(formatted_text)\n</pre> with open(\"docs/changelog_update.md\", \"w\") as f:     f.write(formatted_text) In\u00a0[\u00a0]: Copied! <pre># Print the formatted text\nprint(formatted_text)\n</pre> # Print the formatted text print(formatted_text) <p>Copy the formatted text and paste it to the CHANGELOG.md file</p>"},{"location":"common/","title":"common module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"common/#samgeo.common.array_to_image","title":"<code>array_to_image(array, output, source=None, dtype=None, compress='deflate', **kwargs)</code>","text":"<p>Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The NumPy array to be saved as a GeoTIFF.</p> required <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>source</code> <code>str</code> <p>The path to an existing GeoTIFF file with map projection information. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output array. Defaults to None.</p> <code>None</code> <code>compress</code> <code>str</code> <p>The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".</p> <code>'deflate'</code> Source code in <code>samgeo/common.py</code> <pre><code>def array_to_image(\n    array, output, source=None, dtype=None, compress=\"deflate\", **kwargs\n):\n    \"\"\"Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.\n\n    Args:\n        array (np.ndarray): The NumPy array to be saved as a GeoTIFF.\n        output (str): The path to the output image.\n        source (str, optional): The path to an existing GeoTIFF file with map projection information. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output array. Defaults to None.\n        compress (str, optional): The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".\n    \"\"\"\n\n    from PIL import Image\n\n    if isinstance(array, str) and os.path.exists(array):\n        array = cv2.imread(array)\n        array = cv2.cvtColor(array, cv2.COLOR_BGR2RGB)\n\n    if output.endswith(\".tif\"):\n        if source is not None:\n            with rasterio.open(source) as src:\n                crs = src.crs\n                transform = src.transform\n                if compress is None:\n                    compress = src.compression\n        else:\n            crs = kwargs.get(\"crs\", None)\n            transform = kwargs.get(\"transform\", None)\n\n        # Determine the minimum and maximum values in the array\n\n        min_value = np.min(array)\n        max_value = np.max(array)\n\n        if dtype is None:\n            # Determine the best dtype for the array\n            if min_value &gt;= 0 and max_value &lt;= 1:\n                dtype = np.float32\n            elif min_value &gt;= 0 and max_value &lt;= 255:\n                dtype = np.uint8\n            elif min_value &gt;= -128 and max_value &lt;= 127:\n                dtype = np.int8\n            elif min_value &gt;= 0 and max_value &lt;= 65535:\n                dtype = np.uint16\n            elif min_value &gt;= -32768 and max_value &lt;= 32767:\n                dtype = np.int16\n            else:\n                dtype = np.float64\n\n        # Convert the array to the best dtype\n        array = array.astype(dtype)\n\n        # Define the GeoTIFF metadata\n        if array.ndim == 2:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": 1,\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n        elif array.ndim == 3:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": array.shape[2],\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n\n        if compress is not None:\n            metadata[\"compress\"] = compress\n        else:\n            raise ValueError(\"Array must be 2D or 3D.\")\n\n        # Create a new GeoTIFF file and write the array to it\n        with rasterio.open(output, \"w\", **metadata) as dst:\n            if array.ndim == 2:\n                dst.write(array, 1)\n            elif array.ndim == 3:\n                for i in range(array.shape[2]):\n                    dst.write(array[:, :, i], i + 1)\n\n    else:\n        img = Image.fromarray(array)\n        img.save(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>samgeo/common.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: list, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"common/#samgeo.common.blend_images","title":"<code>blend_images(img1, img2, alpha=0.5, output=False, show=True, figsize=(12, 10), axis='off', **kwargs)</code>","text":"<p>Blends two images together using the addWeighted function from the OpenCV library.</p> <p>Parameters:</p> Name Type Description Default <code>img1</code> <code>ndarray</code> <p>The first input image on top represented as a NumPy array.</p> required <code>img2</code> <code>ndarray</code> <p>The second input image at the bottom represented as a NumPy array.</p> required <code>alpha</code> <code>float</code> <p>The weighting factor for the first image in the blend. By default, this is set to 0.5.</p> <code>0.5</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether to display the blended image. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>The axis of the figure. Defaults to \"off\".</p> <code>'off'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the cv2.addWeighted() function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>numpy.ndarray: The blended image as a NumPy array.</p> Source code in <code>samgeo/common.py</code> <pre><code>def blend_images(\n    img1,\n    img2,\n    alpha=0.5,\n    output=False,\n    show=True,\n    figsize=(12, 10),\n    axis=\"off\",\n    **kwargs,\n):\n    \"\"\"\n    Blends two images together using the addWeighted function from the OpenCV library.\n\n    Args:\n        img1 (numpy.ndarray): The first input image on top represented as a NumPy array.\n        img2 (numpy.ndarray): The second input image at the bottom represented as a NumPy array.\n        alpha (float): The weighting factor for the first image in the blend. By default, this is set to 0.5.\n        output (str, optional): The path to the output image. Defaults to False.\n        show (bool, optional): Whether to display the blended image. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (12, 10).\n        axis (str, optional): The axis of the figure. Defaults to \"off\".\n        **kwargs: Additional keyword arguments to pass to the cv2.addWeighted() function.\n\n    Returns:\n        numpy.ndarray: The blended image as a NumPy array.\n    \"\"\"\n    # Resize the images to have the same dimensions\n    if isinstance(img1, str):\n        if img1.startswith(\"http\"):\n            img1 = download_file(img1)\n\n        if not os.path.exists(img1):\n            raise ValueError(f\"Input path {img1} does not exist.\")\n\n        img1 = cv2.imread(img1)\n\n    if isinstance(img2, str):\n        if img2.startswith(\"http\"):\n            img2 = download_file(img2)\n\n        if not os.path.exists(img2):\n            raise ValueError(f\"Input path {img2} does not exist.\")\n\n        img2 = cv2.imread(img2)\n\n    if img1.dtype == np.float32:\n        img1 = (img1 * 255).astype(np.uint8)\n\n    if img2.dtype == np.float32:\n        img2 = (img2 * 255).astype(np.uint8)\n\n    if img1.dtype != img2.dtype:\n        img2 = img2.astype(img1.dtype)\n\n    img1 = cv2.resize(img1, (img2.shape[1], img2.shape[0]))\n\n    # Blend the images using the addWeighted function\n    beta = 1 - alpha\n    blend_img = cv2.addWeighted(img1, alpha, img2, beta, 0, **kwargs)\n\n    if output:\n        array_to_image(blend_img, output, img2)\n\n    if show:\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=figsize)\n        plt.imshow(blend_img)\n        plt.axis(axis)\n        plt.show()\n    else:\n        return blend_img\n</code></pre>"},{"location":"common/#samgeo.common.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <p>geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def boxes_to_vector(coords, src_crs, dst_crs=\"EPSG:4326\", output=None, **kwargs):\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"common/#samgeo.common.check_file_path","title":"<code>check_file_path(file_path, make_dirs=True)</code>","text":"<p>Gets the absolute file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file.</p> required <code>make_dirs</code> <code>bool</code> <p>Whether to create the directory if it does not exist. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory could not be found.</p> <code>TypeError</code> <p>If the input directory path is not a string.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The absolute path to the file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def check_file_path(file_path, make_dirs=True):\n    \"\"\"Gets the absolute file path.\n\n    Args:\n        file_path (str): The path to the file.\n        make_dirs (bool, optional): Whether to create the directory if it does not exist. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If the directory could not be found.\n        TypeError: If the input directory path is not a string.\n\n    Returns:\n        str: The absolute path to the file.\n    \"\"\"\n    if isinstance(file_path, str):\n        if file_path.startswith(\"~\"):\n            file_path = os.path.expanduser(file_path)\n        else:\n            file_path = os.path.abspath(file_path)\n\n        file_dir = os.path.dirname(file_path)\n        if not os.path.exists(file_dir) and make_dirs:\n            os.makedirs(file_dir)\n\n        return file_path\n\n    else:\n        raise TypeError(\"The provided file path must be a string.\")\n</code></pre>"},{"location":"common/#samgeo.common.choose_device","title":"<code>choose_device(empty_cache=True, quiet=True)</code>","text":"<p>Choose a device (CPU or GPU) for deep learning.</p> <p>Parameters:</p> Name Type Description Default <code>empty_cache</code> <code>bool</code> <p>Whether to empty the CUDA cache if a GPU is used. Defaults to True.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress device information printout. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The device name.</p> Source code in <code>samgeo/common.py</code> <pre><code>def choose_device(empty_cache: bool = True, quiet: bool = True) -&gt; str:\n    \"\"\"Choose a device (CPU or GPU) for deep learning.\n\n    Args:\n        empty_cache (bool): Whether to empty the CUDA cache if a GPU is used. Defaults to True.\n        quiet (bool): Whether to suppress device information printout. Defaults to True.\n\n    Returns:\n        str: The device name.\n    \"\"\"\n    import torch\n\n    # if using Apple MPS, fall back to CPU for unsupported ops\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n    # select the device for computation\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    if not quiet:\n        print(f\"Using device: {device}\")\n\n    if device.type == \"cuda\":\n        if empty_cache:\n            torch.cuda.empty_cache()\n        # use bfloat16 for the entire notebook\n        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n        if torch.cuda.get_device_properties(0).major &gt;= 8:\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n    elif device.type == \"mps\":\n        if not quiet:\n            print(\n                \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n                \"give numerically different outputs and sometimes degraded performance on MPS. \"\n                \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n            )\n    return device\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_geojson","title":"<code>coords_to_geojson(coords, output=None)</code>","text":"<p>Convert a list of coordinates (lon, lat) to a GeoJSON string or file.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of coordinates (lon, lat).</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A GeoJSON dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_geojson(coords, output=None):\n    \"\"\"Convert a list of coordinates (lon, lat) to a GeoJSON string or file.\n\n    Args:\n        coords (list): A list of coordinates (lon, lat).\n        output (str, optional): The output file path. Defaults to None.\n\n    Returns:\n        dict: A GeoJSON dictionary.\n    \"\"\"\n\n    import json\n\n    if len(coords) == 0:\n        return\n    # Create a GeoJSON FeatureCollection object\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": []}\n\n    # Iterate through the coordinates list and create a GeoJSON Feature object for each coordinate\n    for coord in coords:\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": {\"type\": \"Point\", \"coordinates\": coord},\n            \"properties\": {},\n        }\n        feature_collection[\"features\"].append(feature)\n\n    # Convert the FeatureCollection object to a JSON string\n    geojson_str = json.dumps(feature_collection)\n\n    if output is not None:\n        with open(output, \"w\") as f:\n            f.write(geojson_str)\n    else:\n        return geojson_str\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>ndarray</code> <p>A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]     or [[[x1, y1]], [[x2, y2]], ...].</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <p>Whether to return out-of-bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D or 3D array of pixel coordinates in the same format as the input.</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: np.ndarray,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds=False,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]\n                or [[[x1, y1]], [[x2, y2]], ...].\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out-of-bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A 2D or 3D array of pixel coordinates in the same format as the input.\n    \"\"\"\n    from rasterio.warp import transform as transform_coords\n\n    out_of_bounds = []\n    if isinstance(coords, np.ndarray):\n        input_is_3d = coords.ndim == 3  # Check if the input is a 3D array\n    else:\n        input_is_3d = False\n\n    # Flatten the 3D array to 2D if necessary\n    if input_is_3d:\n        original_shape = coords.shape  # Store the original shape\n        coords = coords.reshape(-1, 2)  # Flatten to 2D\n\n    # Convert ndarray to a list if necessary\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(coord_crs, src.crs, xs, ys, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n\n    for i, (x, y) in enumerate(result):\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # Convert the output back to the original shape if input was 3D\n    output = np.array(output)\n    if input_is_3d:\n        output = output.reshape(original_shape)\n\n    # Handle cases where no valid pixel coordinates are found\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        return output, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint","title":"<code>download_checkpoint(model_type='vit_h', checkpoint_dir=None, hq=False)</code>","text":"<p>Download the SAM model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. Can be one of ['vit_h', 'vit_l', 'vit_b']. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_dir</code> <code>str</code> <p>The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.</p> <code>False</code> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint(model_type=\"vit_h\", checkpoint_dir=None, hq=False):\n    \"\"\"Download the SAM model checkpoint.\n\n    Args:\n        model_type (str, optional): The model type. Can be one of ['vit_h', 'vit_l', 'vit_b'].\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_dir (str, optional): The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".\n        hq (bool, optional): Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.\n    \"\"\"\n\n    if not hq:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_vit_h_4b8939.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n            },\n            \"vit_l\": {\n                \"name\": \"sam_vit_l_0b3195.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_vit_b_01ec64.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n            },\n        }\n    else:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_hq_vit_h.pth\",\n                \"url\": [\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.zip\",\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.z01\",\n                ],\n            },\n            \"vit_l\": {\n                \"name\": \"sam_hq_vit_l.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_l.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_hq_vit_b.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_b.pth\",\n            },\n            \"vit_tiny\": {\n                \"name\": \"sam_hq_vit_tiny.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_tiny.pth\",\n            },\n        }\n\n    if model_type not in model_types:\n        raise ValueError(\n            f\"Invalid model_type: {model_type}. It must be one of {', '.join(model_types)}\"\n        )\n\n    if checkpoint_dir is None:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    checkpoint = os.path.join(checkpoint_dir, model_types[model_type][\"name\"])\n    if not os.path.exists(checkpoint):\n        print(f\"Model checkpoint for {model_type} not found.\")\n        url = model_types[model_type][\"url\"]\n        if isinstance(url, str):\n            download_file(url, checkpoint)\n        elif isinstance(url, list):\n            download_files(url, checkpoint_dir, multi_part=True)\n    return checkpoint\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint_legacy","title":"<code>download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs)</code>","text":"<p>Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The checkpoint URL. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs):\n    \"\"\"Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n\n    Args:\n        url (str, optional): The checkpoint URL. Defaults to None.\n        output (str, optional): The output file path. Defaults to None.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    checkpoints = {\n        \"sam_vit_h_4b8939.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        \"sam_vit_l_0b3195.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n        \"sam_vit_b_01ec64.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n    }\n\n    if isinstance(url, str) and url in checkpoints:\n        url = checkpoints[url]\n\n    if url is None:\n        url = checkpoints[\"sam_vit_h_4b8939.pth\"]\n\n    if output is None:\n        output = os.path.basename(url)\n\n    return download_file(url, output, overwrite=overwrite, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.download_file","title":"<code>download_file(url=None, output=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False)</code>","text":"<p>Download a file from URL, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Google Drive URL is also supported. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_file(\n    url=None,\n    output=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n):\n    \"\"\"Download a file from URL, including Google Drive shared URL.\n\n    Args:\n        url (str, optional): Google Drive URL is also supported. Defaults to None.\n        output (str, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string,\n            in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    import zipfile\n\n    try:\n        import gdown\n    except ImportError:\n        print(\n            \"The gdown package is required for this function. Use `pip install gdown` to install it.\"\n        )\n        return\n\n    if output is None:\n        if isinstance(url, str) and url.startswith(\"http\"):\n            output = os.path.basename(url)\n\n    out_dir = os.path.abspath(os.path.dirname(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(url, str):\n        if os.path.exists(os.path.abspath(output)) and (not overwrite):\n            print(\n                f\"{output} already exists. Skip downloading. Set overwrite=True to overwrite.\"\n            )\n            return os.path.abspath(output)\n        else:\n            url = github_raw_url(url)\n\n    if \"https://drive.google.com/file/d/\" in url:\n        fuzzy = True\n\n    output = gdown.download(\n        url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume\n    )\n\n    if unzip and output.endswith(\".zip\"):\n        with zipfile.ZipFile(output, \"r\") as zip_ref:\n            if not quiet:\n                print(\"Extracting files...\")\n            if subfolder:\n                basename = os.path.splitext(os.path.basename(output))[0]\n\n                output = os.path.join(out_dir, basename)\n                if not os.path.exists(output):\n                    os.makedirs(output)\n                zip_ref.extractall(output)\n            else:\n                zip_ref.extractall(os.path.dirname(output))\n\n    return os.path.abspath(output)\n</code></pre>"},{"location":"common/#samgeo.common.download_files","title":"<code>download_files(urls, out_dir=None, filenames=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False, multi_part=False)</code>","text":"<p>Download files from URLs, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>The list of urls to download. Google Drive URL is also supported.</p> required <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>filenames</code> <code>list</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <code>multi_part</code> <code>bool</code> <p>If the file is a multi-part file. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\nbase_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\nurls = [base_url + f for f in files]\nleafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n</code></pre> Source code in <code>samgeo/common.py</code> <pre><code>def download_files(\n    urls,\n    out_dir=None,\n    filenames=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n    multi_part=False,\n):\n    \"\"\"Download files from URLs, including Google Drive shared URL.\n\n    Args:\n        urls (list): The list of urls to download. Google Drive URL is also supported.\n        out_dir (str, optional): The output directory. Defaults to None.\n        filenames (list, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n        multi_part (bool, optional): If the file is a multi-part file. Defaults to False.\n\n    Examples:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n    \"\"\"\n\n    if out_dir is None:\n        out_dir = os.getcwd()\n\n    if filenames is None:\n        filenames = [None] * len(urls)\n\n    filepaths = []\n    for url, output in zip(urls, filenames):\n        if output is None:\n            filename = os.path.join(out_dir, os.path.basename(url))\n        else:\n            filename = os.path.join(out_dir, output)\n\n        filepaths.append(filename)\n        if multi_part:\n            unzip = False\n\n        download_file(\n            url,\n            filename,\n            quiet,\n            proxy,\n            speed,\n            use_cookies,\n            verify,\n            id,\n            fuzzy,\n            resume,\n            unzip,\n            overwrite,\n            subfolder,\n        )\n\n    if multi_part:\n        archive = os.path.splitext(filename)[0] + \".zip\"\n        out_dir = os.path.dirname(filename)\n        extract_archive(archive, out_dir)\n\n        for file in filepaths:\n            os.remove(file)\n</code></pre>"},{"location":"common/#samgeo.common.extract_archive","title":"<code>extract_archive(archive, outdir=None, **kwargs)</code>","text":"<p>Extracts a multipart archive.</p> <p>This function uses the patoolib library to extract a multipart archive. If the patoolib library is not installed, it attempts to install it. If the archive does not end with \".zip\", it appends \".zip\" to the archive name. If the extraction fails (for example, if the files already exist), it skips the extraction.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>str</code> <p>The path to the archive file.</p> required <code>outdir</code> <code>str</code> <p>The directory where the archive should be extracted.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for the patoolib.extract_archive function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>An exception is raised if the extraction fails for reasons other than the files already existing.</p> <p>Example:</p> <pre><code>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\nbase_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\nurls = [base_url + f for f in files]\nleafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n</code></pre> Source code in <code>samgeo/common.py</code> <pre><code>def extract_archive(archive, outdir=None, **kwargs):\n    \"\"\"\n    Extracts a multipart archive.\n\n    This function uses the patoolib library to extract a multipart archive.\n    If the patoolib library is not installed, it attempts to install it.\n    If the archive does not end with \".zip\", it appends \".zip\" to the archive name.\n    If the extraction fails (for example, if the files already exist), it skips the extraction.\n\n    Args:\n        archive (str): The path to the archive file.\n        outdir (str): The directory where the archive should be extracted.\n        **kwargs: Arbitrary keyword arguments for the patoolib.extract_archive function.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: An exception is raised if the extraction fails for reasons other than the files already existing.\n\n    Example:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n\n    \"\"\"\n    try:\n        import patoolib\n    except ImportError:\n        install_package(\"patool\")\n        import patoolib\n\n    if not archive.endswith(\".zip\"):\n        archive = archive + \".zip\"\n\n    if outdir is None:\n        outdir = os.path.dirname(archive)\n\n    try:\n        patoolib.extract_archive(archive, outdir=outdir, **kwargs)\n    except Exception as e:\n        print(\"The unzipped files might already exist. Skipping extraction.\")\n        return\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.geotiff_to_jpg","title":"<code>geotiff_to_jpg(geotiff_path, output_path)</code>","text":"<p>Convert a GeoTIFF file to a JPG file.</p> <p>Parameters:</p> Name Type Description Default <code>geotiff_path</code> <code>str</code> <p>The path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>The path to the output JPG file.</p> required Source code in <code>samgeo/common.py</code> <pre><code>def geotiff_to_jpg(geotiff_path: str, output_path: str) -&gt; None:\n    \"\"\"Convert a GeoTIFF file to a JPG file.\n\n    Args:\n        geotiff_path (str): The path to the input GeoTIFF file.\n        output_path (str): The path to the output JPG file.\n    \"\"\"\n\n    from PIL import Image\n\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_path) as src:\n        # Read the first band (for grayscale) or all bands\n        array = src.read()\n\n        # If the array has more than 3 bands, reduce it to the first 3 (RGB)\n        if array.shape[0] &gt;= 3:\n            array = array[:3, :, :]  # Select the first 3 bands (R, G, B)\n        elif array.shape[0] == 1:\n            # For single-band images, repeat the band to create a grayscale RGB\n            array = np.repeat(array, 3, axis=0)\n\n        # Transpose the array from (bands, height, width) to (height, width, bands)\n        array = np.transpose(array, (1, 2, 0))\n\n        # Normalize the array to 8-bit (0-255) range for JPG\n        array = array.astype(np.float32)\n        array -= array.min()\n        array /= array.max()\n        array *= 255\n        array = array.astype(np.uint8)\n\n        # Convert to a PIL Image and save as JPG\n        image = Image.fromarray(array)\n        image.save(output_path)\n</code></pre>"},{"location":"common/#samgeo.common.geotiff_to_jpg_batch","title":"<code>geotiff_to_jpg_batch(input_folder, output_folder=None)</code>","text":"<p>Convert all GeoTIFF files in a folder to JPG files.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The path to the folder containing GeoTIFF files.</p> required <code>output_folder</code> <code>str</code> <p>The path to the folder to save the output JPG files.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the output folder containing the JPG files.</p> Source code in <code>samgeo/common.py</code> <pre><code>def geotiff_to_jpg_batch(input_folder: str, output_folder: str = None) -&gt; str:\n    \"\"\"Convert all GeoTIFF files in a folder to JPG files.\n\n    Args:\n        input_folder (str): The path to the folder containing GeoTIFF files.\n        output_folder (str): The path to the folder to save the output JPG files.\n\n    Returns:\n        str: The path to the output folder containing the JPG files.\n    \"\"\"\n\n    if output_folder is None:\n        output_folder = make_temp_dir()\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    geotiff_files = [\n        f for f in os.listdir(input_folder) if f.endswith(\".tif\") or f.endswith(\".tiff\")\n    ]\n\n    # Initialize tqdm progress bar\n    for filename in tqdm(geotiff_files, desc=\"Converting GeoTIFF to JPG\"):\n        geotiff_path = os.path.join(input_folder, filename)\n        jpg_filename = os.path.splitext(filename)[0] + \".jpg\"\n        output_path = os.path.join(output_folder, jpg_filename)\n        geotiff_to_jpg(geotiff_path, output_path)\n\n    return output_folder\n</code></pre>"},{"location":"common/#samgeo.common.get_basemaps","title":"<code>get_basemaps(free_only=True)</code>","text":"<p>Returns a dictionary of xyz basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of xyz basemaps.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_basemaps(free_only=True):\n    \"\"\"Returns a dictionary of xyz basemaps.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz basemaps.\n    \"\"\"\n\n    basemaps = {}\n    xyz_dict = get_xyz_dict(free_only=free_only)\n    for item in xyz_dict:\n        name = xyz_dict[item].name\n        url = xyz_dict[item].build_url()\n        basemaps[name] = url\n\n    return basemaps\n</code></pre>"},{"location":"common/#samgeo.common.get_vector_crs","title":"<code>get_vector_crs(filename, **kwargs)</code>","text":"<p>Gets the CRS of a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The CRS of the vector file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_vector_crs(filename, **kwargs):\n    \"\"\"Gets the CRS of a vector file.\n\n    Args:\n        filename (str): The vector file path.\n\n    Returns:\n        str: The CRS of the vector file.\n    \"\"\"\n    gdf = gpd.read_file(filename, **kwargs)\n    epsg = gdf.crs.to_epsg()\n    if epsg is None:\n        return gdf.crs\n    else:\n        return f\"EPSG:{epsg}\"\n</code></pre>"},{"location":"common/#samgeo.common.get_xyz_dict","title":"<code>get_xyz_dict(free_only=True)</code>","text":"<p>Returns a dictionary of xyz services.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of xyz services.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_xyz_dict(free_only=True):\n    \"\"\"Returns a dictionary of xyz services.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz services.\n    \"\"\"\n    import collections\n\n    import xyzservices.providers as xyz\n\n    def _unpack_sub_parameters(var, param):\n        temp = var\n        for sub_param in param.split(\".\"):\n            temp = getattr(temp, sub_param)\n        return temp\n\n    xyz_dict = {}\n    for item in xyz.values():\n        try:\n            name = item[\"name\"]\n            tile = _unpack_sub_parameters(xyz, name)\n            if _unpack_sub_parameters(xyz, name).requires_token():\n                if free_only:\n                    pass\n                else:\n                    xyz_dict[name] = tile\n            else:\n                xyz_dict[name] = tile\n\n        except Exception:\n            for sub_item in item:\n                name = item[sub_item][\"name\"]\n                tile = _unpack_sub_parameters(xyz, name)\n                if _unpack_sub_parameters(xyz, name).requires_token():\n                    if free_only:\n                        pass\n                    else:\n                        xyz_dict[name] = tile\n                else:\n                    xyz_dict[name] = tile\n\n    xyz_dict = collections.OrderedDict(sorted(xyz_dict.items()))\n    return xyz_dict\n</code></pre>"},{"location":"common/#samgeo.common.github_raw_url","title":"<code>github_raw_url(url)</code>","text":"<p>Get the raw URL for a GitHub file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The GitHub URL.</p> required <p>Returns:     str: The raw URL.</p> Source code in <code>samgeo/common.py</code> <pre><code>def github_raw_url(url):\n    \"\"\"Get the raw URL for a GitHub file.\n\n    Args:\n        url (str): The GitHub URL.\n    Returns:\n        str: The raw URL.\n    \"\"\"\n    if isinstance(url, str) and url.startswith(\"https://github.com/\") and \"blob\" in url:\n        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n    return url\n</code></pre>"},{"location":"common/#samgeo.common.image_to_cog","title":"<code>image_to_cog(source, dst_path=None, profile='deflate', **kwargs)</code>","text":"<p>Converts an image to a COG file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A dataset path, URL or rasterio.io.DatasetReader object.</p> required <code>dst_path</code> <code>str</code> <p>An output dataset path or or PathLike object. Defaults to None.</p> <code>None</code> <code>profile</code> <code>str</code> <p>COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".</p> <code>'deflate'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If rio-cogeo is not installed.</p> <code>FileNotFoundError</code> <p>If the source file could not be found.</p> Source code in <code>samgeo/common.py</code> <pre><code>def image_to_cog(source, dst_path=None, profile=\"deflate\", **kwargs):\n    \"\"\"Converts an image to a COG file.\n\n    Args:\n        source (str): A dataset path, URL or rasterio.io.DatasetReader object.\n        dst_path (str, optional): An output dataset path or or PathLike object. Defaults to None.\n        profile (str, optional): COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".\n\n    Raises:\n        ImportError: If rio-cogeo is not installed.\n        FileNotFoundError: If the source file could not be found.\n    \"\"\"\n    try:\n        from rio_cogeo.cogeo import cog_translate\n        from rio_cogeo.profiles import cog_profiles\n\n    except ImportError:\n        raise ImportError(\n            \"The rio-cogeo package is not installed. Please install it with `pip install rio-cogeo` or `conda install rio-cogeo -c conda-forge`.\"\n        )\n\n    if not source.startswith(\"http\"):\n        source = check_file_path(source)\n\n        if not os.path.exists(source):\n            raise FileNotFoundError(\"The provided input file could not be found.\")\n\n    if dst_path is None:\n        if not source.startswith(\"http\"):\n            dst_path = os.path.splitext(source)[0] + \"_cog.tif\"\n        else:\n            dst_path = temp_file_path(extension=\".tif\")\n\n    dst_path = check_file_path(dst_path)\n\n    dst_profile = cog_profiles.get(profile)\n    cog_translate(source, dst_path, dst_profile, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.images_to_video","title":"<code>images_to_video(images, output_video, fps=30, video_size=None)</code>","text":"<p>Converts a series of images into a video. The input can be either a directory containing the images or a list of image file paths.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Union[str, List[str]]</code> <p>A directory containing images or a list of image file paths.</p> required <code>output_video</code> <code>str</code> <p>The filename of the output video (e.g., 'output.mp4').</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video. Default is 30.</p> <code>30</code> <code>video_size</code> <code>Optional[tuple]</code> <p>The size (width, height) of the video. If not provided, the size of the first image is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided path is not a directory, if the images list is empty, or if the first image cannot be read.</p> Example usage <p>images_to_video('path_to_image_directory', 'output_video.mp4', fps=30, video_size=(1280, 720)) images_to_video(['image1.jpg', 'image2.jpg', 'image3.jpg'], 'output_video.mp4', fps=30)</p> Source code in <code>samgeo/common.py</code> <pre><code>def images_to_video(\n    images: Union[str, List[str]],\n    output_video: str,\n    fps: int = 30,\n    video_size: Optional[tuple] = None,\n) -&gt; None:\n    \"\"\"\n    Converts a series of images into a video. The input can be either a directory\n    containing the images or a list of image file paths.\n\n    Args:\n        images (Union[str, List[str]]): A directory containing images or a list\n            of image file paths.\n        output_video (str): The filename of the output video (e.g., 'output.mp4').\n        fps (int, optional): Frames per second for the output video. Default is 30.\n        video_size (Optional[tuple], optional): The size (width, height) of the\n            video. If not provided, the size of the first image is used.\n\n    Raises:\n        ValueError: If the provided path is not a directory, if the images list\n            is empty, or if the first image cannot be read.\n\n    Example usage:\n        images_to_video('path_to_image_directory', 'output_video.mp4', fps=30, video_size=(1280, 720))\n        images_to_video(['image1.jpg', 'image2.jpg', 'image3.jpg'], 'output_video.mp4', fps=30)\n    \"\"\"\n    if isinstance(images, str):\n        if not os.path.isdir(images):\n            raise ValueError(f\"The provided path {images} is not a valid directory.\")\n\n        # Get all image files in the directory (sorted by filename)\n\n        files = sorted(os.listdir(images))\n        if len(files) == 0:\n            raise ValueError(f\"No image files found in the directory {images}\")\n        elif files[0].endswith(\".tif\"):\n            images = geotiff_to_jpg_batch(images)\n\n        images = [\n            os.path.join(images, img)\n            for img in sorted(os.listdir(images))\n            if img.endswith((\".jpg\", \".png\"))\n        ]\n\n    if not isinstance(images, list) or not images:\n        raise ValueError(\n            \"The images parameter should either be a non-empty list of image paths or a valid directory.\"\n        )\n\n    # Read the first image to get the dimensions if video_size is not provided\n    first_image_path = images[0]\n    frame = cv2.imread(first_image_path)\n\n    if frame is None:\n        raise ValueError(f\"Error reading the first image {first_image_path}\")\n\n    if video_size is None:\n        height, width, _ = frame.shape\n        video_size = (width, height)\n\n    # TODO: This video codec is giving me some problems, not sure if it's the correct one\n    fourcc = cv2.VideoWriter_fourcc(*\"avc1\")  # Define the codec for mp4\n    video_writer = cv2.VideoWriter(output_video, fourcc, fps, video_size)\n\n    for image_path in images:\n        frame = cv2.imread(image_path)\n        if frame is None:\n            print(f\"Warning: Could not read image {image_path}. Skipping.\")\n            continue\n\n        if video_size != (frame.shape[1], frame.shape[0]):\n            frame = cv2.resize(frame, video_size)\n\n        video_writer.write(frame)\n\n    video_writer.release()\n    print(f\"Video saved as {output_video}\")\n</code></pre>"},{"location":"common/#samgeo.common.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>samgeo/common.py</code> <pre><code>def install_package(package):\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n\n    for package in packages:\n        if package.startswith(\"https://github.com\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"common/#samgeo.common.is_colab","title":"<code>is_colab()</code>","text":"<p>Tests if the code is being executed within Google Colab.</p> Source code in <code>samgeo/common.py</code> <pre><code>def is_colab():\n    \"\"\"Tests if the code is being executed within Google Colab.\"\"\"\n    import sys\n\n    return \"google.colab\" in sys.modules\n</code></pre>"},{"location":"common/#samgeo.common.make_temp_dir","title":"<code>make_temp_dir(**kwargs)</code>","text":"<p>Create a temporary directory and return the path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the temporary directory.</p> Source code in <code>samgeo/common.py</code> <pre><code>def make_temp_dir(**kwargs) -&gt; str:\n    \"\"\"Create a temporary directory and return the path.\n\n    Returns:\n        str: The path to the temporary directory.\n    \"\"\"\n    import tempfile\n\n    temp_dir = tempfile.mkdtemp(**kwargs)\n    return temp_dir\n</code></pre>"},{"location":"common/#samgeo.common.merge_rasters","title":"<code>merge_rasters(input_dir, output, input_pattern='*.tif', output_format='GTiff', output_nodata=None, output_options=['COMPRESS=DEFLATE'])</code>","text":"<p>Merge a directory of rasters into a single raster.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>The path to the input directory.</p> required <code>output</code> <code>str</code> <p>The path to the output raster.</p> required <code>input_pattern</code> <code>str</code> <p>The pattern to match the input files. Defaults to \"*.tif\".</p> <code>'*.tif'</code> <code>output_format</code> <code>str</code> <p>The output format. Defaults to \"GTiff\".</p> <code>'GTiff'</code> <code>output_nodata</code> <code>float</code> <p>The output nodata value. Defaults to None.</p> <code>None</code> <code>output_options</code> <code>list</code> <p>A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].</p> <code>['COMPRESS=DEFLATE']</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def merge_rasters(\n    input_dir,\n    output,\n    input_pattern=\"*.tif\",\n    output_format=\"GTiff\",\n    output_nodata=None,\n    output_options=[\"COMPRESS=DEFLATE\"],\n):\n    \"\"\"Merge a directory of rasters into a single raster.\n\n    Args:\n        input_dir (str): The path to the input directory.\n        output (str): The path to the output raster.\n        input_pattern (str, optional): The pattern to match the input files. Defaults to \"*.tif\".\n        output_format (str, optional): The output format. Defaults to \"GTiff\".\n        output_nodata (float, optional): The output nodata value. Defaults to None.\n        output_options (list, optional): A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    import glob\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n\n    # Get a list of all the input files\n    input_files = glob.glob(os.path.join(input_dir, input_pattern))\n\n    # Prepare the gdal.Warp options\n    warp_options = gdal.WarpOptions(\n        format=output_format, dstNodata=output_nodata, creationOptions=output_options\n    )\n\n    # Merge the input files into a single output file\n    gdal.Warp(\n        destNameOrDestDS=output,\n        srcDSOrSrcDSTab=input_files,\n        options=warp_options,\n    )\n</code></pre>"},{"location":"common/#samgeo.common.orthogonalize","title":"<code>orthogonalize(filepath, output=None, maxAngleChange=15, skewTolerance=15)</code>","text":"<p>Orthogonalizes polygon by making all angles 90 or 180 degrees. The source     code is adapted from https://github.com/Mashin6/orthogonalize-polygon, which     is distributed under the terms of the GNU General Public License v3.0.     Credits to the original author Martin Machyna.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | GeoDataFrame</code> <p>The path to the input file or a GeoDataFrame.</p> required <code>output</code> <code>str</code> <p>The path to the output file. Defaults to None.</p> <code>None</code> <code>maxAngleChange</code> <code>int</code> <p>angle (0,45&gt; degrees. Sets the maximum angle deviation                 from the cardinal direction for the segment to be still                 considered to continue in the same direction as the                 previous segment.</p> <code>15</code> <code>skewTolerance</code> <code>int</code> <p>angle &lt;0,45&gt; degrees. Sets skew tolerance for segments that                 are at 45\u02da\u00b1Tolerance angle from the overall rectangular shape                 of the polygon. Useful when preserving e.g. bay windows on a                 house.</p> <code>15</code> Source code in <code>samgeo/common.py</code> <pre><code>def orthogonalize(filepath, output=None, maxAngleChange=15, skewTolerance=15):\n    \"\"\"Orthogonalizes polygon by making all angles 90 or 180 degrees. The source\n        code is adapted from https://github.com/Mashin6/orthogonalize-polygon, which\n        is distributed under the terms of the GNU General Public License v3.0.\n        Credits to the original author Martin Machyna.\n\n    Args:\n        filepath (str | geopandas.GeoDataFrame): The path to the input file or a GeoDataFrame.\n        output (str, optional): The path to the output file. Defaults to None.\n        maxAngleChange (int, optional): angle (0,45&gt; degrees. Sets the maximum angle deviation\n                            from the cardinal direction for the segment to be still\n                            considered to continue in the same direction as the\n                            previous segment.\n        skewTolerance (int, optional): angle &lt;0,45&gt; degrees. Sets skew tolerance for segments that\n                            are at 45\u02da\u00b1Tolerance angle from the overall rectangular shape\n                            of the polygon. Useful when preserving e.g. bay windows on a\n                            house.\n    \"\"\"\n\n    import math\n    import statistics\n\n    from shapely.geometry import MultiPolygon, Polygon\n\n    def calculate_initial_compass_bearing(pointA, pointB):\n        \"\"\"\n        Calculates the bearing between two points.\n\n        The formulae used is the following:\n            \u03b8 = atan2(sin(\u0394long).cos(lat2),\n                    cos(lat1).sin(lat2) - sin(lat1).cos(lat2).cos(\u0394long))\n\n        :Parameters:\n        - `pointA: The tuple representing the latitude/longitude for the\n            first point. Latitude and longitude must be in decimal degrees\n        - `pointB: The tuple representing the latitude/longitude for the\n            second point. Latitude and longitude must be in decimal degrees\n\n        :Returns:\n        The bearing in degrees\n\n        :Returns Type:\n        float\n        \"\"\"\n        if (not isinstance(pointA, tuple)) or (not isinstance(pointB, tuple)):\n            raise TypeError(\"Only tuples are supported as arguments\")\n\n        lat1 = math.radians(pointA[0])\n        lat2 = math.radians(pointB[0])\n\n        diffLong = math.radians(pointB[1] - pointA[1])\n\n        x = math.sin(diffLong) * math.cos(lat2)\n        y = math.cos(lat1) * math.sin(lat2) - (\n            math.sin(lat1) * math.cos(lat2) * math.cos(diffLong)\n        )\n\n        initial_bearing = math.atan2(x, y)\n\n        # Now we have the initial bearing but math.atan2 return values\n        # from -180\u00b0 to + 180\u00b0 which is not what we want for a compass bearing\n        # The solution is to normalize the initial bearing as shown below\n        initial_bearing = math.degrees(initial_bearing)\n        compass_bearing = (initial_bearing + 360) % 360\n\n        return compass_bearing\n\n    def calculate_segment_angles(polySimple, maxAngleChange=45):\n        \"\"\"\n        Calculates angles of all polygon segments to cardinal directions.\n\n        :Parameters:\n        - `polySimple: shapely polygon object containing simplified building.\n        - `maxAngleChange: angle (0,45&gt; degrees. Sets the maximum angle deviation\n                            from the cardinal direction for the segment to be still\n                            considered to continue in the same direction as the\n                            previous segment.\n\n        :Returns:\n        - orgAngle: Segments bearing\n        - corAngle: Segments angles to closest cardinal direction\n        - dirAngle: Segments direction [N, E, S, W] as [0, 1, 2, 3]\n\n        :Returns Type:\n        list\n        \"\"\"\n        # Convert limit angle to angle for subtraction\n        maxAngleChange = 45 - maxAngleChange\n\n        # Get points Lat/Lon\n        simple_X = polySimple.exterior.xy[0]\n        simple_Y = polySimple.exterior.xy[1]\n\n        # Calculate angle to cardinal directions for each segment of polygon\n        orgAngle = []  # Original angles\n        corAngle = []  # Correction angles used for rotation\n        dirAngle = []  # 0,1,2,3 = N,E,S,W\n        limit = [0] * 4\n\n        for i in range(0, (len(simple_X) - 1)):\n            point1 = (simple_Y[i], simple_X[i])\n            point2 = (simple_Y[i + 1], simple_X[i + 1])\n            angle = calculate_initial_compass_bearing(point1, point2)\n\n            if angle &gt; (45 + limit[1]) and angle &lt;= (135 - limit[1]):\n                orgAngle.append(angle)\n                corAngle.append(angle - 90)\n                dirAngle.append(1)\n\n            elif angle &gt; (135 + limit[2]) and angle &lt;= (225 - limit[2]):\n                orgAngle.append(angle)\n                corAngle.append(angle - 180)\n                dirAngle.append(2)\n\n            elif angle &gt; (225 + limit[3]) and angle &lt;= (315 - limit[3]):\n                orgAngle.append(angle)\n                corAngle.append(angle - 270)\n                dirAngle.append(3)\n\n            elif angle &gt; (315 + limit[0]) and angle &lt;= 360:\n                orgAngle.append(angle)\n                corAngle.append(angle - 360)\n                dirAngle.append(0)\n\n            elif angle &gt;= 0 and angle &lt;= (45 - limit[0]):\n                orgAngle.append(angle)\n                corAngle.append(angle)\n                dirAngle.append(0)\n\n            limit = [0] * 4\n            limit[dirAngle[i]] = (\n                maxAngleChange  # Set angle limit for the current direction\n            )\n            limit[(dirAngle[i] + 1) % 4] = (\n                -maxAngleChange\n            )  # Extend the angles for the adjacent directions\n            limit[(dirAngle[i] - 1) % 4] = -maxAngleChange\n\n        return orgAngle, corAngle, dirAngle\n\n    def rotate_polygon(polySimple, angle):\n        \"\"\"Rotates polygon around its centroid for given angle.\"\"\"\n        if polySimple.is_empty or not polySimple.is_valid:\n            return polySimple\n\n        try:\n            # Create WGS84 referenced GeoSeries\n            bS = gpd.GeoDataFrame(geometry=[polySimple], crs=\"EPSG:4326\")\n\n            # Temporary reproject to Mercator and rotate\n            bSR = bS.to_crs(\"epsg:3857\")\n            if len(bSR) == 0:\n                return polySimple\n\n            bSR = bSR.rotate(angle, origin=\"centroid\", use_radians=False)\n            bSR = bSR.to_crs(\"epsg:4326\")\n\n            # Validate result before returning\n            if len(bSR) == 0 or bSR.geometry.is_empty.any():\n                return polySimple\n\n            return bSR.geometry.iloc[0]\n\n        except Exception as e:\n            print(f\"Rotation failed: {str(e)}\")\n            return polySimple\n\n    def orthogonalize_polygon(polygon, maxAngleChange=15, skewTolerance=15):\n        \"\"\"\n        Master function that makes all angles in polygon outer and inner rings either 90 or 180 degrees.\n        Idea adapted from JOSM function orthogonalize\n        1) Calculate bearing [0-360 deg] of each polygon segment\n        2) From bearing determine general direction [N, E, S ,W], then calculate angle deviation from nearest cardinal direction for each segment\n        3) Rotate polygon by median deviation angle to align segments with xy coord axes (cardinal directions)\n        4) For vertical segments replace X coordinates of their points with mean value\n        For horizontal segments replace Y coordinates of their points with mean value\n        5) Rotate back\n\n        :Parameters:\n        - `polygon: shapely polygon object containing simplified building.\n        - `maxAngleChange: angle (0,45&gt; degrees. Sets the maximum angle deviation\n                            from the cardinal direction for the segment to be still\n                            considered to continue in the same direction as the\n                            previous segment.\n        - `skewTolerance: angle &lt;0,45&gt; degrees. Sets skew tolerance for segments that\n                            are at 45\u02da\u00b1Tolerance angle from the overall rectangular shape\n                            of the polygon. Useful when preserving e.g. bay windows on a\n                            house.\n\n        :Returns:\n        - polyOrthog: orthogonalized shapely polygon where all angles are 90 or 180 degrees\n\n        :Returns Type:\n        shapely Polygon\n        \"\"\"\n        # Check if polygon has inner rings that we want to orthogonalize as well\n        rings = [Polygon(polygon.exterior)]\n        for inner in list(polygon.interiors):\n            rings.append(Polygon(inner))\n\n        polyOrthog = []\n        for polySimple in rings:\n            # Get angles from cardinal directions of all segments\n            orgAngle, corAngle, dirAngle = calculate_segment_angles(polySimple)\n\n            # Calculate median angle that will be used for rotation\n            if statistics.stdev(corAngle) &lt; 30:\n                medAngle = statistics.median(corAngle)\n                # avAngle = statistics.mean(corAngle)\n            else:\n                medAngle = 45  # Account for cases when building is at ~45\u02da and we can't decide if to turn clockwise or anti-clockwise\n\n            # Rotate polygon to align its edges to cardinal directions\n            polySimpleR = rotate_polygon(polySimple, medAngle)\n\n            # Get directions of rotated polygon segments\n            orgAngle, corAngle, dirAngle = calculate_segment_angles(\n                polySimpleR, maxAngleChange\n            )\n\n            # Get Lat/Lon of rotated polygon points\n            rotatedX = polySimpleR.exterior.xy[0].tolist()\n            rotatedY = polySimpleR.exterior.xy[1].tolist()\n\n            # Scan backwards to check if starting segment is a continuation of straight region in the same direction\n            shift = 0\n            for i in range(1, len(dirAngle)):\n                if dirAngle[0] == dirAngle[-i]:\n                    shift = i\n                else:\n                    break\n            # If the first segment is part of continuing straight region then reset the index to its beginning\n            if shift != 0:\n                dirAngle = dirAngle[-shift:] + dirAngle[:-shift]\n                orgAngle = orgAngle[-shift:] + orgAngle[:-shift]\n                rotatedX = (\n                    rotatedX[-shift - 1 : -1] + rotatedX[:-shift]\n                )  # First and last points are the same in closed polygons\n                rotatedY = rotatedY[-shift - 1 : -1] + rotatedY[:-shift]\n\n            # Fix 180 degree turns (N-&gt;S, S-&gt;N, E-&gt;W, W-&gt;E)\n            # Subtract two adjacent directions and if the difference is 2, which means we have 180\u02da turn (0,1,3 are OK) then use the direction of the previous segment\n            dirAngleRoll = dirAngle[1:] + dirAngle[0:1]\n            dirAngle = [\n                (\n                    dirAngle[i - 1]\n                    if abs(dirAngle[i] - dirAngleRoll[i]) == 2\n                    else dirAngle[i]\n                )\n                for i in range(len(dirAngle))\n            ]\n\n            # Cycle through all segments\n            # Adjust points coordinates by taking the average of points in segment\n            dirAngle.append(dirAngle[0])  # Append dummy value\n            orgAngle.append(orgAngle[0])  # Append dummy value\n            segmentBuffer = (\n                []\n            )  # Buffer for determining which segments are part of one large straight line\n\n            for i in range(0, len(dirAngle) - 1):\n                # Preserving skewed walls: Leave walls that are obviously meant to be skewed 45\u02da+/- tolerance\u02da (e.g.angle 30-60 degrees) off main walls as untouched\n                if orgAngle[i] % 90 &gt; (45 - skewTolerance) and orgAngle[i] % 90 &lt; (\n                    45 + skewTolerance\n                ):\n                    continue\n\n                # Dealing with adjacent segments following the same direction\n                segmentBuffer.append(i)\n                if (\n                    dirAngle[i] == dirAngle[i + 1]\n                ):  # If next segment is of same orientation, we need 180 deg angle for straight line. Keep checking.\n                    if orgAngle[i + 1] % 90 &gt; (45 - skewTolerance) and orgAngle[\n                        i + 1\n                    ] % 90 &lt; (45 + skewTolerance):\n                        pass\n                    else:\n                        continue\n\n                if dirAngle[i] in {0, 2}:  # for N,S segments avereage x coordinate\n                    tempX = statistics.mean(\n                        rotatedX[segmentBuffer[0] : segmentBuffer[-1] + 2]\n                    )\n                    # Update with new coordinates\n                    rotatedX[segmentBuffer[0] : segmentBuffer[-1] + 2] = [tempX] * (\n                        len(segmentBuffer) + 1\n                    )  # Segment has 2 points therefore +1\n                elif dirAngle[i] in {1, 3}:  # for E,W segments avereage y coordinate\n                    tempY = statistics.mean(\n                        rotatedY[segmentBuffer[0] : segmentBuffer[-1] + 2]\n                    )\n                    # Update with new coordinates\n                    rotatedY[segmentBuffer[0] : segmentBuffer[-1] + 2] = [tempY] * (\n                        len(segmentBuffer) + 1\n                    )\n\n                if (\n                    0 in segmentBuffer\n                ):  # Copy change in first point to its last point so we don't lose it during Reverse shift\n                    rotatedX[-1] = rotatedX[0]\n                    rotatedY[-1] = rotatedY[0]\n\n                segmentBuffer = []\n\n            # Reverse shift so we get polygon with the same start/end point as before\n            if shift != 0:\n                rotatedX = (\n                    rotatedX[shift:] + rotatedX[1 : shift + 1]\n                )  # First and last points are the same in closed polygons\n                rotatedY = rotatedY[shift:] + rotatedY[1 : shift + 1]\n            else:\n                rotatedX[0] = rotatedX[-1]  # Copy updated coordinates to first node\n                rotatedY[0] = rotatedY[-1]\n\n            # Create polygon from new points\n            polyNew = Polygon(zip(rotatedX, rotatedY))\n\n            # Rotate polygon back\n            polyNew = rotate_polygon(polyNew, -medAngle)\n\n            # Add to list of finihed rings\n            polyOrthog.append(polyNew)\n\n        # Recreate the original object\n        polyOrthog = Polygon(\n            polyOrthog[0].exterior, [inner.exterior for inner in polyOrthog[1:]]\n        )\n        return polyOrthog\n\n    if isinstance(filepath, str):\n        buildings = gpd.read_file(filepath)\n    elif isinstance(filepath, gpd.GeoDataFrame):\n        buildings = filepath\n    else:\n        raise TypeError(\"Input must be a file path or a GeoDataFrame.\")\n\n    for i in range(0, len(buildings)):\n        build = buildings.loc[i, \"geometry\"]\n\n        if build.geom_type == \"MultiPolygon\":  # Multipolygons\n            multipolygon = []\n\n            for poly in build:\n                buildOrtho = orthogonalize_polygon(poly, maxAngleChange, skewTolerance)\n                multipolygon.append(buildOrtho)\n\n            buildings.loc[i, \"geometry\"] = gpd.GeoSeries(\n                MultiPolygon(multipolygon)\n            ).values  # Workaround for Pandas/Geopandas bug\n            # buildings.loc[i, 'geometry'] = MultiPolygon(multipolygon)   # Does not work\n\n        else:  # Polygons\n            buildOrtho = orthogonalize_polygon(build)\n\n            buildings.loc[i, \"geometry\"] = buildOrtho\n\n    if output is not None:\n        buildings.to_file(output)\n    else:\n        return buildings\n</code></pre>"},{"location":"common/#samgeo.common.overlay_images","title":"<code>overlay_images(image1, image2, alpha=0.5, backend='TkAgg', height_ratios=[10, 1], show_args1={}, show_args2={})</code>","text":"<p>Overlays two images using a slider to control the opacity of the top image.</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>str | ndarray</code> <p>The first input image at the bottom represented as a NumPy array or the path to the image.</p> required <code>image2</code> <code>_type_</code> <p>The second input image on top represented as a NumPy array or the path to the image.</p> required <code>alpha</code> <code>float</code> <p>The alpha value of the top image. Defaults to 0.5.</p> <code>0.5</code> <code>backend</code> <code>str</code> <p>The backend of the matplotlib plot. Defaults to \"TkAgg\".</p> <code>'TkAgg'</code> <code>height_ratios</code> <code>list</code> <p>The height ratios of the two subplots. Defaults to [10, 1].</p> <code>[10, 1]</code> <code>show_args1</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.</p> <code>{}</code> <code>show_args2</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def overlay_images(\n    image1,\n    image2,\n    alpha=0.5,\n    backend=\"TkAgg\",\n    height_ratios=[10, 1],\n    show_args1={},\n    show_args2={},\n):\n    \"\"\"Overlays two images using a slider to control the opacity of the top image.\n\n    Args:\n        image1 (str | np.ndarray): The first input image at the bottom represented as a NumPy array or the path to the image.\n        image2 (_type_): The second input image on top represented as a NumPy array or the path to the image.\n        alpha (float, optional): The alpha value of the top image. Defaults to 0.5.\n        backend (str, optional): The backend of the matplotlib plot. Defaults to \"TkAgg\".\n        height_ratios (list, optional): The height ratios of the two subplots. Defaults to [10, 1].\n        show_args1 (dict, optional): The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.\n        show_args2 (dict, optional): The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.\n\n    \"\"\"\n    import sys\n\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import matplotlib.widgets as mpwidgets\n\n    if \"google.colab\" in sys.modules:\n        backend = \"inline\"\n        print(\n            \"The TkAgg backend is not supported in Google Colab. The overlay_images function will not work on Colab.\"\n        )\n        return\n\n    matplotlib.use(backend)\n\n    if isinstance(image1, str):\n        if image1.startswith(\"http\"):\n            image1 = download_file(image1)\n\n        if not os.path.exists(image1):\n            raise ValueError(f\"Input path {image1} does not exist.\")\n\n    if isinstance(image2, str):\n        if image2.startswith(\"http\"):\n            image2 = download_file(image2)\n\n        if not os.path.exists(image2):\n            raise ValueError(f\"Input path {image2} does not exist.\")\n\n    # Load the two images\n    x = plt.imread(image1)\n    y = plt.imread(image2)\n\n    # Create the plot\n    fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": height_ratios})\n    img0 = ax0.imshow(x, **show_args1)\n    img1 = ax0.imshow(y, alpha=alpha, **show_args2)\n\n    # Define the update function\n    def update(value):\n        img1.set_alpha(value)\n        fig.canvas.draw_idle()\n\n    # Create the slider\n    slider0 = mpwidgets.Slider(ax=ax1, label=\"alpha\", valmin=0, valmax=1, valinit=alpha)\n    slider0.on_changed(update)\n\n    # Display the plot\n    plt.show()\n</code></pre>"},{"location":"common/#samgeo.common.random_string","title":"<code>random_string(string_length=6)</code>","text":"<p>Generates a random string of fixed length.</p> <p>Parameters:</p> Name Type Description Default <code>string_length</code> <code>int</code> <p>Fixed length. Defaults to 3.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A random string</p> Source code in <code>samgeo/common.py</code> <pre><code>def random_string(string_length=6):\n    \"\"\"Generates a random string of fixed length.\n\n    Args:\n        string_length (int, optional): Fixed length. Defaults to 3.\n\n    Returns:\n        str: A random string\n    \"\"\"\n    import random\n    import string\n\n    # random.seed(1001)\n    letters = string.ascii_lowercase\n    return \"\".join(random.choice(letters) for i in range(string_length))\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_geojson","title":"<code>raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".geojson\"):\n        output += \".geojson\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_gpkg","title":"<code>raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".gpkg\"):\n        output += \".gpkg\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_shp","title":"<code>raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".shp\"):\n        output += \".shp\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_vector","title":"<code>raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs)</code>","text":"<p>Vectorize a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs):\n    \"\"\"Vectorize a raster dataset.\n\n    Args:\n        source (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n    from rasterio import features\n\n    with rasterio.open(source) as src:\n        band = src.read()\n\n        mask = band != 0\n        shapes = features.shapes(band, mask=mask, transform=src.transform)\n\n    fc = [\n        {\"geometry\": shapely.geometry.shape(shape), \"properties\": {\"value\": value}}\n        for shape, value in shapes\n    ]\n    if simplify_tolerance is not None:\n        for i in fc:\n            i[\"geometry\"] = i[\"geometry\"].simplify(tolerance=simplify_tolerance)\n\n    gdf = gpd.GeoDataFrame.from_features(fc)\n    if src.crs is not None:\n        gdf.set_crs(crs=src.crs, inplace=True)\n\n    if dst_crs is not None:\n        gdf = gdf.to_crs(dst_crs)\n\n    gdf.to_file(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import pandas as pd\n    import rioxarray as rxr\n    import scipy.ndimage as ndi\n    import xarray as xr\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"equivalent_diameter_area\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n            if out_vector is not None:\n                tmp_vector = temp_file_path(\".gpkg\")\n                raster_to_vector(out_image, tmp_vector)\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n        return da, df\n</code></pre>"},{"location":"common/#samgeo.common.regularize","title":"<code>regularize(source, output=None, crs='EPSG:4326', **kwargs)</code>","text":"<p>Regularize a polygon GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | GeoDataFrame</code> <p>The input file path or a GeoDataFrame.</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>gpd.GeoDataFrame: The output GeoDataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def regularize(source, output=None, crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Regularize a polygon GeoDataFrame.\n\n    Args:\n        source (str | gpd.GeoDataFrame): The input file path or a GeoDataFrame.\n        output (str, optional): The output file path. Defaults to None.\n\n\n    Returns:\n        gpd.GeoDataFrame: The output GeoDataFrame.\n    \"\"\"\n    if isinstance(source, str):\n        gdf = gpd.read_file(source)\n    elif isinstance(source, gpd.GeoDataFrame):\n        gdf = source\n    else:\n        raise ValueError(\"The input source must be a GeoDataFrame or a file path.\")\n\n    polygons = gdf.geometry.apply(lambda geom: geom.minimum_rotated_rectangle)\n    result = gpd.GeoDataFrame(geometry=polygons, data=gdf.drop(\"geometry\", axis=1))\n\n    if crs is not None:\n        result.to_crs(crs, inplace=True)\n    if output is not None:\n        result.to_file(output, **kwargs)\n    else:\n        return result\n</code></pre>"},{"location":"common/#samgeo.common.reproject","title":"<code>reproject(image, output, dst_crs='EPSG:4326', resampling='nearest', to_cog=True, **kwargs)</code>","text":"<p>Reprojects an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The input image filepath.</p> required <code>output</code> <code>str</code> <p>The output image filepath.</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>resampling</code> <code>Resampling</code> <p>The resampling method. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>to_cog</code> <code>bool</code> <p>Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.open.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def reproject(\n    image, output, dst_crs=\"EPSG:4326\", resampling=\"nearest\", to_cog=True, **kwargs\n):\n    \"\"\"Reprojects an image.\n\n    Args:\n        image (str): The input image filepath.\n        output (str): The output image filepath.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        resampling (Resampling, optional): The resampling method. Defaults to \"nearest\".\n        to_cog (bool, optional): Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rasterio.open.\n\n    \"\"\"\n    import rasterio as rio\n    from rasterio.warp import Resampling, calculate_default_transform, reproject\n\n    if isinstance(resampling, str):\n        resampling = getattr(Resampling, resampling)\n\n    image = os.path.abspath(image)\n    output = os.path.abspath(output)\n\n    if not os.path.exists(os.path.dirname(output)):\n        os.makedirs(os.path.dirname(output))\n\n    with rio.open(image, **kwargs) as src:\n        transform, width, height = calculate_default_transform(\n            src.crs, dst_crs, src.width, src.height, *src.bounds\n        )\n        kwargs = src.meta.copy()\n        kwargs.update(\n            {\n                \"crs\": dst_crs,\n                \"transform\": transform,\n                \"width\": width,\n                \"height\": height,\n            }\n        )\n\n        with rio.open(output, \"w\", **kwargs) as dst:\n            for i in range(1, src.count + 1):\n                reproject(\n                    source=rio.band(src, i),\n                    destination=rio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=dst_crs,\n                    resampling=resampling,\n                    **kwargs,\n                )\n\n    if to_cog:\n        image_to_cog(output, output)\n</code></pre>"},{"location":"common/#samgeo.common.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A list of (x, y) coordinates.</p> Source code in <code>samgeo/common.py</code> <pre><code>def rowcol_to_xy(\n    src_fp,\n    rows=None,\n    cols=None,\n    boxes=None,\n    zs=None,\n    offset=\"center\",\n    output=None,\n    dst_crs=\"EPSG:4326\",\n    **kwargs,\n):\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"common/#samgeo.common.sam_map_gui","title":"<code>sam_map_gui(sam, basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for the draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def sam_map_gui(sam, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo):\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        repeat_mode (bool, optional): Whether to use the repeat mode for the draw control. Defaults to True.\n        out_dir (str, optional): The output directory. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n\n        import ipyevents\n        import ipyleaflet\n        import ipywidgets as widgets\n        import leafmap\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first or install the full samgeo package with: pip install segment-geospatial[all].\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(repeat_mode=repeat_mode, **kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except:\n        pass\n\n    m.fg_markers = []\n    m.bg_markers = []\n\n    fg_layer = ipyleaflet.LayerGroup(layers=m.fg_markers, name=\"Foreground\")\n    bg_layer = ipyleaflet.LayerGroup(layers=m.bg_markers, name=\"Background\")\n    m.add(fg_layer)\n    m.add(bg_layer)\n    m.fg_layer = fg_layer\n    m.bg_layer = bg_layer\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 0px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=padding),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    plus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load foreground points\",\n        icon=\"plus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    minus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load background points\",\n        icon=\"minus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    radio_buttons = widgets.RadioButtons(\n        options=[\"Foreground\", \"Background\"],\n        description=\"Class Type:\",\n        disabled=False,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    def on_radio_button_click(change):\n        # Set a flag to indicate UI interaction is happening\n        m._ui_interaction = True\n        # Use a small delay to reset the flag after the event is processed\n        import threading\n\n        threading.Timer(0.1, lambda: setattr(m, \"_ui_interaction\", False)).start()\n\n    radio_buttons.observe(on_radio_button_click, \"value\")\n\n    fg_count = widgets.IntText(\n        value=0,\n        description=\"Foreground #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n    bg_count = widgets.IntText(\n        value=0,\n        description=\"Background #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Mask opacity:\",\n        min=0,\n        max=1,\n        value=0.7,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    buttons = widgets.VBox(\n        [\n            radio_buttons,\n            widgets.HBox([fg_count, bg_count]),\n            opacity_slider,\n            widgets.HBox([rectangular, colorpicker]),\n            widgets.HBox(\n                [segment_button, save_button, reset_button],\n                layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n            ),\n        ]\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            mask_layer = m.find_layer(\"Masks\")\n            if mask_layer is not None:\n                mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, plus_button, minus_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        buttons,\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def marker_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    gdf = gpd.read_file(chooser.selected)\n                    centroids = gdf.centroid\n                    coords = [[point.x, point.y] for point in centroids]\n                    for coord in coords:\n                        if plus_button.value:\n                            if is_colab():  # Colab does not support AwesomeIcon\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"green\",\n                                    fill_color=\"green\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"plus-circle\",\n                                        marker_color=\"green\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.fg_layer.add(marker)\n                            m.fg_markers.append(marker)\n                            fg_count.value = len(m.fg_markers)\n                        elif minus_button.value:\n                            if is_colab():\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"red\",\n                                    fill_color=\"red\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"minus-circle\",\n                                        marker_color=\"red\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.bg_layer.add(marker)\n                            m.bg_markers.append(marker)\n                            bg_count.value = len(m.bg_markers)\n\n                except Exception as e:\n                    print(e)\n\n            if m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                delattr(m, \"marker_control\")\n\n            plus_button.value = False\n            minus_button.value = False\n\n    def marker_button_click(change):\n        if change[\"new\"]:\n            sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n            filechooser = FileChooser(\n                path=os.getcwd(),\n                sandbox_path=sandbox_path,\n                layout=widgets.Layout(width=\"454px\"),\n            )\n            filechooser.use_dir_icons = True\n            filechooser.filter_pattern = [\"*.shp\", \"*.geojson\", \"*.gpkg\"]\n            filechooser.register_callback(marker_callback)\n            marker_control = ipyleaflet.WidgetControl(\n                widget=filechooser, position=\"topright\"\n            )\n            m.add_control(marker_control)\n            m.marker_control = marker_control\n        else:\n            if hasattr(m, \"marker_control\") and m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                m.marker_control.close()\n\n    plus_button.observe(marker_button_click, \"value\")\n    minus_button.observe(marker_button_click, \"value\")\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def handle_map_interaction(**kwargs):\n        try:\n            if kwargs.get(\"type\") == \"click\":\n                # Skip if we're interacting with UI\n                if hasattr(m, \"_ui_interaction\") and m._ui_interaction:\n                    return\n                latlon = kwargs.get(\"coordinates\")\n                if radio_buttons.value == \"Foreground\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"green\",\n                            fill_color=\"green\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"plus-circle\",\n                                marker_color=\"green\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    fg_layer.add(marker)\n                    m.fg_markers.append(marker)\n                    fg_count.value = len(m.fg_markers)\n                elif radio_buttons.value == \"Background\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"red\",\n                            fill_color=\"red\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"minus-circle\",\n                                marker_color=\"red\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    bg_layer.add(marker)\n                    m.bg_markers.append(marker)\n                    bg_count.value = len(m.bg_markers)\n\n        except (TypeError, KeyError) as e:\n            print(f\"Error handling map interaction: {e}\")\n\n    m.on_interaction(handle_map_interaction)\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(m.fg_markers) == 0:\n                    print(\"Please add some foreground markers.\")\n                    segment_button.value = False\n                    return\n\n                else:\n                    try:\n                        fg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.fg_markers\n                        ]\n                        bg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.bg_markers\n                        ]\n                        point_coords = fg_points + bg_points\n                        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n\n                        filename = f\"masks_{random_string()}.tif\"\n                        filename = os.path.join(out_dir, filename)\n                        if sam.model_version == \"sam\":\n                            sam.predict(\n                                point_coords=point_coords,\n                                point_labels=point_labels,\n                                point_crs=\"EPSG:4326\",\n                                output=filename,\n                            )\n                        elif sam.model_version == \"sam2\":\n                            sam.predict_by_points(\n                                point_coords_batch=point_coords,\n                                point_labels_batch=point_labels,\n                                point_crs=\"EPSG:4326\",\n                                output=filename,\n                            )\n                        if m.find_layer(\"Masks\") is not None:\n                            m.remove_layer(m.find_layer(\"Masks\"))\n                        if m.find_layer(\"Regularized\") is not None:\n                            m.remove_layer(m.find_layer(\"Regularized\"))\n\n                        if hasattr(sam, \"prediction_fp\") and os.path.exists(\n                            sam.prediction_fp\n                        ):\n                            try:\n                                os.remove(sam.prediction_fp)\n                            except:\n                                pass\n                        # Skip the image layer if localtileserver is not available\n                        try:\n                            m.add_raster(\n                                filename,\n                                nodata=0,\n                                cmap=\"tab20\",\n                                opacity=opacity_slider.value,\n                                layer_name=\"Masks\",\n                                zoom_to_layer=False,\n                            )\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=\"Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                        except:\n                            pass\n                        output.clear_output()\n                        segment_button.value = False\n                        sam.prediction_fp = filename\n                    except Exception as e:\n                        segment_button.value = False\n                        print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.prediction_fp, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n\n                    fg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.fg_markers\n                    ]\n                    bg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.bg_markers\n                    ]\n\n                    coords_to_geojson(\n                        fg_points, filename.replace(\".tif\", \"_fg_markers.geojson\")\n                    )\n                    coords_to_geojson(\n                        bg_points, filename.replace(\".tif\", \"_bg_markers.geojson\")\n                    )\n\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                filechooser = FileChooser(\n                    path=os.getcwd(),\n                    filename=\"masks.tif\",\n                    sandbox_path=sandbox_path,\n                    layout=widgets.Layout(width=\"454px\"),\n                )\n                filechooser.use_dir_icons = True\n                filechooser.filter_pattern = [\"*.tif\"]\n                filechooser.register_callback(filechooser_callback)\n                save_control = ipyleaflet.WidgetControl(\n                    widget=filechooser, position=\"topright\"\n                )\n                m.add_control(save_control)\n                m.save_control = save_control\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.7\n            rectangular.value = False\n            colorpicker.value = \"#ffff00\"\n            output.clear_output()\n            try:\n                m.remove_layer(m.find_layer(\"Masks\"))\n                if m.find_layer(\"Regularized\") is not None:\n                    m.remove_layer(m.find_layer(\"Regularized\"))\n                m.clear_drawings()\n                if hasattr(m, \"fg_markers\"):\n                    m.user_rois = None\n                    m.fg_markers = []\n                    m.bg_markers = []\n                    m.fg_layer.clear_layers()\n                    m.bg_layer.clear_layers()\n                    fg_count.value = 0\n                    bg_count.value = 0\n                try:\n                    os.remove(sam.prediction_fp)\n                except:\n                    pass\n            except:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.show_canvas","title":"<code>show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/common.py</code> <pre><code>def show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        image = cv2.imread(image)\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be a URL or a NumPy array.\")\n\n    # Create an empty list to store the mouse click coordinates\n    left_clicks = []\n    right_clicks = []\n\n    # Create a mouse callback function\n    def get_mouse_coordinates(event, x, y):\n        if event == cv2.EVENT_LBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            left_clicks.append((x, y))\n\n            # Draw a green circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, fg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n        elif event == cv2.EVENT_RBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            right_clicks.append((x, y))\n\n            # Draw a red circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, bg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n    # Create a window to display the image\n    cv2.namedWindow(\"Image\")\n\n    # Set the mouse callback function for the window\n    cv2.setMouseCallback(\"Image\", get_mouse_coordinates)\n\n    # Display the image in the window\n    cv2.imshow(\"Image\", image)\n\n    # Wait for a key press to exit\n    cv2.waitKey(0)\n\n    # Destroy the window\n    cv2.destroyAllWindows()\n\n    return left_clicks, right_clicks\n</code></pre>"},{"location":"common/#samgeo.common.show_image_gui","title":"<code>show_image_gui(path)</code>","text":"<p>Show an interactive GUI to explore images. Args:     path (str): The path to the image file or directory containing images.</p> Source code in <code>samgeo/common.py</code> <pre><code>def show_image_gui(path: str) -&gt; None:\n    \"\"\"Show an interactive GUI to explore images.\n    Args:\n        path (str): The path to the image file or directory containing images.\n    \"\"\"\n\n    from PIL import Image\n\n    try:\n        import matplotlib\n        from ipywidgets import IntSlider, interact\n    except ImportError as e:\n        print(\n            f\"There was an error importing {e.name}, which is a dependency of leafmap. Install it with pip install leafmap or pip install segment-geospatial[all]\"\n        )\n\n    def setup_interactive_matplotlib():\n        \"\"\"Sets up ipympl backend for interactive plotting in Jupyter.\"\"\"\n        # Use the ipympl backend for interactive plotting\n        try:\n            import ipympl  # noqa: F401\n\n            matplotlib.use(\"module://ipympl.backend_nbagg\")\n        except ImportError:\n            print(\"ipympl is not installed. Falling back to default backend.\")\n\n    def load_images_from_folder(folder):\n        \"\"\"Load all images from the specified folder.\"\"\"\n        images = []\n        filenames = []\n        for filename in sorted(os.listdir(folder)):\n            if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n                img = Image.open(os.path.join(folder, filename))\n                img_array = np.array(img)\n                images.append(img_array)\n                filenames.append(filename)\n        return images, filenames\n\n    def load_single_image(image_path):\n        \"\"\"Load a single image from the specified image file path.\"\"\"\n        img = Image.open(image_path)\n        img_array = np.array(img)\n        return [img_array], [\n            os.path.basename(image_path)\n        ]  # Return as lists for consistency\n\n    # Check if the input path is a file or a directory\n    if os.path.isfile(path):\n        images, filenames = load_single_image(path)\n    elif os.path.isdir(path):\n        images, filenames = load_images_from_folder(path)\n    else:\n        print(\"Invalid path. Please provide a valid image file or directory.\")\n        return\n\n    total_images = len(images)\n\n    if total_images == 0:\n        print(\"No images found.\")\n        return\n\n    # Set up interactive plotting\n    import matplotlib.pyplot as plt\n\n    setup_interactive_matplotlib()\n\n    fig, ax = plt.subplots()\n    fig.canvas.toolbar_visible = True\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = True\n\n    # Display the first image initially\n    im_display = ax.imshow(images[0])\n    ax.set_title(f\"Image: {filenames[0]}\")\n    plt.tight_layout()\n\n    # Function to update the image when the slider changes (for multiple images)\n    def update_image(image_index):\n        im_display.set_data(images[image_index])\n        ax.set_title(f\"Image: {filenames[image_index]}\")\n        fig.canvas.draw()\n\n    # Function to show pixel information on click\n    def onclick(event):\n        if event.xdata is not None and event.ydata is not None:\n            col = int(event.xdata)\n            row = int(event.ydata)\n            pixel_value = images[current_image_index][\n                row, col\n            ]  # Use current image index\n            ax.set_title(\n                f\"Image: {filenames[current_image_index]} - X: {col}, Y: {row}, Pixel Value: {pixel_value}\"\n            )\n            fig.canvas.draw()\n\n    # Track the current image index (whether from slider or for single image)\n    current_image_index = 0\n\n    # Slider widget to choose between images (only if there is more than one image)\n    if total_images &gt; 1:\n        slider = IntSlider(min=0, max=total_images - 1, step=1, description=\"Image\")\n\n        def on_slider_change(change):\n            nonlocal current_image_index\n            current_image_index = change[\"new\"]  # Update current image index\n            update_image(current_image_index)\n\n        slider.observe(on_slider_change, names=\"value\")\n        fig.canvas.mpl_connect(\"button_press_event\", onclick)\n        interact(update_image, image_index=slider)\n    else:\n        # If there's only one image, no need for a slider, just show pixel info on click\n        fig.canvas.mpl_connect(\"button_press_event\", onclick)\n\n    # Show the plot\n    plt.show()\n</code></pre>"},{"location":"common/#samgeo.common.simplify","title":"<code>simplify(source, tolerance=0.01, output=None, **kwargs)</code>","text":"<p>Simplify a polygon GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | GeoDataFrame</code> <p>The input file path or a GeoDataFrame.</p> required <code>tolerance</code> <code>float</code> <p>The tolerance value for simplification. Defaults to 0.01.</p> <code>0.01</code> <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>gpd.GeoDataFrame: The output GeoDataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def simplify(source, tolerance=0.01, output=None, **kwargs):\n    \"\"\"Simplify a polygon GeoDataFrame.\n\n    Args:\n        source (str | gpd.GeoDataFrame): The input file path or a GeoDataFrame.\n        tolerance (float, optional): The tolerance value for simplification. Defaults to 0.01.\n        output (str, optional): The output file path. Defaults to None.\n\n    Returns:\n        gpd.GeoDataFrame: The output GeoDataFrame.\n    \"\"\"\n    if isinstance(source, str):\n        gdf = gpd.read_file(source)\n    elif isinstance(source, gpd.GeoDataFrame):\n        gdf = source\n    else:\n        raise ValueError(\"The input source must be a GeoDataFrame or a file path.\")\n\n    polygons = gdf.geometry.apply(\n        lambda geom: geom.simplify(tolerance, preserve_topology=True, **kwargs)\n    )\n    result = gpd.GeoDataFrame(geometry=polygons, data=gdf.drop(\"geometry\", axis=1))\n\n    if output is not None:\n        result.to_file(output)\n    else:\n        return result\n</code></pre>"},{"location":"common/#samgeo.common.split_raster","title":"<code>split_raster(filename, out_dir, tile_size=256, overlap=0)</code>","text":"<p>Split a raster into tiles.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path or http URL to the raster file.</p> required <code>out_dir</code> <code>str</code> <p>The path to the output directory.</p> required <code>tile_size</code> <code>int | tuple</code> <p>The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>The number of pixels to overlap between tiles. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def split_raster(filename, out_dir, tile_size=256, overlap=0):\n    \"\"\"Split a raster into tiles.\n\n    Args:\n        filename (str): The path or http URL to the raster file.\n        out_dir (str): The path to the output directory.\n        tile_size (int | tuple, optional): The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.\n        overlap (int, optional): The number of pixels to overlap between tiles. Defaults to 0.\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n\n    if isinstance(filename, str):\n        if filename.startswith(\"http\"):\n            output = filename.split(\"/\")[-1]\n            download_file(filename, output)\n            filename = output\n\n    # Open the input GeoTIFF file\n    ds = gdal.Open(filename)\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(tile_size, int):\n        tile_width = tile_size\n        tile_height = tile_size\n    elif isinstance(tile_size, tuple):\n        tile_width = tile_size[0]\n        tile_height = tile_size[1]\n\n    # Get the size of the input raster\n    width = ds.RasterXSize\n    height = ds.RasterYSize\n\n    # Calculate the number of tiles needed in both directions, taking into account the overlap\n    num_tiles_x = (width - overlap) // (tile_width - overlap) + int(\n        (width - overlap) % (tile_width - overlap) &gt; 0\n    )\n    num_tiles_y = (height - overlap) // (tile_height - overlap) + int(\n        (height - overlap) % (tile_height - overlap) &gt; 0\n    )\n\n    # Get the georeferencing information of the input raster\n    geotransform = ds.GetGeoTransform()\n\n    # Loop over all the tiles\n    for i in range(num_tiles_x):\n        for j in range(num_tiles_y):\n            # Calculate the pixel coordinates of the tile, taking into account the overlap and clamping to the edge of the raster\n            x_min = i * (tile_width - overlap)\n            y_min = j * (tile_height - overlap)\n            x_max = min(x_min + tile_width, width)\n            y_max = min(y_min + tile_height, height)\n\n            # Adjust the size of the last tile in each row and column to include any remaining pixels\n            if i == num_tiles_x - 1:\n                x_min = max(x_max - tile_width, 0)\n            if j == num_tiles_y - 1:\n                y_min = max(y_max - tile_height, 0)\n\n            # Calculate the size of the tile, taking into account the overlap\n            tile_width = x_max - x_min\n            tile_height = y_max - y_min\n\n            # Set the output file name\n            output_file = f\"{out_dir}/tile_{i}_{j}.tif\"\n\n            # Create a new dataset for the tile\n            driver = gdal.GetDriverByName(\"GTiff\")\n            tile_ds = driver.Create(\n                output_file,\n                tile_width,\n                tile_height,\n                ds.RasterCount,\n                ds.GetRasterBand(1).DataType,\n            )\n\n            # Calculate the georeferencing information for the output tile\n            tile_geotransform = (\n                geotransform[0] + x_min * geotransform[1],\n                geotransform[1],\n                0,\n                geotransform[3] + y_min * geotransform[5],\n                0,\n                geotransform[5],\n            )\n\n            # Set the geotransform and projection of the tile\n            tile_ds.SetGeoTransform(tile_geotransform)\n            tile_ds.SetProjection(ds.GetProjection())\n\n            # Read the data from the input raster band(s) and write it to the tile band(s)\n            for k in range(ds.RasterCount):\n                band = ds.GetRasterBand(k + 1)\n                tile_band = tile_ds.GetRasterBand(k + 1)\n                tile_data = band.ReadAsArray(x_min, y_min, tile_width, tile_height)\n                tile_band.WriteArray(tile_data)\n\n            # Close the tile dataset\n            tile_ds = None\n\n    # Close the input dataset\n    ds = None\n</code></pre>"},{"location":"common/#samgeo.common.temp_file_path","title":"<code>temp_file_path(extension)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The temporary file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def temp_file_path(extension):\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        extension (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not extension.startswith(\".\"):\n        extension = \".\" + extension\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{extension}\")\n\n    return file_path\n</code></pre>"},{"location":"common/#samgeo.common.text_sam_gui","title":"<code>text_sam_gui(sam, basemap='SATELLITE', out_dir=None, box_threshold=0.25, text_threshold=0.25, cmap='viridis', opacity=0.7, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def text_sam_gui(\n    sam,\n    basemap=\"SATELLITE\",\n    out_dir=None,\n    box_threshold=0.25,\n    text_threshold=0.25,\n    cmap=\"viridis\",\n    opacity=0.7,\n    **kwargs,\n):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo):\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        out_dir (str, optional): The output directory. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n\n        import ipyevents\n        import ipyleaflet\n        import ipywidgets as widgets\n        import leafmap\n        import leafmap.colormaps as cm\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first.\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(**kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except:\n        pass\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 4px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    text_prompt = widgets.Text(\n        description=\"Text prompt:\",\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    box_slider = widgets.FloatSlider(\n        description=\"Box threshold:\",\n        min=0,\n        max=1,\n        value=box_threshold,\n        step=0.01,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    text_slider = widgets.FloatSlider(\n        description=\"Text threshold:\",\n        min=0,\n        max=1,\n        step=0.01,\n        value=text_threshold,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    cmap_dropdown = widgets.Dropdown(\n        description=\"Palette:\",\n        options=cm.list_colormaps(),\n        value=cmap,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Opacity:\",\n        min=0,\n        max=1,\n        value=opacity,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            if hasattr(m, \"layer_name\"):\n                mask_layer = m.find_layer(m.layer_name)\n                if mask_layer is not None:\n                    mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        text_prompt,\n        box_slider,\n        text_slider,\n        cmap_dropdown,\n        opacity_slider,\n        widgets.HBox([rectangular, colorpicker]),\n        widgets.HBox(\n            [segment_button, save_button, reset_button],\n            layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n        ),\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(text_prompt.value) == 0:\n                    print(\"Please enter a text prompt first.\")\n                elif sam.source is None:\n                    print(\"Please run sam.set_image() first.\")\n                else:\n                    print(\"Segmenting...\")\n                    layer_name = text_prompt.value.replace(\" \", \"_\")\n                    filename = os.path.join(\n                        out_dir, f\"{layer_name}_{random_string()}.tif\"\n                    )\n                    try:\n                        sam.predict(\n                            sam.source,\n                            text_prompt.value,\n                            box_slider.value,\n                            text_slider.value,\n                            output=filename,\n                        )\n                        sam.output = filename\n                        if m.find_layer(layer_name) is not None:\n                            m.remove_layer(m.find_layer(layer_name))\n                        if m.find_layer(f\"{layer_name}_rect\") is not None:\n                            m.remove_layer(m.find_layer(f\"{layer_name} Regularized\"))\n                    except Exception as e:\n                        output.clear_output()\n                        print(e)\n                    if os.path.exists(filename):\n                        try:\n                            m.add_raster(\n                                filename,\n                                layer_name=layer_name,\n                                palette=cmap_dropdown.value,\n                                opacity=opacity_slider.value,\n                                nodata=0,\n                                zoom_to_layer=False,\n                            )\n                            m.layer_name = layer_name\n\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=f\"{layer_name} Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                            output.clear_output()\n                        except Exception as e:\n                            print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.output, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                output.clear_output()\n                if not hasattr(m, \"layer_name\"):\n                    print(\"Please click the Segment button first.\")\n                else:\n                    sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                    filechooser = FileChooser(\n                        path=os.getcwd(),\n                        filename=f\"{m.layer_name}.tif\",\n                        sandbox_path=sandbox_path,\n                        layout=widgets.Layout(width=\"454px\"),\n                    )\n                    filechooser.use_dir_icons = True\n                    filechooser.filter_pattern = [\"*.tif\"]\n                    filechooser.register_callback(filechooser_callback)\n                    save_control = ipyleaflet.WidgetControl(\n                        widget=filechooser, position=\"topright\"\n                    )\n                    m.add_control(save_control)\n                    m.save_control = save_control\n\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            save_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.7\n            box_slider.value = 0.25\n            text_slider.value = 0.25\n            cmap_dropdown.value = \"viridis\"\n            text_prompt.value = \"\"\n            output.clear_output()\n            try:\n                if hasattr(m, \"layer_name\") and m.find_layer(m.layer_name) is not None:\n                    m.remove_layer(m.find_layer(m.layer_name))\n                m.clear_drawings()\n            except:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.tms_to_geotiff","title":"<code>tms_to_geotiff(output, bbox, zoom=None, resolution=None, source='OpenStreetMap', crs='EPSG:3857', to_cog=False, return_image=False, overwrite=False, quiet=False, **kwargs)</code>","text":"<p>Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.     Credits to the GitHub user @gumblex.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output GeoTIFF file.</p> required <code>bbox</code> <code>list</code> <p>The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]</p> required <code>zoom</code> <code>int</code> <p>The map zoom level. Defaults to None.</p> <code>None</code> <code>resolution</code> <code>float</code> <p>The resolution in meters. Defaults to None.</p> <code>None</code> <code>source</code> <code>str</code> <p>The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>crs</code> <code>str</code> <p>The output CRS. Defaults to \"EPSG:3857\".</p> <code>'EPSG:3857'</code> <code>to_cog</code> <code>bool</code> <p>Convert to Cloud Optimized GeoTIFF. Defaults to False.</p> <code>False</code> <code>return_image</code> <code>bool</code> <p>Return the image as PIL.Image. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the output file if it already exists. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>Suppress output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def tms_to_geotiff(\n    output,\n    bbox,\n    zoom=None,\n    resolution=None,\n    source=\"OpenStreetMap\",\n    crs=\"EPSG:3857\",\n    to_cog=False,\n    return_image=False,\n    overwrite=False,\n    quiet=False,\n    **kwargs,\n):\n    \"\"\"Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.\n        Credits to the GitHub user @gumblex.\n\n    Args:\n        output (str): The output GeoTIFF file.\n        bbox (list): The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]\n        zoom (int, optional): The map zoom level. Defaults to None.\n        resolution (float, optional): The resolution in meters. Defaults to None.\n        source (str, optional): The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\",\n            \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".\n        crs (str, optional): The output CRS. Defaults to \"EPSG:3857\".\n        to_cog (bool, optional): Convert to Cloud Optimized GeoTIFF. Defaults to False.\n        return_image (bool, optional): Return the image as PIL.Image. Defaults to False.\n        overwrite (bool, optional): Overwrite the output file if it already exists. Defaults to False.\n        quiet (bool, optional): Suppress output. Defaults to False.\n        **kwargs: Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().\n\n    \"\"\"\n\n    import concurrent.futures\n    import io\n    import itertools\n    import math\n    import re\n\n    from PIL import Image\n\n    try:\n        from osgeo import gdal, osr\n    except ImportError:\n        raise ImportError(\"GDAL is not installed. Install it with pip install GDAL\")\n\n    try:\n        import httpx\n\n        SESSION = httpx.Client()\n    except ImportError:\n        import requests\n\n        SESSION = requests.Session()\n\n    if not overwrite and os.path.exists(output):\n        print(\n            f\"The output file {output} already exists. Use `overwrite=True` to overwrite it.\"\n        )\n        return\n\n    xyz_tiles = {\n        \"OPENSTREETMAP\": \"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n        \"ROADMAP\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"SATELLITE\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"TERRAIN\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"HYBRID\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n    }\n\n    basemaps = get_basemaps()\n\n    if isinstance(source, str):\n        if source.upper() in xyz_tiles:\n            source = xyz_tiles[source.upper()]\n        elif source in basemaps:\n            source = basemaps[source]\n        elif source.startswith(\"http\"):\n            pass\n    else:\n        raise ValueError(\n            'source must be one of \"OpenStreetMap\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or a URL'\n        )\n\n    def resolution_to_zoom_level(resolution):\n        \"\"\"\n        Convert map resolution in meters to zoom level for Web Mercator (EPSG:3857) tiles.\n        \"\"\"\n        # Web Mercator tile size in meters at zoom level 0\n        initial_resolution = 156543.03392804097\n\n        # Calculate the zoom level\n        zoom_level = math.log2(initial_resolution / resolution)\n\n        return int(zoom_level)\n\n    if isinstance(bbox, list) and len(bbox) == 4:\n        west, south, east, north = bbox\n    else:\n        raise ValueError(\n            \"bbox must be a list of 4 coordinates in the format of [xmin, ymin, xmax, ymax]\"\n        )\n\n    if zoom is None and resolution is None:\n        raise ValueError(\"Either zoom or resolution must be provided\")\n    elif zoom is not None and resolution is not None:\n        raise ValueError(\"Only one of zoom or resolution can be provided\")\n\n    if resolution is not None:\n        zoom = resolution_to_zoom_level(resolution)\n\n    EARTH_EQUATORIAL_RADIUS = 6378137.0\n\n    Image.MAX_IMAGE_PIXELS = None\n\n    gdal.UseExceptions()\n    web_mercator = osr.SpatialReference()\n    try:\n        web_mercator.ImportFromEPSG(3857)\n    except RuntimeError as e:\n        # https://github.com/PDAL/PDAL/issues/2544#issuecomment-637995923\n        if \"PROJ\" in str(e):\n            pattern = r\"/[\\w/]+\"\n            match = re.search(pattern, str(e))\n            if match:\n                file_path = match.group(0)\n                os.environ[\"PROJ_LIB\"] = file_path\n                os.environ[\"GDAL_DATA\"] = file_path.replace(\"proj\", \"gdal\")\n                web_mercator.ImportFromEPSG(3857)\n\n    WKT_3857 = web_mercator.ExportToWkt()\n\n    def from4326_to3857(lat, lon):\n        xtile = math.radians(lon) * EARTH_EQUATORIAL_RADIUS\n        ytile = (\n            math.log(math.tan(math.radians(45 + lat / 2.0))) * EARTH_EQUATORIAL_RADIUS\n        )\n        return (xtile, ytile)\n\n    def deg2num(lat, lon, zoom):\n        lat_r = math.radians(lat)\n        n = 2**zoom\n        xtile = (lon + 180) / 360 * n\n        ytile = (1 - math.log(math.tan(lat_r) + 1 / math.cos(lat_r)) / math.pi) / 2 * n\n        return (xtile, ytile)\n\n    def is_empty(im):\n        extrema = im.getextrema()\n        if len(extrema) &gt;= 3:\n            if len(extrema) &gt; 3 and extrema[-1] == (0, 0):\n                return True\n            for ext in extrema[:3]:\n                if ext != (0, 0):\n                    return False\n            return True\n        else:\n            return extrema[0] == (0, 0)\n\n    def paste_tile(bigim, base_size, tile, corner_xy, bbox):\n        if tile is None:\n            return bigim\n        im = Image.open(io.BytesIO(tile))\n        mode = \"RGB\" if im.mode == \"RGB\" else \"RGBA\"\n        size = im.size\n        if bigim is None:\n            base_size[0] = size[0]\n            base_size[1] = size[1]\n            newim = Image.new(\n                mode, (size[0] * (bbox[2] - bbox[0]), size[1] * (bbox[3] - bbox[1]))\n            )\n        else:\n            newim = bigim\n\n        dx = abs(corner_xy[0] - bbox[0])\n        dy = abs(corner_xy[1] - bbox[1])\n        xy0 = (size[0] * dx, size[1] * dy)\n        if mode == \"RGB\":\n            newim.paste(im, xy0)\n        else:\n            if im.mode != mode:\n                im = im.convert(mode)\n            if not is_empty(im):\n                newim.paste(im, xy0)\n        im.close()\n        return newim\n\n    def finish_picture(bigim, base_size, bbox, x0, y0, x1, y1):\n        xfrac = x0 - bbox[0]\n        yfrac = y0 - bbox[1]\n        x2 = round(base_size[0] * xfrac)\n        y2 = round(base_size[1] * yfrac)\n        imgw = round(base_size[0] * (x1 - x0))\n        imgh = round(base_size[1] * (y1 - y0))\n        retim = bigim.crop((x2, y2, x2 + imgw, y2 + imgh))\n        if retim.mode == \"RGBA\" and retim.getextrema()[3] == (255, 255):\n            retim = retim.convert(\"RGB\")\n        bigim.close()\n        return retim\n\n    def get_tile(url):\n        retry = 3\n        while 1:\n            try:\n                r = SESSION.get(url, timeout=60)\n                break\n            except Exception:\n                retry -= 1\n                if not retry:\n                    raise\n        if r.status_code == 404:\n            return None\n        elif not r.content:\n            return None\n        r.raise_for_status()\n        return r.content\n\n    def draw_tile(\n        source, lat0, lon0, lat1, lon1, zoom, filename, quiet=False, **kwargs\n    ):\n        x0, y0 = deg2num(lat0, lon0, zoom)\n        x1, y1 = deg2num(lat1, lon1, zoom)\n        x0, x1 = sorted([x0, x1])\n        y0, y1 = sorted([y0, y1])\n        corners = tuple(\n            itertools.product(\n                range(math.floor(x0), math.ceil(x1)),\n                range(math.floor(y0), math.ceil(y1)),\n            )\n        )\n        totalnum = len(corners)\n        futures = []\n        with concurrent.futures.ThreadPoolExecutor(5) as executor:\n            for x, y in corners:\n                futures.append(\n                    executor.submit(get_tile, source.format(z=zoom, x=x, y=y))\n                )\n            bbox = (math.floor(x0), math.floor(y0), math.ceil(x1), math.ceil(y1))\n            bigim = None\n            base_size = [256, 256]\n            for k, (fut, corner_xy) in enumerate(zip(futures, corners), 1):\n                bigim = paste_tile(bigim, base_size, fut.result(), corner_xy, bbox)\n                if not quiet:\n                    print(\n                        f\"Downloaded image {str(k).zfill(len(str(totalnum)))}/{totalnum}\"\n                    )\n\n        if not quiet:\n            print(\"Saving GeoTIFF. Please wait...\")\n        img = finish_picture(bigim, base_size, bbox, x0, y0, x1, y1)\n        imgbands = len(img.getbands())\n        driver = gdal.GetDriverByName(\"GTiff\")\n\n        if \"options\" not in kwargs:\n            kwargs[\"options\"] = [\n                \"COMPRESS=DEFLATE\",\n                \"PREDICTOR=2\",\n                \"ZLEVEL=9\",\n                \"TILED=YES\",\n            ]\n\n        gtiff = driver.Create(\n            filename,\n            img.size[0],\n            img.size[1],\n            imgbands,\n            gdal.GDT_Byte,\n            **kwargs,\n        )\n        xp0, yp0 = from4326_to3857(lat0, lon0)\n        xp1, yp1 = from4326_to3857(lat1, lon1)\n        pwidth = abs(xp1 - xp0) / img.size[0]\n        pheight = abs(yp1 - yp0) / img.size[1]\n        gtiff.SetGeoTransform((min(xp0, xp1), pwidth, 0, max(yp0, yp1), 0, -pheight))\n        gtiff.SetProjection(WKT_3857)\n        for band in range(imgbands):\n            array = np.array(img.getdata(band), dtype=\"u8\")\n            array = array.reshape((img.size[1], img.size[0]))\n            band = gtiff.GetRasterBand(band + 1)\n            band.WriteArray(array)\n        gtiff.FlushCache()\n\n        if not quiet:\n            print(f\"Image saved to {filename}\")\n        return img\n\n    try:\n        image = draw_tile(\n            source, south, west, north, east, zoom, output, quiet, **kwargs\n        )\n        if return_image:\n            return image\n        if crs.upper() != \"EPSG:3857\":\n            reproject(output, output, crs, to_cog=to_cog)\n        elif to_cog:\n            image_to_cog(output, output)\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.transform_coords","title":"<code>transform_coords(x, y, src_crs, dst_crs, **kwargs)</code>","text":"<p>Transform coordinates from one CRS to another.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The x coordinate.</p> required <code>y</code> <code>float</code> <p>The y coordinate.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS, e.g., \"EPSG:4326\".</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS, e.g., \"EPSG:3857\".</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The transformed coordinates in the format of (x, y)</p> Source code in <code>samgeo/common.py</code> <pre><code>def transform_coords(x, y, src_crs, dst_crs, **kwargs):\n    \"\"\"Transform coordinates from one CRS to another.\n\n    Args:\n        x (float): The x coordinate.\n        y (float): The y coordinate.\n        src_crs (str): The source CRS, e.g., \"EPSG:4326\".\n        dst_crs (str): The destination CRS, e.g., \"EPSG:3857\".\n\n    Returns:\n        dict: The transformed coordinates in the format of (x, y)\n    \"\"\"\n    transformer = pyproj.Transformer.from_crs(\n        src_crs, dst_crs, always_xy=True, **kwargs\n    )\n    return transformer.transform(x, y)\n</code></pre>"},{"location":"common/#samgeo.common.update_package","title":"<code>update_package(out_dir=None, keep=False, **kwargs)</code>","text":"<p>Updates the package from the GitHub repository without the need to use pip or conda.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the downloaded package. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the download_file() function.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def update_package(out_dir=None, keep=False, **kwargs):\n    \"\"\"Updates the package from the GitHub repository without the need to use pip or conda.\n\n    Args:\n        out_dir (str, optional): The output directory. Defaults to None.\n        keep (bool, optional): Whether to keep the downloaded package. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the download_file() function.\n    \"\"\"\n\n    import shutil\n\n    try:\n        if out_dir is None:\n            out_dir = os.getcwd()\n        url = (\n            \"https://github.com/opengeos/segment-geospatial/archive/refs/heads/main.zip\"\n        )\n        filename = \"segment-geospatial-main.zip\"\n        download_file(url, filename, **kwargs)\n\n        pkg_dir = os.path.join(out_dir, \"segment-geospatial-main\")\n        work_dir = os.getcwd()\n        os.chdir(pkg_dir)\n\n        if shutil.which(\"pip\") is None:\n            cmd = \"pip3 install .\"\n        else:\n            cmd = \"pip install .\"\n\n        os.system(cmd)\n        os.chdir(work_dir)\n\n        if not keep:\n            shutil.rmtree(pkg_dir)\n            try:\n                os.remove(filename)\n            except:\n                pass\n\n        print(\"Package updated successfully.\")\n\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The geojson dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def vector_to_geojson(filename, output=None, **kwargs):\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"common/#samgeo.common.video_to_images","title":"<code>video_to_images(video_path, output_dir, frame_rate=None, prefix='')</code>","text":"<p>Converts a video into a series of images. Each frame of the video is saved as an image.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the images will be saved.</p> required <code>frame_rate</code> <code>Optional[int]</code> <p>The number of frames to save per second of video. If None, all frames will be saved. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix for the output image filenames. Defaults to 'frame_'.</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the video file cannot be read or if the output directory is invalid.</p> Example usage <p>video_to_images('input_video.mp4', 'output_images', frame_rate=1, prefix='image_')</p> Source code in <code>samgeo/common.py</code> <pre><code>def video_to_images(\n    video_path: str,\n    output_dir: str,\n    frame_rate: Optional[int] = None,\n    prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Converts a video into a series of images. Each frame of the video is saved as an image.\n\n    Args:\n        video_path (str): The path to the video file.\n        output_dir (str): The directory where the images will be saved.\n        frame_rate (Optional[int], optional): The number of frames to save per second of video.\n            If None, all frames will be saved. Defaults to None.\n        prefix (str, optional): The prefix for the output image filenames. Defaults to 'frame_'.\n\n    Raises:\n        ValueError: If the video file cannot be read or if the output directory is invalid.\n\n    Example usage:\n        video_to_images('input_video.mp4', 'output_images', frame_rate=1, prefix='image_')\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Error opening video file {video_path}\")\n\n    # Get video properties\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_rate = (\n        frame_rate if frame_rate else video_fps\n    )  # Default to original FPS if not provided\n\n    # Calculate the number of digits based on the total frames (e.g., if total frames are 1000, width = 4)\n    num_digits = len(str(total_frames))\n\n    print(f\"Video FPS: {video_fps}\")\n    print(f\"Total Frames: {total_frames}\")\n    print(f\"Saving every {video_fps // frame_rate} frame(s)\")\n\n    frame_count = 0\n    saved_frame_count = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Save frames based on frame_rate\n        if frame_count % (video_fps // frame_rate) == 0:\n            img_path = os.path.join(\n                output_dir, f\"{prefix}{saved_frame_count:0{num_digits}d}.jpg\"\n            )\n            cv2.imwrite(img_path, frame)\n            saved_frame_count += 1\n            # print(f\"Saved {img_path}\")\n\n        frame_count += 1\n\n    # Release the video capture object\n    cap.release()\n    print(f\"Finished saving {saved_frame_count} images to {output_dir}\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>segment-geospatial could always use more documentation, whether as part of the official segment-geospatial docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up segment-geospatial for local development.</p> <ol> <li> <p>Fork the segment-geospatial repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/segment-geospatial.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv segment-geospatial\n$ cd segment-geospatial/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 segment-geospatial tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests (see the section below - Unit Testing).</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.md.</li> <li>The pull request should work for Python 3.8, 3.9, 3.10, and 3.11. Check https://github.com/giswqs/segment-geospatial/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"contributing/#unit-testing","title":"Unit Testing","text":"<p>Unit tests are in the <code>tests</code> folder. If you add new functionality to the package, please add a unit test for it. You can either add the test to an existing test file or create a new one. For example, if you add a new function to <code>samgeo/samgeo.py</code>, you can add the unit test to <code>tests/test_samgeo.py</code>. If you add a new module to <code>samgeo/&lt;MODULE-NAME&gt;</code>, you can create a new test file in <code>tests/test_&lt;MODULE-NAME&gt;</code>. Please refer to <code>tests/test_samgeo.py</code> for examples. For more information about unit testing, please refer to this tutorial - Getting Started With Testing in Python.</p> <p>To run the unit tests, navigate to the root directory of the package and run the following command:</p> <pre><code>python -m unittest discover tests/\n</code></pre>"},{"location":"contributing/#add-new-dependencies","title":"Add new dependencies","text":"<p>If you PR involves adding new dependencies, please make sure that the new dependencies are available on both PyPI and conda-forge. Search here to see if the package is available on conda-forge. If the package is not available on conda-forge, it can't be added as a required dependency in <code>requirements.txt</code>. Instead, it should be added as an optional dependency in <code>requirements_dev.txt</code>.</p> <p>If the package is available on PyPI and conda-forge, but if it is challenging to install the package on some operating systems, we would recommend adding the package as an optional dependency in <code>requirements_dev.txt</code> rather than a required dependency in <code>requirements.txt</code>.</p> <p>The dependencies required for building the documentation should be added to <code>requirements_docs.txt</code>. In most cases, contributors do not need to add new dependencies to <code>requirements_docs.txt</code> unless the documentation fails to build due to missing dependencies.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"fast_sam/","title":"fast_sam module","text":"<p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM. https://github.com/opengeos/FastSAM</p>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo","title":"<code>SamGeo</code>","text":"<p>               Bases: <code>FastSAM</code></p> <p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>class SamGeo(FastSAM):\n    \"\"\"Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).\"\"\"\n\n    def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n        \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n        if \"checkpoint_dir\" in kwargs:\n            checkpoint_dir = kwargs[\"checkpoint_dir\"]\n            kwargs.pop(\"checkpoint_dir\")\n        else:\n            checkpoint_dir = os.environ.get(\n                \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n            )\n\n        models = {\n            \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n            \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n        }\n\n        if model not in models:\n            raise ValueError(\n                f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n            )\n\n        model_path = os.path.join(checkpoint_dir, model)\n\n        if not os.path.exists(model_path):\n            print(f\"Downloading {model} to {model_path}...\")\n            download_file(models[model], model_path)\n\n        super().__init__(model, **kwargs)\n\n    def set_image(self, image, device=None, **kwargs):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n            device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n            kwargs: Additional keyword arguments to pass to the FastSAM model.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        everything_results = self(image, device=device, **kwargs)\n\n        self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n\n    def everything_prompt(self, output=None, **kwargs):\n        \"\"\"Segment the image with the everything prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n        Args:\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.everything_prompt()\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def point_prompt(self, points, pointlabel, output=None, **kwargs):\n        \"\"\"Segment the image with the point prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n        Args:\n            points (list): A list of points.\n            pointlabel (list): A list of labels for each point.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.point_prompt(points, pointlabel)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n        \"\"\"Segment the image with the box prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n        Args:\n            bbox (list, optional): The bounding box. Defaults to None.\n            bboxes (list, optional): A list of bounding boxes. Defaults to None.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.box_prompt(bbox, bboxes)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def text_prompt(self, text, output=None, **kwargs):\n        \"\"\"Segment the image with the text prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n        Args:\n            text (str): The text to segment.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.text_prompt(text)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def save_masks(\n        self,\n        output=None,\n        better_quality=True,\n        dtype=None,\n        mask_multiplier=255,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Save the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n        annotations = self.annotations\n        if isinstance(annotations[0], dict):\n            annotations = [annotation[\"segmentation\"] for annotation in annotations]\n        image = self.prompt_process.img\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        height = image.shape[0]\n        width = image.shape[1]\n\n        if better_quality:\n            if isinstance(annotations[0], torch.Tensor):\n                annotations = np.array(annotations.cpu())\n            for i, mask in enumerate(annotations):\n                mask = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n                )\n                annotations[i] = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n                )\n        if self.device == \"cpu\":\n            annotations = np.array(annotations)\n\n        else:\n            if isinstance(annotations[0], np.ndarray):\n                annotations = torch.from_numpy(annotations)\n\n        if isinstance(annotations, torch.Tensor):\n            annotations = annotations.cpu().numpy()\n\n        if dtype is None:\n            # Set output image data type based on the number of objects\n            if len(annotations) &lt; 255:\n                dtype = np.uint8\n            elif len(annotations) &lt; 65535:\n                dtype = np.uint16\n            else:\n                dtype = np.uint32\n\n        masks = np.sum(annotations, axis=0)\n\n        masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n        masks[masks &gt; 0] = 1\n        masks = masks.astype(dtype) * mask_multiplier\n        self.objects = masks\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n        else:\n            return masks\n\n    def fast_show_mask(\n        self,\n        random_color=False,\n    ):\n        \"\"\"Show the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Args:\n            random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n        image = self.prompt_process.img\n        target_height = image.shape[0]\n        target_width = image.shape[1]\n        annotations = self.annotations\n        annotation = np.array(annotations.cpu())\n\n        mask_sum = annotation.shape[0]\n        height = annotation.shape[1]\n        weight = annotation.shape[2]\n        # Sort annotations based on area.\n        areas = np.sum(annotation, axis=(1, 2))\n        sorted_indices = np.argsort(areas)\n        annotation = annotation[sorted_indices]\n\n        index = (annotation != 0).argmax(axis=0)\n        if random_color:\n            color = np.random.random((mask_sum, 1, 1, 3))\n        else:\n            color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n                [30 / 255, 144 / 255, 255 / 255]\n            )\n        transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n        visual = np.concatenate([color, transparency], axis=-1)\n        mask_image = np.expand_dims(annotation, -1) * visual\n\n        show = np.zeros((height, weight, 4))\n        h_indices, w_indices = np.meshgrid(\n            np.arange(height), np.arange(weight), indexing=\"ij\"\n        )\n        indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n        # Use vectorized indexing to update the values of 'show'.\n        show[h_indices, w_indices, :] = mask_image[indices]\n\n        show = cv2.resize(\n            show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n        )\n\n        return show\n\n    def raster_to_vector(\n        self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n    ):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            image,\n            output,\n            simplify_tolerance=simplify_tolerance,\n            dst_crs=dst_crs,\n            **kwargs,\n        )\n\n    def show_anns(\n        self,\n        output=None,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        annotations = self.annotations\n        prompt_process = self.prompt_process\n\n        if output is None:\n            output = temp_file_path(\".png\")\n\n        prompt_process.plot(annotations, output, **kwargs)\n\n        show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.__init__","title":"<code>__init__(model='FastSAM-x.pt', **kwargs)</code>","text":"<p>Initialize the FastSAM algorithm.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n    \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n    if \"checkpoint_dir\" in kwargs:\n        checkpoint_dir = kwargs[\"checkpoint_dir\"]\n        kwargs.pop(\"checkpoint_dir\")\n    else:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    models = {\n        \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n        \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n    }\n\n    if model not in models:\n        raise ValueError(\n            f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n        )\n\n    model_path = os.path.join(checkpoint_dir, model)\n\n    if not os.path.exists(model_path):\n        print(f\"Downloading {model} to {model_path}...\")\n        download_file(models[model], model_path)\n\n    super().__init__(model, **kwargs)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.box_prompt","title":"<code>box_prompt(bbox=None, bboxes=None, output=None, **kwargs)</code>","text":"<p>Segment the image with the box prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list</code> <p>The bounding box. Defaults to None.</p> <code>None</code> <code>bboxes</code> <code>list</code> <p>A list of bounding boxes. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n    \"\"\"Segment the image with the box prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n    Args:\n        bbox (list, optional): The bounding box. Defaults to None.\n        bboxes (list, optional): A list of bounding boxes. Defaults to None.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.box_prompt(bbox, bboxes)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.everything_prompt","title":"<code>everything_prompt(output=None, **kwargs)</code>","text":"<p>Segment the image with the everything prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def everything_prompt(self, output=None, **kwargs):\n    \"\"\"Segment the image with the everything prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n    Args:\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.everything_prompt()\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.fast_show_mask","title":"<code>fast_show_mask(random_color=False)</code>","text":"<p>Show the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Parameters:</p> Name Type Description Default <code>random_color</code> <code>bool</code> <p>Whether to use random colors for each object. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>np.ndarray: The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def fast_show_mask(\n    self,\n    random_color=False,\n):\n    \"\"\"Show the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Args:\n        random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n    image = self.prompt_process.img\n    target_height = image.shape[0]\n    target_width = image.shape[1]\n    annotations = self.annotations\n    annotation = np.array(annotations.cpu())\n\n    mask_sum = annotation.shape[0]\n    height = annotation.shape[1]\n    weight = annotation.shape[2]\n    # Sort annotations based on area.\n    areas = np.sum(annotation, axis=(1, 2))\n    sorted_indices = np.argsort(areas)\n    annotation = annotation[sorted_indices]\n\n    index = (annotation != 0).argmax(axis=0)\n    if random_color:\n        color = np.random.random((mask_sum, 1, 1, 3))\n    else:\n        color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n            [30 / 255, 144 / 255, 255 / 255]\n        )\n    transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n    visual = np.concatenate([color, transparency], axis=-1)\n    mask_image = np.expand_dims(annotation, -1) * visual\n\n    show = np.zeros((height, weight, 4))\n    h_indices, w_indices = np.meshgrid(\n        np.arange(height), np.arange(weight), indexing=\"ij\"\n    )\n    indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n    # Use vectorized indexing to update the values of 'show'.\n    show[h_indices, w_indices, :] = mask_image[indices]\n\n    show = cv2.resize(\n        show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n    )\n\n    return show\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.point_prompt","title":"<code>point_prompt(points, pointlabel, output=None, **kwargs)</code>","text":"<p>Segment the image with the point prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list</code> <p>A list of points.</p> required <code>pointlabel</code> <code>list</code> <p>A list of labels for each point.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def point_prompt(self, points, pointlabel, output=None, **kwargs):\n    \"\"\"Segment the image with the point prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n    Args:\n        points (list): A list of points.\n        pointlabel (list): A list of labels for each point.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.point_prompt(points, pointlabel)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def raster_to_vector(\n    self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        image,\n        output,\n        simplify_tolerance=simplify_tolerance,\n        dst_crs=dst_crs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.save_masks","title":"<code>save_masks(output=None, better_quality=True, dtype=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    better_quality=True,\n    dtype=None,\n    mask_multiplier=255,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Save the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n    annotations = self.annotations\n    if isinstance(annotations[0], dict):\n        annotations = [annotation[\"segmentation\"] for annotation in annotations]\n    image = self.prompt_process.img\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    height = image.shape[0]\n    width = image.shape[1]\n\n    if better_quality:\n        if isinstance(annotations[0], torch.Tensor):\n            annotations = np.array(annotations.cpu())\n        for i, mask in enumerate(annotations):\n            mask = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n            )\n            annotations[i] = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n            )\n    if self.device == \"cpu\":\n        annotations = np.array(annotations)\n\n    else:\n        if isinstance(annotations[0], np.ndarray):\n            annotations = torch.from_numpy(annotations)\n\n    if isinstance(annotations, torch.Tensor):\n        annotations = annotations.cpu().numpy()\n\n    if dtype is None:\n        # Set output image data type based on the number of objects\n        if len(annotations) &lt; 255:\n            dtype = np.uint8\n        elif len(annotations) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n    masks = np.sum(annotations, axis=0)\n\n    masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n    masks[masks &gt; 0] = 1\n    masks = masks.astype(dtype) * mask_multiplier\n    self.objects = masks\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n    else:\n        return masks\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.set_image","title":"<code>set_image(image, device=None, **kwargs)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the FastSAM model.</p> <code>{}</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def set_image(self, image, device=None, **kwargs):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n        device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n        kwargs: Additional keyword arguments to pass to the FastSAM model.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    everything_results = self(image, device=device, **kwargs)\n\n    self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.show_anns","title":"<code>show_anns(output=None, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> required <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> required Source code in <code>samgeo/fast_sam.py</code> <pre><code>def show_anns(\n    self,\n    output=None,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    annotations = self.annotations\n    prompt_process = self.prompt_process\n\n    if output is None:\n        output = temp_file_path(\".png\")\n\n    prompt_process.plot(annotations, output, **kwargs)\n\n    show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.text_prompt","title":"<code>text_prompt(text, output=None, **kwargs)</code>","text":"<p>Segment the image with the text prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to segment.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def text_prompt(self, text, output=None, **kwargs):\n    \"\"\"Segment the image with the text prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n    Args:\n        text (str): The text to segment.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.text_prompt(text)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"hq_sam/","title":"hq_sam module","text":"<p>Segment remote sensing imagery with HQ-SAM (High Quality Segment Anything Model). See https://github.com/SysCV/sam-hq</p>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo","title":"<code>SamGeo</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        hq=False,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n\n        hq = True  # Using HQ-SAM\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n            )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in ascending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                objects[m] = index + 1\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__call__","title":"<code>__call__(image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__init__","title":"<code>__init__(model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, hq=False, sam_kwargs=None, **kwargs)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use the HQ-SAM model. Defaults to False.</p> <code>False</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    hq=False,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n\n    hq = True  # Using HQ-SAM\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache()</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.generate","title":"<code>generate(source, output=None, foreground=True, batch=False, erosion_kernel=None, mask_multiplier=255, unique=True, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in ascending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            objects[m] = index + 1\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype=np.float32, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>float32</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.set_image","title":"<code>set_image(image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: The predicted mask as a numpy array.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>segment-geospatial is available on PyPI. To install segment-geospatial, run this command in your terminal:</p> <pre><code>pip install segment-geospatial\n</code></pre>"},{"location":"installation/#install-from-conda-forge","title":"Install from conda-forge","text":"<p>segment-geospatial is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install segment-geospatial using the following commands. It is recommended to create a fresh conda environment for segment-geospatial. The following commands will create a new conda environment named <code>geo</code> and install segment-geospatial and its dependencies:</p> <pre><code>conda create -n geo python\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge segment-geospatial\n</code></pre> <p>If your system has a GPU, but the above commands do not install the GPU version of pytorch, you can force the installation of the GPU version of pytorch using the following command:</p> <pre><code>mamba install -c conda-forge segment-geospatial \"pytorch=*=cuda*\"\n</code></pre> <p>Samgeo-geospatial has some optional dependencies that are not included in the default conda environment. To install these dependencies, run the following command:</p> <pre><code>mamba install -c conda-forge groundingdino-py segment-anything-fast\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/segment-geospatial\n</code></pre>"},{"location":"installation/#use-docker","title":"Use docker","text":"<p>You can also use docker to run segment-geospatial:</p> <pre><code>docker run -it -p 8888:8888 giswqs/segment-geospatial:latest\n</code></pre> <p>To enable GPU for segment-geospatial, run the following command to run a short benchmark on your GPU:</p> <pre><code>docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance.\n        -fullscreen       (run n-body simulation in fullscreen mode)\n        -fp64             (use double precision floating point values for simulation)\n        -hostmem          (stores simulation data in host memory)\n        -benchmark        (run benchmark to measure performance)\n        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)\n        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)\n        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)\n        -compare          (compares simulation results running once on the default GPU and once on the CPU)\n        -cpu              (run n-body simulation on the CPU)\n        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n&gt; Windowed mode\n&gt; Simulation data stored in video memory\n&gt; Single precision floating point simulation\n&gt; 1 Devices used for simulation\nGPU Device 0: \"Turing\" with compute capability 7.5\n\n&gt; Compute 7.5 CUDA device: [Quadro RTX 5000]\n49152 bodies, total time for 10 iterations: 69.386 ms\n= 348.185 billion interactions per second\n= 6963.703 single-precision GFLOP/s at 20 flops per interaction\n</code></pre> <p>If you encounter the following error:</p> <pre><code>nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.\n</code></pre> <p>Try adding <code>sudo</code> to the command:</p> <pre><code>sudo docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>Once everything is working, you can run the following command to start a Jupyter Notebook server:</p> <pre><code>docker run -it -p 8888:8888 --gpus=all giswqs/segment-geospatial:latest\n</code></pre>"},{"location":"samgeo/","title":"samgeo module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"samgeo/#samgeo.samgeo.SamGeo","title":"<code>SamGeo</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n        hq = False  # Not using HQ-SAM\n\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.model_version = \"sam\"\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        batch_sample_size=(512, 512),\n        batch_nodata_threshold=1.0,\n        nodata_value=None,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        min_size=0,\n        max_size=None,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            batch_sample_size (tuple, optional): When batch=True, the size of the sample window when iterating over rasters.\n            batch_nodata_threshold (float,optional): Batch samples with a fraction of nodata pixels above this threshold will\n                not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful\n                when rasters have large areas of nodata values which can be skipped.\n            nodata_value (int, optional): Nodata value to use in checking batch_nodata_threshold. The default, None,\n                will use the nodata value in the raster metadata if present.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (int, optional): The maximum size of the objects. Defaults to None.\n            **kwargs: Other arguments for save_masks().\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    sample_size=batch_sample_size,\n                    sample_nodata_threshold=batch_nodata_threshold,\n                    nodata_value=nodata_value,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n        self._min_size = min_size\n        self._max_size = max_size\n\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **kwargs,\n        )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        min_size=0,\n        max_size=None,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (int, optional): The maximum size of the objects. Defaults to None.\n            **kwargs: Other arguments for array_to_image().\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in descending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            count = len(sorted_masks)\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and ann[\"area\"] &gt; max_size:\n                    continue\n                objects[m] = count - index\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and m[\"area\"] &gt; max_size:\n                    continue\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n                continue\n            if (\n                hasattr(self, \"_max_size\")\n                and isinstance(self._max_size, int)\n                and ann[\"area\"] &gt; self._max_size\n            ):\n                continue\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        # if \"dpi\" not in kwargs:\n        #     kwargs[\"dpi\"] = 100\n\n        # if \"bbox_inches\" not in kwargs:\n        #     kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source, **kwargs)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__call__","title":"<code>__call__(image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__init__","title":"<code>__init__(model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, sam_kwargs=None, **kwargs)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n    hq = False  # Not using HQ-SAM\n\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.model_version = \"sam\"\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache()</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.generate","title":"<code>generate(source, output=None, foreground=True, batch=False, batch_sample_size=(512, 512), batch_nodata_threshold=1.0, nodata_value=None, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>batch_sample_size</code> <code>tuple</code> <p>When batch=True, the size of the sample window when iterating over rasters.</p> <code>(512, 512)</code> <code>batch_nodata_threshold</code> <code>(float, optional)</code> <p>Batch samples with a fraction of nodata pixels above this threshold will not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful when rasters have large areas of nodata values which can be skipped.</p> <code>1.0</code> <code>nodata_value</code> <code>int</code> <p>Nodata value to use in checking batch_nodata_threshold. The default, None, will use the nodata value in the raster metadata if present.</p> <code>None</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    batch_sample_size=(512, 512),\n    batch_nodata_threshold=1.0,\n    nodata_value=None,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    min_size=0,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        batch_sample_size (tuple, optional): When batch=True, the size of the sample window when iterating over rasters.\n        batch_nodata_threshold (float,optional): Batch samples with a fraction of nodata pixels above this threshold will\n            not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful\n            when rasters have large areas of nodata values which can be skipped.\n        nodata_value (int, optional): Nodata value to use in checking batch_nodata_threshold. The default, None,\n            will use the nodata value in the raster metadata if present.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (int, optional): The maximum size of the objects. Defaults to None.\n        **kwargs: Other arguments for save_masks().\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                sample_size=batch_sample_size,\n                sample_nodata_threshold=batch_nodata_threshold,\n                nodata_value=nodata_value,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n    self._min_size = min_size\n    self._max_size = max_size\n\n    # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n    self.save_masks(\n        output,\n        foreground,\n        unique,\n        erosion_kernel,\n        mask_multiplier,\n        min_size,\n        max_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Other arguments for array_to_image().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    min_size=0,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (int, optional): The maximum size of the objects. Defaults to None.\n        **kwargs: Other arguments for array_to_image().\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in descending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        count = len(sorted_masks)\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and ann[\"area\"] &gt; max_size:\n                continue\n            objects[m] = count - index\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and m[\"area\"] &gt; max_size:\n                continue\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype=np.float32, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>float32</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.set_image","title":"<code>set_image(image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n            continue\n        if (\n            hasattr(self, \"_max_size\")\n            and isinstance(self._max_size, int)\n            and ann[\"area\"] &gt; self._max_size\n        ):\n            continue\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    # if \"dpi\" not in kwargs:\n    #     kwargs[\"dpi\"] = 100\n\n    # if \"bbox_inches\" not in kwargs:\n    #     kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: The predicted mask as a numpy array.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/","title":"samgeo2 module","text":""},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2","title":"<code>SamGeo2</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model 2 (SAM2). See https://github.com/facebookresearch/segment-anything-2 for details.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>class SamGeo2:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model 2 (SAM2). See\n    https://github.com/facebookresearch/segment-anything-2 for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"sam2-hiera-large\",\n        device: Optional[str] = None,\n        empty_cache: bool = True,\n        automatic: bool = True,\n        video: bool = False,\n        mode: str = \"eval\",\n        hydra_overrides_extra: Optional[List[str]] = None,\n        apply_postprocessing: bool = False,\n        points_per_side: Optional[int] = 32,\n        points_per_batch: int = 64,\n        pred_iou_thresh: float = 0.8,\n        stability_score_thresh: float = 0.95,\n        stability_score_offset: float = 1.0,\n        mask_threshold: float = 0.0,\n        box_nms_thresh: float = 0.7,\n        crop_n_layers: int = 0,\n        crop_nms_thresh: float = 0.7,\n        crop_overlap_ratio: float = 512 / 1500,\n        crop_n_points_downscale_factor: int = 1,\n        point_grids: Optional[List[np.ndarray]] = None,\n        min_mask_region_area: int = 0,\n        output_mode: str = \"binary_mask\",\n        use_m2m: bool = False,\n        multimask_output: bool = False,\n        max_hole_area: float = 0.0,\n        max_sprinkle_area: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SamGeo2 class.\n\n        Args:\n            model_id (str): The model ID to use. Can be one of the following: \"sam2-hiera-tiny\",\n                \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\".\n                Defaults to \"sam2-hiera-large\".\n            device (Optional[str]): The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.\n            empty_cache (bool): Whether to empty the cache. Defaults to True.\n            automatic (bool): Whether to use automatic mask generation. Defaults to True.\n            video (bool): Whether to use video prediction. Defaults to False.\n            mode (str): The mode to use. Defaults to \"eval\".\n            hydra_overrides_extra (Optional[List[str]]): Additional Hydra overrides. Defaults to None.\n            apply_postprocessing (bool): Whether to apply postprocessing. Defaults to False.\n            points_per_side (int or None): The number of points to be sampled\n                along one side of the image. The total number of points is\n                points_per_side**2. If None, 'point_grids' must provide explicit\n                point sampling.\n            points_per_batch (int): Sets the number of points run simultaneously\n                by the model. Higher numbers may be faster but use more GPU memory.\n            pred_iou_thresh (float): A filtering threshold in [0,1], using the\n                model's predicted mask quality.\n            stability_score_thresh (float): A filtering threshold in [0,1], using\n                the stability of the mask under changes to the cutoff used to binarize\n                the model's mask predictions.\n            stability_score_offset (float): The amount to shift the cutoff when\n                calculated the stability score.\n            mask_threshold (float): Threshold for binarizing the mask logits\n            box_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks.\n            crop_n_layers (int): If &gt;0, mask prediction will be run again on\n                crops of the image. Sets the number of layers to run, where each\n                layer has 2**i_layer number of image crops.\n            crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks between different crops.\n            crop_overlap_ratio (float): Sets the degree to which crops overlap.\n                In the first crop layer, crops will overlap by this fraction of\n                the image length. Later layers with more crops scale down this overlap.\n            crop_n_points_downscale_factor (int): The number of points-per-side\n                sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n            point_grids (list(np.ndarray) or None): A list over explicit grids\n                of points used for sampling, normalized to [0,1]. The nth grid in the\n                list is used in the nth crop layer. Exclusive with points_per_side.\n            min_mask_region_area (int): If &gt;0, postprocessing will be applied\n                to remove disconnected regions and holes in masks with area smaller\n                than min_mask_region_area. Requires opencv.\n            output_mode (str): The form masks are returned in. Can be 'binary_mask',\n                'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools.\n                For large resolutions, 'binary_mask' may consume large amounts of\n                memory.\n            use_m2m (bool): Whether to add a one step refinement using previous mask predictions.\n            multimask_output (bool): Whether to output multimask at each point of the grid.\n                Defaults to False.\n            max_hole_area (int): If max_hole_area &gt; 0, we fill small holes in up to\n                the maximum area of max_hole_area in low_res_masks.\n            max_sprinkle_area (int): If max_sprinkle_area &gt; 0, we remove small sprinkles up to\n                the maximum area of max_sprinkle_area in low_res_masks.\n            **kwargs (Any): Additional keyword arguments to pass to\n                SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().\n        \"\"\"\n        if isinstance(model_id, str):\n            if not model_id.startswith(\"facebook/\"):\n                model_id = f\"facebook/{model_id}\"\n        else:\n            raise ValueError(\"model_id must be a string\")\n\n        allowed_models = [\n            \"facebook/sam2-hiera-tiny\",\n            \"facebook/sam2-hiera-small\",\n            \"facebook/sam2-hiera-base-plus\",\n            \"facebook/sam2-hiera-large\",\n        ]\n\n        if model_id not in allowed_models:\n            raise ValueError(\n                f\"model_id must be one of the following: {', '.join(allowed_models)}\"\n            )\n\n        if device is None:\n            device = common.choose_device(empty_cache=empty_cache)\n\n        if hydra_overrides_extra is None:\n            hydra_overrides_extra = []\n\n        self.model_id = model_id\n        self.model_version = \"sam2\"\n        self.device = device\n\n        if video:\n            automatic = False\n\n        if automatic:\n            self.mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                points_per_side=points_per_side,\n                points_per_batch=points_per_batch,\n                pred_iou_thresh=pred_iou_thresh,\n                stability_score_thresh=stability_score_thresh,\n                stability_score_offset=stability_score_offset,\n                mask_threshold=mask_threshold,\n                box_nms_thresh=box_nms_thresh,\n                crop_n_layers=crop_n_layers,\n                crop_nms_thresh=crop_nms_thresh,\n                crop_overlap_ratio=crop_overlap_ratio,\n                crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n                point_grids=point_grids,\n                min_mask_region_area=min_mask_region_area,\n                output_mode=output_mode,\n                use_m2m=use_m2m,\n                multimask_output=multimask_output,\n                **kwargs,\n            )\n        elif video:\n            self.predictor = SAM2VideoPredictor.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                **kwargs,\n            )\n        else:\n            self.predictor = SAM2ImagePredictor.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                mask_threshold=mask_threshold,\n                max_hole_area=max_hole_area,\n                max_sprinkle_area=max_sprinkle_area,\n                **kwargs,\n            )\n\n    def generate(\n        self,\n        source: Union[str, np.ndarray],\n        output: Optional[str] = None,\n        foreground: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: int = None,\n        **kwargs: Any,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Generate masks for the input image.\n\n        Args:\n            source (Union[str, np.ndarray]): The path to the input image or the\n                input image as a numpy array.\n            output (Optional[str]): The path to the output image. Defaults to None.\n            foreground (bool): Whether to generate the foreground mask. Defaults\n                to True.\n            erosion_kernel (Optional[Tuple[int, int]]): The erosion kernel for\n                filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range,\n                for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool): Whether to assign a unique value to each object.\n                Defaults to True.\n                The unique value increases from 1 to the number of objects. The\n                larger the number, the larger the object area.\n            min_size (int): The minimum size of the object. Defaults to 0.\n            max_size (int): The maximum size of the object. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = common.download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self._min_size = min_size\n        self._max_size = max_size\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output,\n                foreground,\n                unique,\n                erosion_kernel,\n                mask_multiplier,\n                min_size,\n                max_size,\n                **kwargs,\n            )\n\n    def save_masks(\n        self,\n        output: Optional[str] = None,\n        foreground: bool = True,\n        unique: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        min_size: int = 0,\n        max_size: int = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the masks to the output path. The output is either a binary mask\n        or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to\n                None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask.\n                Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each\n                object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering\n                object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to\n                None.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1]. You can use this\n                parameter to scale the mask to a larger range, for example\n                [0, 255]. Defaults to 255.\n            min_size (int, optional): The minimum size of the object. Defaults to 0.\n            max_size (int, optional): The maximum size of the object. Defaults to None.\n            **kwargs: Additional keyword arguments for common.array_to_image().\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in descending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            count = len(sorted_masks)\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and ann[\"area\"] &gt; max_size:\n                    continue\n                objects[m] = count - index\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and m[\"area\"] &gt; max_size:\n                    continue\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            common.array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        cmap: str = \"binary_r\",\n        axis: str = \"off\",\n        foreground: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only.\n                Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        alpha: float = 0.35,\n        output: Optional[str] = None,\n        blend: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n                continue\n            if (\n                hasattr(self, \"_max_size\")\n                and isinstance(self._max_size, int)\n                and ann[\"area\"] &gt; self._max_size\n            ):\n                continue\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        # if \"dpi\" not in kwargs:\n        #     kwargs[\"dpi\"] = 100\n\n        # if \"bbox_inches\" not in kwargs:\n        #     kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = common.blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            common.array_to_image(array, output, self.source, **kwargs)\n\n    @torch.no_grad()\n    def set_image(\n        self,\n        image: Union[str, np.ndarray, Image],\n    ) -&gt; None:\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (Union[str, np.ndarray, Image]): The input image as a path,\n                a numpy array, or an Image.\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray) or isinstance(image, Image):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image)\n\n    @torch.no_grad()\n    def set_image_batch(\n        self,\n        image_list: List[Union[np.ndarray, str, Image]],\n    ) -&gt; None:\n        \"\"\"Set a batch of images for prediction.\n\n        Args:\n            image_list (List[Union[np.ndarray, str, Image]]): A list of images,\n            which can be numpy arrays, file paths, or PIL images.\n\n        Raises:\n            ValueError: If an input image path does not exist or if the input\n                image type is not supported.\n        \"\"\"\n        images = []\n        for image in image_list:\n            if isinstance(image, str):\n                if image.startswith(\"http\"):\n                    image = common.download_file(image)\n\n                if not os.path.exists(image):\n                    raise ValueError(f\"Input path {image} does not exist.\")\n\n                image = cv2.imread(image)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            elif isinstance(image, Image):\n                image = np.array(image)\n            elif isinstance(image, np.ndarray):\n                pass\n            else:\n                raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n            images.append(image)\n\n        self.predictor.set_image_batch(images)\n\n    def predict(\n        self,\n        point_coords: Optional[np.ndarray] = None,\n        point_labels: Optional[np.ndarray] = None,\n        boxes: Optional[np.ndarray] = None,\n        mask_input: Optional[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords: bool = True,\n        point_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"float32\",\n        return_results: bool = False,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predict the mask for the input image.\n\n        Args:\n            point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n            point_labels (np.ndarray, optional): The point labels. Defaults to None.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            multimask_output (bool, optional): Whether to output multimask at each\n                point of the grid. Defaults to False.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            normalize_coords (bool, optional): Whether to normalize the coordinates.\n                Defaults to True.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks,\n                scores, and logits. Defaults to False.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n                and the logits.\n        \"\"\"\n        import geopandas as gpd\n\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = common.vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = common.geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = common.coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = common.bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n\n        self.boxes = input_boxes\n\n        masks, scores, logits = predictor.predict(\n            point_coords=point_coords,\n            point_labels=point_labels,\n            box=input_boxes,\n            mask_input=mask_input,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def predict_by_points(\n        self,\n        point_coords_batch: List[np.ndarray] = None,\n        point_labels_batch: List[np.ndarray] = None,\n        box_batch: List[np.ndarray] = None,\n        mask_input_batch: List[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords=True,\n        point_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        index: Optional[int] = None,\n        unique: bool = True,\n        mask_multiplier: int = 255,\n        dtype: str = \"int32\",\n        return_results: bool = False,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predict the mask for the input image.\n\n        Args:\n            point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n            point_labels (np.ndarray, optional): The point labels. Defaults to None.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            multimask_output (bool, optional): Whether to output multimask at each\n                point of the grid. Defaults to True.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            normalize_coords (bool, optional): Whether to normalize the coordinates.\n                Defaults to True.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.int32.\n            return_results (bool, optional): Whether to return the predicted masks,\n                scores, and logits. Defaults to False.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n                and the logits.\n        \"\"\"\n        import geopandas as gpd\n\n        if hasattr(self, \"image_batch\") and self.image_batch is not None:\n            pass\n        elif self.image is not None:\n            self.predictor.set_image_batch([self.image])\n            setattr(self, \"image_batch\", [self.image])\n        else:\n            raise ValueError(\"Please set the input image first using set_image().\")\n\n        if isinstance(point_coords_batch, dict):\n            point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n        if isinstance(point_coords_batch, str) or isinstance(\n            point_coords_batch, gpd.GeoDataFrame\n        ):\n            if isinstance(point_coords_batch, str):\n                gdf = gpd.read_file(point_coords_batch)\n            else:\n                gdf = point_coords_batch\n            if gdf.crs is None and (point_crs is not None):\n                gdf.crs = point_crs\n\n            points = gdf.geometry.apply(lambda geom: [geom.x, geom.y])\n            coordinates_array = np.array([[point] for point in points])\n            points = common.coords_to_xy(self.source, coordinates_array, point_crs)\n            num_points = points.shape[0]\n            if point_labels_batch is None:\n                labels = np.array([[1] for i in range(num_points)])\n            else:\n                labels = point_labels_batch\n\n        elif isinstance(point_coords_batch, list):\n            if point_crs is not None:\n                point_coords_batch_crs = common.coords_to_xy(\n                    self.source, point_coords_batch, point_crs\n                )\n            else:\n                point_coords_batch_crs = point_coords_batch\n            num_points = len(point_coords_batch)\n\n            points = []\n            points.append([[point] for point in point_coords_batch_crs])\n\n            if point_labels_batch is None:\n                labels = np.array([[1] for i in range(num_points)])\n            elif isinstance(point_labels_batch, list):\n                labels = []\n                labels.append([[label] for label in point_labels_batch])\n                labels = labels[0]\n            else:\n                labels = point_labels_batch\n\n            points = np.array(points[0])\n            labels = np.array(labels)\n\n        elif isinstance(point_coords_batch, np.ndarray):\n            points = point_coords_batch\n            labels = point_labels_batch\n        else:\n            raise ValueError(\"point_coords must be a list, a GeoDataFrame, or a path.\")\n\n        predictor = self.predictor\n\n        masks_batch, scores_batch, logits_batch = predictor.predict_batch(\n            point_coords_batch=[points],\n            point_labels_batch=[labels],\n            box_batch=box_batch,\n            mask_input_batch=mask_input_batch,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        masks = masks_batch[0]\n        scores = scores_batch[0]\n        logits = logits_batch[0]\n\n        if multimask_output and (index is not None):\n            masks = masks[:, index, :, :]\n\n        if masks.ndim &gt; 3:\n            masks = masks.squeeze()\n\n        output_masks = []\n        sums = np.sum(masks, axis=(1, 2))\n        for index, mask in enumerate(masks):\n            item = {\"segmentation\": mask.astype(\"bool\"), \"area\": sums[index]}\n            output_masks.append(item)\n\n        self.masks = output_masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            self.save_masks(\n                output,\n                foreground=True,\n                unique=unique,\n                mask_multiplier=mask_multiplier,\n                dtype=dtype,\n                **kwargs,\n            )\n\n        if return_results:\n            return output_masks, scores, logits\n\n    def predict_batch(\n        self,\n        point_coords_batch: List[np.ndarray] = None,\n        point_labels_batch: List[np.ndarray] = None,\n        box_batch: List[np.ndarray] = None,\n        mask_input_batch: List[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords=True,\n    ) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Predict masks for a batch of images.\n\n        Args:\n            point_coords_batch (Optional[List[np.ndarray]]): A batch of point\n                coordinates. Defaults to None.\n            point_labels_batch (Optional[List[np.ndarray]]): A batch of point\n                labels. Defaults to None.\n            box_batch (Optional[List[np.ndarray]]): A batch of bounding boxes.\n                Defaults to None.\n            mask_input_batch (Optional[List[np.ndarray]]): A batch of mask inputs.\n                Defaults to None.\n            multimask_output (bool): Whether to output multimask at each point\n                of the grid. Defaults to False.\n            return_logits (bool): Whether to return the logits. Defaults to False.\n            normalize_coords (bool): Whether to normalize the coordinates.\n                Defaults to True.\n\n        Returns:\n            Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists\n                of masks, multimasks, and logits.\n        \"\"\"\n\n        return self.predictor.predict_batch(\n            point_coords_batch=point_coords_batch,\n            point_labels_batch=point_labels_batch,\n            box_batch=box_batch,\n            mask_input_batch=mask_input_batch,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n    @torch.inference_mode()\n    def init_state(\n        self,\n        video_path: str,\n        offload_video_to_cpu: bool = False,\n        offload_state_to_cpu: bool = False,\n        async_loading_frames: bool = False,\n    ) -&gt; Any:\n        \"\"\"Initialize an inference state.\n\n        Args:\n            video_path (str): The path to the video file.\n            offload_video_to_cpu (bool): Whether to offload the video to CPU.\n                Defaults to False.\n            offload_state_to_cpu (bool): Whether to offload the state to CPU.\n                Defaults to False.\n            async_loading_frames (bool): Whether to load frames asynchronously.\n                Defaults to False.\n\n        Returns:\n            Any: The initialized inference state.\n        \"\"\"\n        return self.predictor.init_state(\n            video_path,\n            offload_video_to_cpu=offload_video_to_cpu,\n            offload_state_to_cpu=offload_state_to_cpu,\n            async_loading_frames=async_loading_frames,\n        )\n\n    @torch.inference_mode()\n    def reset_state(self, inference_state: Any) -&gt; None:\n        \"\"\"Remove all input points or masks in all frames throughout the video.\n\n        Args:\n            inference_state (Any): The current inference state.\n        \"\"\"\n        self.predictor.reset_state(inference_state)\n\n    @torch.inference_mode()\n    def add_new_points_or_box(\n        self,\n        inference_state: Any,\n        frame_idx: int,\n        obj_id: int,\n        points: Optional[np.ndarray] = None,\n        labels: Optional[np.ndarray] = None,\n        clear_old_points: bool = True,\n        normalize_coords: bool = True,\n        box: Optional[np.ndarray] = None,\n    ) -&gt; Any:\n        \"\"\"Add new points or a box to the inference state.\n\n        Args:\n            inference_state (Any): The current inference state.\n            frame_idx (int): The frame index.\n            obj_id (int): The object ID.\n            points (Optional[np.ndarray]): The points to add. Defaults to None.\n            labels (Optional[np.ndarray]): The labels for the points. Defaults to None.\n            clear_old_points (bool): Whether to clear old points. Defaults to True.\n            normalize_coords (bool): Whether to normalize the coordinates. Defaults to True.\n            box (Optional[np.ndarray]): The bounding box to add. Defaults to None.\n\n        Returns:\n            Any: The updated inference state.\n        \"\"\"\n        return self.predictor.add_new_points_or_box(\n            inference_state,\n            frame_idx,\n            obj_id,\n            points=points,\n            labels=labels,\n            clear_old_points=clear_old_points,\n            normalize_coords=normalize_coords,\n            box=box,\n        )\n\n    @torch.inference_mode()\n    def add_new_mask(\n        self,\n        inference_state: Any,\n        frame_idx: int,\n        obj_id: int,\n        mask: np.ndarray,\n    ) -&gt; Any:\n        \"\"\"Add a new mask to the inference state.\n\n        Args:\n            inference_state (Any): The current inference state.\n            frame_idx (int): The frame index.\n            obj_id (int): The object ID.\n            mask (np.ndarray): The mask to add.\n\n        Returns:\n            Any: The updated inference state.\n        \"\"\"\n        return self.predictor.add_new_mask(inference_state, frame_idx, obj_id, mask)\n\n    @torch.inference_mode()\n    def propagate_in_video_preflight(self, inference_state: Any) -&gt; Any:\n        \"\"\"Propagate the inference state in video preflight.\n\n        Args:\n            inference_state (Any): The current inference state.\n\n        Returns:\n            Any: The propagated inference state.\n        \"\"\"\n        return self.predictor.propagate_in_video_preflight(inference_state)\n\n    @torch.inference_mode()\n    def propagate_in_video(\n        self,\n        inference_state: Any,\n        start_frame_idx: Optional[int] = None,\n        max_frame_num_to_track: Optional[int] = None,\n        reverse: bool = False,\n    ) -&gt; Any:\n        \"\"\"Propagate the inference state in video.\n\n        Args:\n            inference_state (Any): The current inference state.\n            start_frame_idx (Optional[int]): The starting frame index. Defaults to None.\n            max_frame_num_to_track (Optional[int]): The maximum number of frames\n                to track. Defaults to None.\n            reverse (bool): Whether to propagate in reverse. Defaults to False.\n\n        Returns:\n            Any: The propagated inference state.\n        \"\"\"\n        return self.predictor.propagate_in_video(\n            inference_state,\n            start_frame_idx=start_frame_idx,\n            max_frame_num_to_track=max_frame_num_to_track,\n            reverse=reverse,\n        )\n\n    def tensor_to_numpy(\n        self,\n        index: Optional[int] = None,\n        output: Optional[str] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"uint8\",\n        save_args: Optional[Dict[str, Any]] = None,\n    ) -&gt; Optional[np.ndarray]:\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (Optional[int], optional): The index of the mask to save.\n                Defaults to None, which will save the mask with the highest score.\n            output (Optional[str], optional): The path to the output image.\n                Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1].\n            dtype (str, optional): The data type of the output image. Defaults\n                to \"uint8\".\n            save_args (Optional[Dict[str, Any]], optional): Optional arguments\n                for saving the output image. Defaults to None.\n\n        Returns:\n            Optional[np.ndarray]: The predicted mask as a numpy array, or None\n                if output is specified.\n        \"\"\"\n        if save_args is None:\n            save_args = {}\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 0\n\n        masks = masks[:, index, :, :]\n        if len(masks.shape) == 4 and masks.shape[1] == 1:\n            masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (_, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            common.array_to_image(\n                mask_overlay, output, self.source, dtype=dtype, **save_args\n            )\n        else:\n            return mask_overlay\n\n    def save_prediction(\n        self,\n        output: str,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"float32\",\n        vector: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (Optional[int], optional): The index of the mask to save.\n                Defaults to None, which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1].\n            dtype (str, optional): The data type of the output image. Defaults\n                to \"float32\".\n            vector (Optional[str], optional): The path to the output vector file.\n                Defaults to None.\n            simplify_tolerance (Optional[float], optional): The maximum allowed\n                geometry displacement. The higher this value, the smaller the\n                number of vertices in the resulting geometry.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            common.raster_to_vector(\n                output, vector, simplify_tolerance=simplify_tolerance\n            )\n\n    def show_map(\n        self,\n        basemap: str = \"SATELLITE\",\n        repeat_mode: bool = True,\n        out_dir: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following:\n                SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for\n                draw control. Defaults to True.\n            out_dir (Optional[str], optional): The path to the output directory.\n                Defaults to None.\n\n        Returns:\n            Any: The map object.\n        \"\"\"\n        return common.sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(\n        self,\n        fg_color: Tuple[int, int, int] = (0, 255, 0),\n        bg_color: Tuple[int, int, int] = (0, 0, 255),\n        radius: int = 5,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            fg_color (Tuple[int, int, int], optional): The color for the foreground points.\n                Defaults to (0, 255, 0).\n            bg_color (Tuple[int, int, int], optional): The color for the background points.\n                Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            Tuple[list, list]: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def _convert_prompts(self, prompts: Dict[int, Any]) -&gt; Dict[int, Any]:\n        \"\"\"Convert the points and labels in the prompts to numpy arrays with specific data types.\n\n        Args:\n            prompts (Dict[str, Any]): A dictionary containing the prompts with points and labels.\n\n        Returns:\n            Dict[str, Any]: The updated dictionary with points and labels converted to numpy arrays.\n        \"\"\"\n        for _, value in prompts.items():\n            # Convert points to np.float32 array\n            if \"points\" in value:\n                value[\"points\"] = np.array(value[\"points\"], dtype=np.float32)\n            # Convert labels to np.int32 array\n            if \"labels\" in value:\n                value[\"labels\"] = np.array(value[\"labels\"], dtype=np.int32)\n            # Convert box to np.float32 array\n            if \"box\" in value:\n                value[\"box\"] = np.array(value[\"box\"], dtype=np.float32)\n\n        return prompts\n\n    def set_video(\n        self,\n        video_path: str,\n        output_dir: str = None,\n        frame_rate: Optional[int] = None,\n        prefix: str = \"\",\n    ) -&gt; None:\n        \"\"\"Set the video path and parameters.\n\n        Args:\n            video_path (str): The path to the video file.\n            start_frame (int, optional): The starting frame index. Defaults to 0.\n            end_frame (Optional[int], optional): The ending frame index. Defaults to None.\n            step (int, optional): The step size. Defaults to 1.\n            frame_rate (Optional[int], optional): The frame rate. Defaults to None.\n        \"\"\"\n\n        if isinstance(video_path, str):\n            if video_path.startswith(\"http\"):\n                video_path = common.download_file(video_path)\n            if os.path.isfile(video_path):\n                if output_dir is None:\n                    output_dir = common.make_temp_dir()\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                print(f\"Output directory: {output_dir}\")\n                common.video_to_images(\n                    video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n                )\n\n            elif os.path.isdir(video_path):\n                files = sorted(os.listdir(video_path))\n                if len(files) == 0:\n                    raise ValueError(f\"No files found in {video_path}.\")\n                elif files[0].endswith(\".tif\"):\n                    self._tif_source = os.path.join(video_path, files[0])\n                    self._tif_dir = video_path\n                    self._tif_names = files\n                    video_path = common.geotiff_to_jpg_batch(video_path)\n                output_dir = video_path\n\n            if not os.path.exists(video_path):\n                raise ValueError(f\"Input path {video_path} does not exist.\")\n        else:\n            raise ValueError(\"Input video_path must be a string.\")\n\n        self.video_path = output_dir\n        self._num_images = len(os.listdir(output_dir))\n        self._frame_names = sorted(os.listdir(output_dir))\n        self.inference_state = self.predictor.init_state(video_path=output_dir)\n\n    def predict_video(\n        self,\n        prompts: Dict[int, Any] = None,\n        point_crs: Optional[str] = None,\n        output_dir: Optional[str] = None,\n        img_ext: str = \"png\",\n    ) -&gt; None:\n        \"\"\"Predict masks for the video.\n\n        Args:\n            prompts (Dict[int, Any]): A dictionary containing the prompts with points and labels.\n            point_crs (Optional[str]): The coordinate reference system (CRS) of the point prompts.\n            output_dir (Optional[str]): The directory to save the output images. Defaults to None.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n        \"\"\"\n\n        from PIL import Image\n\n        def save_image_from_dict(data, output_path=\"output_image.png\"):\n            # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n            array_shape = next(iter(data.values())).shape[1:]\n\n            # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n            output_array = np.zeros(array_shape, dtype=np.uint8)\n\n            # Iterate over each key and array in the dictionary\n            for key, array in data.items():\n                # Assign the key value wherever the boolean array is True\n                output_array[array[0]] = key\n\n            # Convert the output array to a PIL image\n            image = Image.fromarray(output_array)\n\n            # Save the image\n            image.save(output_path)\n\n        if prompts is None:\n            if hasattr(self, \"prompts\"):\n                prompts = self.prompts\n            else:\n                raise ValueError(\"Please provide prompts.\")\n\n        if point_crs is not None and self._tif_source is not None:\n            for prompt in prompts.values():\n                points = prompt.get(\"points\", None)\n                if points is not None:\n                    points = common.coords_to_xy(self._tif_source, points, point_crs)\n                    prompt[\"points\"] = points\n                box = prompt.get(\"box\", None)\n                if box is not None:\n                    box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                    prompt[\"box\"] = box\n\n        prompts = self._convert_prompts(prompts)\n        predictor = self.predictor\n        inference_state = self.inference_state\n        for obj_id, prompt in prompts.items():\n            points = prompt.get(\"points\", None)\n            labels = prompt.get(\"labels\", None)\n            box = prompt.get(\"box\", None)\n            frame_idx = prompt.get(\"frame_idx\", None)\n\n            _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n                inference_state=inference_state,\n                frame_idx=frame_idx,\n                obj_id=obj_id,\n                points=points,\n                labels=labels,\n                box=box,\n            )\n\n        video_segments = {}\n        num_frames = self._num_images\n        num_digits = len(str(num_frames))\n\n        if output_dir is not None:\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n\n        for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n            inference_state\n        ):\n            video_segments[out_frame_idx] = {\n                out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                for i, out_obj_id in enumerate(out_obj_ids)\n            }\n\n            if output_dir is not None:\n                output_path = os.path.join(\n                    output_dir, f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n                )\n                save_image_from_dict(video_segments[out_frame_idx], output_path)\n\n        self.video_segments = video_segments\n\n        # if output_dir is not None:\n        #     self.save_video_segments(output_dir, img_ext)\n\n    def save_video_segments(self, output_dir: str, img_ext: str = \"png\") -&gt; None:\n        \"\"\"Save the video segments to the output directory.\n\n        Args:\n            output_dir (str): The path to the output directory.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n        \"\"\"\n        from PIL import Image\n\n        def save_image_from_dict(\n            data, output_path=\"output_image.png\", crs_source=None, **kwargs\n        ):\n            # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n            array_shape = next(iter(data.values())).shape[1:]\n\n            # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n            output_array = np.zeros(array_shape, dtype=np.uint8)\n\n            # Iterate over each key and array in the dictionary\n            for key, array in data.items():\n                # Assign the key value wherever the boolean array is True\n                output_array[array[0]] = key\n\n            if crs_source is None:\n                # Convert the output array to a PIL image\n                image = Image.fromarray(output_array)\n\n                # Save the image\n                image.save(output_path)\n            else:\n                output_path = output_path.replace(\".png\", \".tif\")\n                common.array_to_image(output_array, output_path, crs_source, **kwargs)\n\n        num_frames = len(self.video_segments)\n        num_digits = len(str(num_frames))\n\n        if hasattr(self, \"_tif_source\") and self._tif_source.endswith(\".tif\"):\n            crs_source = self._tif_source\n            filenames = self._tif_names\n        else:\n            crs_source = None\n            filenames = None\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        # Initialize the tqdm progress bar\n        for frame_idx, video_segment in tqdm(\n            self.video_segments.items(), desc=\"Rendering frames\", total=num_frames\n        ):\n            if filenames is None:\n                output_path = os.path.join(\n                    output_dir, f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n                )\n            else:\n                output_path = os.path.join(output_dir, filenames[frame_idx])\n            save_image_from_dict(video_segment, output_path, crs_source)\n\n    def save_video_segments_blended(\n        self,\n        output_dir: str,\n        img_ext: str = \"png\",\n        alpha: float = 0.6,\n        dpi: int = 200,\n        frame_stride: int = 1,\n        output_video: Optional[str] = None,\n        fps: int = 30,\n    ) -&gt; None:\n        \"\"\"Save blended video segments to the output directory and optionally create a video.\n\n        Args:\n            output_dir (str): The directory to save the output images.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n            alpha (float): The alpha value for the blended masks. Defaults to 0.6.\n\n            dpi (int): The DPI (dots per inch) for the output images. Defaults to 200.\n            frame_stride (int): The stride for selecting frames to save. Defaults to 1.\n            output_video (Optional[str]): The path to the output video file. Defaults to None.\n            fps (int): The frames per second for the output video. Defaults to 30.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        from PIL import Image\n\n        def show_mask(mask, ax, obj_id=None, random_color=False):\n            if random_color:\n                color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n            else:\n                cmap = plt.get_cmap(\"tab10\")\n                cmap_idx = 0 if obj_id is None else obj_id\n                color = np.array([*cmap(cmap_idx)[:3], alpha])\n            h, w = mask.shape[-2:]\n            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n            ax.imshow(mask_image)\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        plt.close(\"all\")\n\n        video_segments = self.video_segments\n        video_dir = self.video_path\n        frame_names = self._frame_names\n        num_frames = len(frame_names)\n        num_digits = len(str(num_frames))\n\n        # Initialize the tqdm progress bar\n        for out_frame_idx in tqdm(\n            range(0, len(frame_names), frame_stride), desc=\"Rendering frames\"\n        ):\n            image = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n\n            # Get original image dimensions\n            w, h = image.size\n\n            # Set DPI and calculate figure size based on the original image dimensions\n            figsize = (\n                w / dpi,\n                h / dpi,\n            )\n            figsize = (\n                figsize[0] * 1.3,\n                figsize[1] * 1.3,\n            )\n\n            # Create a figure with the exact size and DPI\n            fig = plt.figure(figsize=figsize, dpi=dpi)\n\n            # Disable axis to prevent whitespace\n            plt.axis(\"off\")\n\n            # Display the original image\n            plt.imshow(image)\n\n            # Overlay masks for each object ID\n            for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n                show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n\n            # Save the figure with no borders or extra padding\n            filename = f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n            filepath = os.path.join(output_dir, filename)\n            plt.savefig(filepath, dpi=dpi, pad_inches=0, bbox_inches=\"tight\")\n            plt.close(fig)\n\n        if output_video is not None:\n            common.images_to_video(output_dir, output_video, fps=fps)\n\n    def show_images(self, path: str = None) -&gt; None:\n        \"\"\"Show the images in the video.\n\n        Args:\n            path (str, optional): The path to the images. Defaults to None.\n        \"\"\"\n        if path is None:\n            path = self.video_path\n\n        if path is not None:\n            common.show_image_gui(path)\n\n    def show_prompts(\n        self,\n        prompts: Dict[int, Any],\n        frame_idx: int = 0,\n        mask: Any = None,\n        random_color: bool = False,\n        point_crs: Optional[str] = None,\n        figsize: Tuple[int, int] = (9, 6),\n    ) -&gt; None:\n        \"\"\"Show the prompts on the image.\n\n        Args:\n            prompts (Dict[int, Any]): A dictionary containing the prompts with\n                points and labels.\n            frame_idx (int, optional): The frame index. Defaults to 0.\n            mask (Any, optional): The mask. Defaults to None.\n            random_color (bool, optional): Whether to use random colors for the\n                masks. Defaults to False.\n            point_crs (Optional[str], optional): The coordinate reference system\n            figsize (Tuple[int, int], optional): The figure size. Defaults to (9, 6).\n\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n        from PIL import Image\n\n        def show_mask(mask, ax, obj_id=None, random_color=random_color):\n            if random_color:\n                color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n            else:\n                cmap = plt.get_cmap(\"tab10\")\n                cmap_idx = 0 if obj_id is None else obj_id\n                color = np.array([*cmap(cmap_idx)[:3], 0.6])\n            h, w = mask.shape[-2:]\n            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n            ax.imshow(mask_image)\n\n        def show_points(coords, labels, ax, marker_size=200):\n            pos_points = coords[labels == 1]\n            neg_points = coords[labels == 0]\n            ax.scatter(\n                pos_points[:, 0],\n                pos_points[:, 1],\n                color=\"green\",\n                marker=\"*\",\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n            ax.scatter(\n                neg_points[:, 0],\n                neg_points[:, 1],\n                color=\"red\",\n                marker=\"*\",\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n\n        def show_box(box, ax):\n            x0, y0 = box[0], box[1]\n            w, h = box[2] - box[0], box[3] - box[1]\n            ax.add_patch(\n                plt.Rectangle(\n                    (x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2\n                )\n            )\n\n        if point_crs is not None and self._tif_source is not None:\n            for prompt in prompts.values():\n                points = prompt.get(\"points\", None)\n                if points is not None:\n                    points = common.coords_to_xy(self._tif_source, points, point_crs)\n                    prompt[\"points\"] = points\n                box = prompt.get(\"box\", None)\n                if box is not None:\n                    box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                    prompt[\"box\"] = box\n\n        prompts = self._convert_prompts(prompts)\n        self.prompts = prompts\n        video_dir = self.video_path\n        frame_names = self._frame_names\n        fig = plt.figure(figsize=figsize)\n        fig.canvas.toolbar_visible = True\n        fig.canvas.header_visible = False\n        fig.canvas.footer_visible = True\n        plt.title(f\"frame {frame_idx}\")\n        plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n\n        for obj_id, prompt in prompts.items():\n            points = prompt.get(\"points\", None)\n            labels = prompt.get(\"labels\", None)\n            box = prompt.get(\"box\", None)\n            anno_frame_idx = prompt.get(\"frame_idx\", None)\n            if anno_frame_idx == frame_idx:\n                if points is not None:\n                    show_points(points, labels, plt.gca())\n                if box is not None:\n                    show_box(box, plt.gca())\n                if mask is not None:\n                    show_mask(mask, plt.gca(), obj_id=obj_id)\n\n        plt.show()\n\n    def raster_to_vector(self, raster, vector, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a raster image file to a vector dataset.\n\n        Args:\n            raster (str): The path to the raster image.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        common.raster_to_vector(\n            raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def region_groups(\n        self,\n        image: Union[str, \"xr.DataArray\", np.ndarray],\n        connectivity: int = 1,\n        min_size: int = 10,\n        max_size: Optional[int] = None,\n        threshold: Optional[int] = None,\n        properties: Optional[List[str]] = None,\n        intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n        out_csv: Optional[str] = None,\n        out_vector: Optional[str] = None,\n        out_image: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[\n        Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n    ]:\n        \"\"\"\n        Segment regions in an image and filter them based on size.\n\n        Args:\n            image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n                path, xarray DataArray, or numpy array.\n            connectivity (int, optional): Connectivity for labeling. Defaults to 1\n                for 4-connectivity. Use 2 for 8-connectivity.\n            min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n            max_size (Optional[int], optional): Maximum size of regions to keep.\n                Defaults to None.\n            threshold (Optional[int], optional): Threshold for filling holes.\n                Defaults to None, which is equal to min_size.\n            properties (Optional[List[str]], optional): List of properties to measure.\n                See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n                Defaults to None.\n            intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n                Intensity image to use for properties. Defaults to None.\n            out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n                Defaults to None.\n            out_vector (Optional[str], optional): Path to save the vector file.\n                Defaults to None.\n            out_image (Optional[str], optional): Path to save the output image.\n                Defaults to None.\n\n        Returns:\n            Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n        \"\"\"\n        return common.region_groups(\n            image,\n            connectivity=connectivity,\n            min_size=min_size,\n            max_size=max_size,\n            threshold=threshold,\n            properties=properties,\n            intensity_image=intensity_image,\n            out_csv=out_csv,\n            out_vector=out_vector,\n            out_image=out_image,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.__init__","title":"<code>__init__(model_id='sam2-hiera-large', device=None, empty_cache=True, automatic=True, video=False, mode='eval', hydra_overrides_extra=None, apply_postprocessing=False, points_per_side=32, points_per_batch=64, pred_iou_thresh=0.8, stability_score_thresh=0.95, stability_score_offset=1.0, mask_threshold=0.0, box_nms_thresh=0.7, crop_n_layers=0, crop_nms_thresh=0.7, crop_overlap_ratio=512 / 1500, crop_n_points_downscale_factor=1, point_grids=None, min_mask_region_area=0, output_mode='binary_mask', use_m2m=False, multimask_output=False, max_hole_area=0.0, max_sprinkle_area=0.0, **kwargs)</code>","text":"<p>Initializes the SamGeo2 class.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model ID to use. Can be one of the following: \"sam2-hiera-tiny\", \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\". Defaults to \"sam2-hiera-large\".</p> <code>'sam2-hiera-large'</code> <code>device</code> <code>Optional[str]</code> <p>The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.</p> <code>None</code> <code>empty_cache</code> <code>bool</code> <p>Whether to empty the cache. Defaults to True.</p> <code>True</code> <code>automatic</code> <code>bool</code> <p>Whether to use automatic mask generation. Defaults to True.</p> <code>True</code> <code>video</code> <code>bool</code> <p>Whether to use video prediction. Defaults to False.</p> <code>False</code> <code>mode</code> <code>str</code> <p>The mode to use. Defaults to \"eval\".</p> <code>'eval'</code> <code>hydra_overrides_extra</code> <code>Optional[List[str]]</code> <p>Additional Hydra overrides. Defaults to None.</p> <code>None</code> <code>apply_postprocessing</code> <code>bool</code> <p>Whether to apply postprocessing. Defaults to False.</p> <code>False</code> <code>points_per_side</code> <code>int or None</code> <p>The number of points to be sampled along one side of the image. The total number of points is points_per_side**2. If None, 'point_grids' must provide explicit point sampling.</p> <code>32</code> <code>points_per_batch</code> <code>int</code> <p>Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.</p> <code>64</code> <code>pred_iou_thresh</code> <code>float</code> <p>A filtering threshold in [0,1], using the model's predicted mask quality.</p> <code>0.8</code> <code>stability_score_thresh</code> <code>float</code> <p>A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.</p> <code>0.95</code> <code>stability_score_offset</code> <code>float</code> <p>The amount to shift the cutoff when calculated the stability score.</p> <code>1.0</code> <code>mask_threshold</code> <code>float</code> <p>Threshold for binarizing the mask logits</p> <code>0.0</code> <code>box_nms_thresh</code> <code>float</code> <p>The box IoU cutoff used by non-maximal suppression to filter duplicate masks.</p> <code>0.7</code> <code>crop_n_layers</code> <code>int</code> <p>If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.</p> <code>0</code> <code>crop_nms_thresh</code> <code>float</code> <p>The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.</p> <code>0.7</code> <code>crop_overlap_ratio</code> <code>float</code> <p>Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.</p> <code>512 / 1500</code> <code>crop_n_points_downscale_factor</code> <code>int</code> <p>The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.</p> <code>1</code> <code>point_grids</code> <code>list(ndarray) or None</code> <p>A list over explicit grids of points used for sampling, normalized to [0,1]. The nth grid in the list is used in the nth crop layer. Exclusive with points_per_side.</p> <code>None</code> <code>min_mask_region_area</code> <code>int</code> <p>If &gt;0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than min_mask_region_area. Requires opencv.</p> <code>0</code> <code>output_mode</code> <code>str</code> <p>The form masks are returned in. Can be 'binary_mask', 'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools. For large resolutions, 'binary_mask' may consume large amounts of memory.</p> <code>'binary_mask'</code> <code>use_m2m</code> <code>bool</code> <p>Whether to add a one step refinement using previous mask predictions.</p> <code>False</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>max_hole_area</code> <code>int</code> <p>If max_hole_area &gt; 0, we fill small holes in up to the maximum area of max_hole_area in low_res_masks.</p> <code>0.0</code> <code>max_sprinkle_area</code> <code>int</code> <p>If max_sprinkle_area &gt; 0, we remove small sprinkles up to the maximum area of max_sprinkle_area in low_res_masks.</p> <code>0.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"sam2-hiera-large\",\n    device: Optional[str] = None,\n    empty_cache: bool = True,\n    automatic: bool = True,\n    video: bool = False,\n    mode: str = \"eval\",\n    hydra_overrides_extra: Optional[List[str]] = None,\n    apply_postprocessing: bool = False,\n    points_per_side: Optional[int] = 32,\n    points_per_batch: int = 64,\n    pred_iou_thresh: float = 0.8,\n    stability_score_thresh: float = 0.95,\n    stability_score_offset: float = 1.0,\n    mask_threshold: float = 0.0,\n    box_nms_thresh: float = 0.7,\n    crop_n_layers: int = 0,\n    crop_nms_thresh: float = 0.7,\n    crop_overlap_ratio: float = 512 / 1500,\n    crop_n_points_downscale_factor: int = 1,\n    point_grids: Optional[List[np.ndarray]] = None,\n    min_mask_region_area: int = 0,\n    output_mode: str = \"binary_mask\",\n    use_m2m: bool = False,\n    multimask_output: bool = False,\n    max_hole_area: float = 0.0,\n    max_sprinkle_area: float = 0.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the SamGeo2 class.\n\n    Args:\n        model_id (str): The model ID to use. Can be one of the following: \"sam2-hiera-tiny\",\n            \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\".\n            Defaults to \"sam2-hiera-large\".\n        device (Optional[str]): The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.\n        empty_cache (bool): Whether to empty the cache. Defaults to True.\n        automatic (bool): Whether to use automatic mask generation. Defaults to True.\n        video (bool): Whether to use video prediction. Defaults to False.\n        mode (str): The mode to use. Defaults to \"eval\".\n        hydra_overrides_extra (Optional[List[str]]): Additional Hydra overrides. Defaults to None.\n        apply_postprocessing (bool): Whether to apply postprocessing. Defaults to False.\n        points_per_side (int or None): The number of points to be sampled\n            along one side of the image. The total number of points is\n            points_per_side**2. If None, 'point_grids' must provide explicit\n            point sampling.\n        points_per_batch (int): Sets the number of points run simultaneously\n            by the model. Higher numbers may be faster but use more GPU memory.\n        pred_iou_thresh (float): A filtering threshold in [0,1], using the\n            model's predicted mask quality.\n        stability_score_thresh (float): A filtering threshold in [0,1], using\n            the stability of the mask under changes to the cutoff used to binarize\n            the model's mask predictions.\n        stability_score_offset (float): The amount to shift the cutoff when\n            calculated the stability score.\n        mask_threshold (float): Threshold for binarizing the mask logits\n        box_nms_thresh (float): The box IoU cutoff used by non-maximal\n            suppression to filter duplicate masks.\n        crop_n_layers (int): If &gt;0, mask prediction will be run again on\n            crops of the image. Sets the number of layers to run, where each\n            layer has 2**i_layer number of image crops.\n        crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n            suppression to filter duplicate masks between different crops.\n        crop_overlap_ratio (float): Sets the degree to which crops overlap.\n            In the first crop layer, crops will overlap by this fraction of\n            the image length. Later layers with more crops scale down this overlap.\n        crop_n_points_downscale_factor (int): The number of points-per-side\n            sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n        point_grids (list(np.ndarray) or None): A list over explicit grids\n            of points used for sampling, normalized to [0,1]. The nth grid in the\n            list is used in the nth crop layer. Exclusive with points_per_side.\n        min_mask_region_area (int): If &gt;0, postprocessing will be applied\n            to remove disconnected regions and holes in masks with area smaller\n            than min_mask_region_area. Requires opencv.\n        output_mode (str): The form masks are returned in. Can be 'binary_mask',\n            'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools.\n            For large resolutions, 'binary_mask' may consume large amounts of\n            memory.\n        use_m2m (bool): Whether to add a one step refinement using previous mask predictions.\n        multimask_output (bool): Whether to output multimask at each point of the grid.\n            Defaults to False.\n        max_hole_area (int): If max_hole_area &gt; 0, we fill small holes in up to\n            the maximum area of max_hole_area in low_res_masks.\n        max_sprinkle_area (int): If max_sprinkle_area &gt; 0, we remove small sprinkles up to\n            the maximum area of max_sprinkle_area in low_res_masks.\n        **kwargs (Any): Additional keyword arguments to pass to\n            SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().\n    \"\"\"\n    if isinstance(model_id, str):\n        if not model_id.startswith(\"facebook/\"):\n            model_id = f\"facebook/{model_id}\"\n    else:\n        raise ValueError(\"model_id must be a string\")\n\n    allowed_models = [\n        \"facebook/sam2-hiera-tiny\",\n        \"facebook/sam2-hiera-small\",\n        \"facebook/sam2-hiera-base-plus\",\n        \"facebook/sam2-hiera-large\",\n    ]\n\n    if model_id not in allowed_models:\n        raise ValueError(\n            f\"model_id must be one of the following: {', '.join(allowed_models)}\"\n        )\n\n    if device is None:\n        device = common.choose_device(empty_cache=empty_cache)\n\n    if hydra_overrides_extra is None:\n        hydra_overrides_extra = []\n\n    self.model_id = model_id\n    self.model_version = \"sam2\"\n    self.device = device\n\n    if video:\n        automatic = False\n\n    if automatic:\n        self.mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            points_per_side=points_per_side,\n            points_per_batch=points_per_batch,\n            pred_iou_thresh=pred_iou_thresh,\n            stability_score_thresh=stability_score_thresh,\n            stability_score_offset=stability_score_offset,\n            mask_threshold=mask_threshold,\n            box_nms_thresh=box_nms_thresh,\n            crop_n_layers=crop_n_layers,\n            crop_nms_thresh=crop_nms_thresh,\n            crop_overlap_ratio=crop_overlap_ratio,\n            crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n            point_grids=point_grids,\n            min_mask_region_area=min_mask_region_area,\n            output_mode=output_mode,\n            use_m2m=use_m2m,\n            multimask_output=multimask_output,\n            **kwargs,\n        )\n    elif video:\n        self.predictor = SAM2VideoPredictor.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            **kwargs,\n        )\n    else:\n        self.predictor = SAM2ImagePredictor.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            mask_threshold=mask_threshold,\n            max_hole_area=max_hole_area,\n            max_sprinkle_area=max_sprinkle_area,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.add_new_mask","title":"<code>add_new_mask(inference_state, frame_idx, obj_id, mask)</code>","text":"<p>Add a new mask to the inference state.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index.</p> required <code>obj_id</code> <code>int</code> <p>The object ID.</p> required <code>mask</code> <code>ndarray</code> <p>The mask to add.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The updated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef add_new_mask(\n    self,\n    inference_state: Any,\n    frame_idx: int,\n    obj_id: int,\n    mask: np.ndarray,\n) -&gt; Any:\n    \"\"\"Add a new mask to the inference state.\n\n    Args:\n        inference_state (Any): The current inference state.\n        frame_idx (int): The frame index.\n        obj_id (int): The object ID.\n        mask (np.ndarray): The mask to add.\n\n    Returns:\n        Any: The updated inference state.\n    \"\"\"\n    return self.predictor.add_new_mask(inference_state, frame_idx, obj_id, mask)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.add_new_points_or_box","title":"<code>add_new_points_or_box(inference_state, frame_idx, obj_id, points=None, labels=None, clear_old_points=True, normalize_coords=True, box=None)</code>","text":"<p>Add new points or a box to the inference state.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index.</p> required <code>obj_id</code> <code>int</code> <p>The object ID.</p> required <code>points</code> <code>Optional[ndarray]</code> <p>The points to add. Defaults to None.</p> <code>None</code> <code>labels</code> <code>Optional[ndarray]</code> <p>The labels for the points. Defaults to None.</p> <code>None</code> <code>clear_old_points</code> <code>bool</code> <p>Whether to clear old points. Defaults to True.</p> <code>True</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>box</code> <code>Optional[ndarray]</code> <p>The bounding box to add. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The updated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef add_new_points_or_box(\n    self,\n    inference_state: Any,\n    frame_idx: int,\n    obj_id: int,\n    points: Optional[np.ndarray] = None,\n    labels: Optional[np.ndarray] = None,\n    clear_old_points: bool = True,\n    normalize_coords: bool = True,\n    box: Optional[np.ndarray] = None,\n) -&gt; Any:\n    \"\"\"Add new points or a box to the inference state.\n\n    Args:\n        inference_state (Any): The current inference state.\n        frame_idx (int): The frame index.\n        obj_id (int): The object ID.\n        points (Optional[np.ndarray]): The points to add. Defaults to None.\n        labels (Optional[np.ndarray]): The labels for the points. Defaults to None.\n        clear_old_points (bool): Whether to clear old points. Defaults to True.\n        normalize_coords (bool): Whether to normalize the coordinates. Defaults to True.\n        box (Optional[np.ndarray]): The bounding box to add. Defaults to None.\n\n    Returns:\n        Any: The updated inference state.\n    \"\"\"\n    return self.predictor.add_new_points_or_box(\n        inference_state,\n        frame_idx,\n        obj_id,\n        points=points,\n        labels=labels,\n        clear_old_points=clear_old_points,\n        normalize_coords=normalize_coords,\n        box=box,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.generate","title":"<code>generate(source, output=None, foreground=True, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, ndarray]</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>Optional[Tuple[int, int]]</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing the generated masks.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def generate(\n    self,\n    source: Union[str, np.ndarray],\n    output: Optional[str] = None,\n    foreground: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: int = None,\n    **kwargs: Any,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate masks for the input image.\n\n    Args:\n        source (Union[str, np.ndarray]): The path to the input image or the\n            input image as a numpy array.\n        output (Optional[str]): The path to the output image. Defaults to None.\n        foreground (bool): Whether to generate the foreground mask. Defaults\n            to True.\n        erosion_kernel (Optional[Tuple[int, int]]): The erosion kernel for\n            filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range,\n            for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool): Whether to assign a unique value to each object.\n            Defaults to True.\n            The unique value increases from 1 to the number of objects. The\n            larger the number, the larger the object area.\n        min_size (int): The minimum size of the object. Defaults to 0.\n        max_size (int): The maximum size of the object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = common.download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self._min_size = min_size\n    self._max_size = max_size\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.init_state","title":"<code>init_state(video_path, offload_video_to_cpu=False, offload_state_to_cpu=False, async_loading_frames=False)</code>","text":"<p>Initialize an inference state.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>offload_video_to_cpu</code> <code>bool</code> <p>Whether to offload the video to CPU. Defaults to False.</p> <code>False</code> <code>offload_state_to_cpu</code> <code>bool</code> <p>Whether to offload the state to CPU. Defaults to False.</p> <code>False</code> <code>async_loading_frames</code> <code>bool</code> <p>Whether to load frames asynchronously. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef init_state(\n    self,\n    video_path: str,\n    offload_video_to_cpu: bool = False,\n    offload_state_to_cpu: bool = False,\n    async_loading_frames: bool = False,\n) -&gt; Any:\n    \"\"\"Initialize an inference state.\n\n    Args:\n        video_path (str): The path to the video file.\n        offload_video_to_cpu (bool): Whether to offload the video to CPU.\n            Defaults to False.\n        offload_state_to_cpu (bool): Whether to offload the state to CPU.\n            Defaults to False.\n        async_loading_frames (bool): Whether to load frames asynchronously.\n            Defaults to False.\n\n    Returns:\n        Any: The initialized inference state.\n    \"\"\"\n    return self.predictor.init_state(\n        video_path,\n        offload_video_to_cpu=offload_video_to_cpu,\n        offload_state_to_cpu=offload_state_to_cpu,\n        async_loading_frames=async_loading_frames,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, mask_input=None, multimask_output=False, return_logits=False, normalize_coords=True, point_crs=None, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict the mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>ndarray</code> <p>The point coordinates. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>ndarray</code> <p>The point labels. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask, and the logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict(\n    self,\n    point_coords: Optional[np.ndarray] = None,\n    point_labels: Optional[np.ndarray] = None,\n    boxes: Optional[np.ndarray] = None,\n    mask_input: Optional[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords: bool = True,\n    point_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"float32\",\n    return_results: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Predict the mask for the input image.\n\n    Args:\n        point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n        point_labels (np.ndarray, optional): The point labels. Defaults to None.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        multimask_output (bool, optional): Whether to output multimask at each\n            point of the grid. Defaults to False.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        normalize_coords (bool, optional): Whether to normalize the coordinates.\n            Defaults to True.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks,\n            scores, and logits. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n            and the logits.\n    \"\"\"\n    import geopandas as gpd\n\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = common.vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = common.geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = common.coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = common.bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n\n    self.boxes = input_boxes\n\n    masks, scores, logits = predictor.predict(\n        point_coords=point_coords,\n        point_labels=point_labels,\n        box=input_boxes,\n        mask_input=mask_input,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_batch","title":"<code>predict_batch(point_coords_batch=None, point_labels_batch=None, box_batch=None, mask_input_batch=None, multimask_output=False, return_logits=False, normalize_coords=True)</code>","text":"<p>Predict masks for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of point coordinates. Defaults to None.</p> <code>None</code> <code>point_labels_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of point labels. Defaults to None.</p> <code>None</code> <code>box_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of bounding boxes. Defaults to None.</p> <code>None</code> <code>mask_input_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of mask inputs. Defaults to None.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>Whether to return the logits. Defaults to False.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[List[ndarray], List[ndarray], List[ndarray]]</code> <p>Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists of masks, multimasks, and logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_batch(\n    self,\n    point_coords_batch: List[np.ndarray] = None,\n    point_labels_batch: List[np.ndarray] = None,\n    box_batch: List[np.ndarray] = None,\n    mask_input_batch: List[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords=True,\n) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Predict masks for a batch of images.\n\n    Args:\n        point_coords_batch (Optional[List[np.ndarray]]): A batch of point\n            coordinates. Defaults to None.\n        point_labels_batch (Optional[List[np.ndarray]]): A batch of point\n            labels. Defaults to None.\n        box_batch (Optional[List[np.ndarray]]): A batch of bounding boxes.\n            Defaults to None.\n        mask_input_batch (Optional[List[np.ndarray]]): A batch of mask inputs.\n            Defaults to None.\n        multimask_output (bool): Whether to output multimask at each point\n            of the grid. Defaults to False.\n        return_logits (bool): Whether to return the logits. Defaults to False.\n        normalize_coords (bool): Whether to normalize the coordinates.\n            Defaults to True.\n\n    Returns:\n        Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists\n            of masks, multimasks, and logits.\n    \"\"\"\n\n    return self.predictor.predict_batch(\n        point_coords_batch=point_coords_batch,\n        point_labels_batch=point_labels_batch,\n        box_batch=box_batch,\n        mask_input_batch=mask_input_batch,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_by_points","title":"<code>predict_by_points(point_coords_batch=None, point_labels_batch=None, box_batch=None, mask_input_batch=None, multimask_output=False, return_logits=False, normalize_coords=True, point_crs=None, output=None, index=None, unique=True, mask_multiplier=255, dtype='int32', return_results=False, **kwargs)</code>","text":"<p>Predict the mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>ndarray</code> <p>The point coordinates. Defaults to None.</p> required <code>point_labels</code> <code>ndarray</code> <p>The point labels. Defaults to None.</p> required <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> required <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> required <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to True.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.int32.</p> <code>'int32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask, and the logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_by_points(\n    self,\n    point_coords_batch: List[np.ndarray] = None,\n    point_labels_batch: List[np.ndarray] = None,\n    box_batch: List[np.ndarray] = None,\n    mask_input_batch: List[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords=True,\n    point_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    index: Optional[int] = None,\n    unique: bool = True,\n    mask_multiplier: int = 255,\n    dtype: str = \"int32\",\n    return_results: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Predict the mask for the input image.\n\n    Args:\n        point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n        point_labels (np.ndarray, optional): The point labels. Defaults to None.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        multimask_output (bool, optional): Whether to output multimask at each\n            point of the grid. Defaults to True.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        normalize_coords (bool, optional): Whether to normalize the coordinates.\n            Defaults to True.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.int32.\n        return_results (bool, optional): Whether to return the predicted masks,\n            scores, and logits. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n            and the logits.\n    \"\"\"\n    import geopandas as gpd\n\n    if hasattr(self, \"image_batch\") and self.image_batch is not None:\n        pass\n    elif self.image is not None:\n        self.predictor.set_image_batch([self.image])\n        setattr(self, \"image_batch\", [self.image])\n    else:\n        raise ValueError(\"Please set the input image first using set_image().\")\n\n    if isinstance(point_coords_batch, dict):\n        point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n    if isinstance(point_coords_batch, str) or isinstance(\n        point_coords_batch, gpd.GeoDataFrame\n    ):\n        if isinstance(point_coords_batch, str):\n            gdf = gpd.read_file(point_coords_batch)\n        else:\n            gdf = point_coords_batch\n        if gdf.crs is None and (point_crs is not None):\n            gdf.crs = point_crs\n\n        points = gdf.geometry.apply(lambda geom: [geom.x, geom.y])\n        coordinates_array = np.array([[point] for point in points])\n        points = common.coords_to_xy(self.source, coordinates_array, point_crs)\n        num_points = points.shape[0]\n        if point_labels_batch is None:\n            labels = np.array([[1] for i in range(num_points)])\n        else:\n            labels = point_labels_batch\n\n    elif isinstance(point_coords_batch, list):\n        if point_crs is not None:\n            point_coords_batch_crs = common.coords_to_xy(\n                self.source, point_coords_batch, point_crs\n            )\n        else:\n            point_coords_batch_crs = point_coords_batch\n        num_points = len(point_coords_batch)\n\n        points = []\n        points.append([[point] for point in point_coords_batch_crs])\n\n        if point_labels_batch is None:\n            labels = np.array([[1] for i in range(num_points)])\n        elif isinstance(point_labels_batch, list):\n            labels = []\n            labels.append([[label] for label in point_labels_batch])\n            labels = labels[0]\n        else:\n            labels = point_labels_batch\n\n        points = np.array(points[0])\n        labels = np.array(labels)\n\n    elif isinstance(point_coords_batch, np.ndarray):\n        points = point_coords_batch\n        labels = point_labels_batch\n    else:\n        raise ValueError(\"point_coords must be a list, a GeoDataFrame, or a path.\")\n\n    predictor = self.predictor\n\n    masks_batch, scores_batch, logits_batch = predictor.predict_batch(\n        point_coords_batch=[points],\n        point_labels_batch=[labels],\n        box_batch=box_batch,\n        mask_input_batch=mask_input_batch,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    masks = masks_batch[0]\n    scores = scores_batch[0]\n    logits = logits_batch[0]\n\n    if multimask_output and (index is not None):\n        masks = masks[:, index, :, :]\n\n    if masks.ndim &gt; 3:\n        masks = masks.squeeze()\n\n    output_masks = []\n    sums = np.sum(masks, axis=(1, 2))\n    for index, mask in enumerate(masks):\n        item = {\"segmentation\": mask.astype(\"bool\"), \"area\": sums[index]}\n        output_masks.append(item)\n\n    self.masks = output_masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        self.save_masks(\n            output,\n            foreground=True,\n            unique=unique,\n            mask_multiplier=mask_multiplier,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    if return_results:\n        return output_masks, scores, logits\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_video","title":"<code>predict_video(prompts=None, point_crs=None, output_dir=None, img_ext='png')</code>","text":"<p>Predict masks for the video.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Dict[int, Any]</code> <p>A dictionary containing the prompts with points and labels.</p> <code>None</code> <code>point_crs</code> <code>Optional[str]</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>The directory to save the output images. Defaults to None.</p> <code>None</code> <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_video(\n    self,\n    prompts: Dict[int, Any] = None,\n    point_crs: Optional[str] = None,\n    output_dir: Optional[str] = None,\n    img_ext: str = \"png\",\n) -&gt; None:\n    \"\"\"Predict masks for the video.\n\n    Args:\n        prompts (Dict[int, Any]): A dictionary containing the prompts with points and labels.\n        point_crs (Optional[str]): The coordinate reference system (CRS) of the point prompts.\n        output_dir (Optional[str]): The directory to save the output images. Defaults to None.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n    \"\"\"\n\n    from PIL import Image\n\n    def save_image_from_dict(data, output_path=\"output_image.png\"):\n        # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n        array_shape = next(iter(data.values())).shape[1:]\n\n        # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n        output_array = np.zeros(array_shape, dtype=np.uint8)\n\n        # Iterate over each key and array in the dictionary\n        for key, array in data.items():\n            # Assign the key value wherever the boolean array is True\n            output_array[array[0]] = key\n\n        # Convert the output array to a PIL image\n        image = Image.fromarray(output_array)\n\n        # Save the image\n        image.save(output_path)\n\n    if prompts is None:\n        if hasattr(self, \"prompts\"):\n            prompts = self.prompts\n        else:\n            raise ValueError(\"Please provide prompts.\")\n\n    if point_crs is not None and self._tif_source is not None:\n        for prompt in prompts.values():\n            points = prompt.get(\"points\", None)\n            if points is not None:\n                points = common.coords_to_xy(self._tif_source, points, point_crs)\n                prompt[\"points\"] = points\n            box = prompt.get(\"box\", None)\n            if box is not None:\n                box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                prompt[\"box\"] = box\n\n    prompts = self._convert_prompts(prompts)\n    predictor = self.predictor\n    inference_state = self.inference_state\n    for obj_id, prompt in prompts.items():\n        points = prompt.get(\"points\", None)\n        labels = prompt.get(\"labels\", None)\n        box = prompt.get(\"box\", None)\n        frame_idx = prompt.get(\"frame_idx\", None)\n\n        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n            inference_state=inference_state,\n            frame_idx=frame_idx,\n            obj_id=obj_id,\n            points=points,\n            labels=labels,\n            box=box,\n        )\n\n    video_segments = {}\n    num_frames = self._num_images\n    num_digits = len(str(num_frames))\n\n    if output_dir is not None:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n        inference_state\n    ):\n        video_segments[out_frame_idx] = {\n            out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n            for i, out_obj_id in enumerate(out_obj_ids)\n        }\n\n        if output_dir is not None:\n            output_path = os.path.join(\n                output_dir, f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n            )\n            save_image_from_dict(video_segments[out_frame_idx], output_path)\n\n    self.video_segments = video_segments\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.propagate_in_video","title":"<code>propagate_in_video(inference_state, start_frame_idx=None, max_frame_num_to_track=None, reverse=False)</code>","text":"<p>Propagate the inference state in video.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>start_frame_idx</code> <code>Optional[int]</code> <p>The starting frame index. Defaults to None.</p> <code>None</code> <code>max_frame_num_to_track</code> <code>Optional[int]</code> <p>The maximum number of frames to track. Defaults to None.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Whether to propagate in reverse. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The propagated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef propagate_in_video(\n    self,\n    inference_state: Any,\n    start_frame_idx: Optional[int] = None,\n    max_frame_num_to_track: Optional[int] = None,\n    reverse: bool = False,\n) -&gt; Any:\n    \"\"\"Propagate the inference state in video.\n\n    Args:\n        inference_state (Any): The current inference state.\n        start_frame_idx (Optional[int]): The starting frame index. Defaults to None.\n        max_frame_num_to_track (Optional[int]): The maximum number of frames\n            to track. Defaults to None.\n        reverse (bool): Whether to propagate in reverse. Defaults to False.\n\n    Returns:\n        Any: The propagated inference state.\n    \"\"\"\n    return self.predictor.propagate_in_video(\n        inference_state,\n        start_frame_idx=start_frame_idx,\n        max_frame_num_to_track=max_frame_num_to_track,\n        reverse=reverse,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.propagate_in_video_preflight","title":"<code>propagate_in_video_preflight(inference_state)</code>","text":"<p>Propagate the inference state in video preflight.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The propagated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef propagate_in_video_preflight(self, inference_state: Any) -&gt; Any:\n    \"\"\"Propagate the inference state in video preflight.\n\n    Args:\n        inference_state (Any): The current inference state.\n\n    Returns:\n        Any: The propagated inference state.\n    \"\"\"\n    return self.predictor.propagate_in_video_preflight(inference_state)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.raster_to_vector","title":"<code>raster_to_vector(raster, vector, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a raster image file to a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>The path to the raster image.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def raster_to_vector(self, raster, vector, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a raster image file to a vector dataset.\n\n    Args:\n        raster (str): The path to the raster image.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    common.raster_to_vector(\n        raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to use for properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def region_groups(\n    self,\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[\n    Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to use for properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    return common.region_groups(\n        image,\n        connectivity=connectivity,\n        min_size=min_size,\n        max_size=max_size,\n        threshold=threshold,\n        properties=properties,\n        intensity_image=intensity_image,\n        out_csv=out_csv,\n        out_vector=out_vector,\n        out_image=out_image,\n        **kwargs,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.reset_state","title":"<code>reset_state(inference_state)</code>","text":"<p>Remove all input points or masks in all frames throughout the video.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef reset_state(self, inference_state: Any) -&gt; None:\n    \"\"\"Remove all input points or masks in all frames throughout the video.\n\n    Args:\n        inference_state (Any): The current inference state.\n    \"\"\"\n    self.predictor.reset_state(inference_state)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for common.array_to_image().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_masks(\n    self,\n    output: Optional[str] = None,\n    foreground: bool = True,\n    unique: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    min_size: int = 0,\n    max_size: int = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the masks to the output path. The output is either a binary mask\n    or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to\n            None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask.\n            Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each\n            object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering\n            object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to\n            None.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1]. You can use this\n            parameter to scale the mask to a larger range, for example\n            [0, 255]. Defaults to 255.\n        min_size (int, optional): The minimum size of the object. Defaults to 0.\n        max_size (int, optional): The maximum size of the object. Defaults to None.\n        **kwargs: Additional keyword arguments for common.array_to_image().\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in descending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        count = len(sorted_masks)\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and ann[\"area\"] &gt; max_size:\n                continue\n            objects[m] = count - index\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and m[\"area\"] &gt; max_size:\n                continue\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        common.array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype='float32', vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>str</code> <p>The data type of the output image. Defaults to \"float32\".</p> <code>'float32'</code> <code>vector</code> <code>Optional[str]</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_prediction(\n    self,\n    output: str,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"float32\",\n    vector: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (Optional[int], optional): The index of the mask to save.\n            Defaults to None, which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1].\n        dtype (str, optional): The data type of the output image. Defaults\n            to \"float32\".\n        vector (Optional[str], optional): The path to the output vector file.\n            Defaults to None.\n        simplify_tolerance (Optional[float], optional): The maximum allowed\n            geometry displacement. The higher this value, the smaller the\n            number of vertices in the resulting geometry.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        common.raster_to_vector(\n            output, vector, simplify_tolerance=simplify_tolerance\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_video_segments","title":"<code>save_video_segments(output_dir, img_ext='png')</code>","text":"<p>Save the video segments to the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The path to the output directory.</p> required <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_video_segments(self, output_dir: str, img_ext: str = \"png\") -&gt; None:\n    \"\"\"Save the video segments to the output directory.\n\n    Args:\n        output_dir (str): The path to the output directory.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n    \"\"\"\n    from PIL import Image\n\n    def save_image_from_dict(\n        data, output_path=\"output_image.png\", crs_source=None, **kwargs\n    ):\n        # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n        array_shape = next(iter(data.values())).shape[1:]\n\n        # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n        output_array = np.zeros(array_shape, dtype=np.uint8)\n\n        # Iterate over each key and array in the dictionary\n        for key, array in data.items():\n            # Assign the key value wherever the boolean array is True\n            output_array[array[0]] = key\n\n        if crs_source is None:\n            # Convert the output array to a PIL image\n            image = Image.fromarray(output_array)\n\n            # Save the image\n            image.save(output_path)\n        else:\n            output_path = output_path.replace(\".png\", \".tif\")\n            common.array_to_image(output_array, output_path, crs_source, **kwargs)\n\n    num_frames = len(self.video_segments)\n    num_digits = len(str(num_frames))\n\n    if hasattr(self, \"_tif_source\") and self._tif_source.endswith(\".tif\"):\n        crs_source = self._tif_source\n        filenames = self._tif_names\n    else:\n        crs_source = None\n        filenames = None\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Initialize the tqdm progress bar\n    for frame_idx, video_segment in tqdm(\n        self.video_segments.items(), desc=\"Rendering frames\", total=num_frames\n    ):\n        if filenames is None:\n            output_path = os.path.join(\n                output_dir, f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n            )\n        else:\n            output_path = os.path.join(output_dir, filenames[frame_idx])\n        save_image_from_dict(video_segment, output_path, crs_source)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_video_segments_blended","title":"<code>save_video_segments_blended(output_dir, img_ext='png', alpha=0.6, dpi=200, frame_stride=1, output_video=None, fps=30)</code>","text":"<p>Save blended video segments to the output directory and optionally create a video.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory to save the output images.</p> required <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the blended masks. Defaults to 0.6.</p> <code>0.6</code> <code>dpi</code> <code>int</code> <p>The DPI (dots per inch) for the output images. Defaults to 200.</p> <code>200</code> <code>frame_stride</code> <code>int</code> <p>The stride for selecting frames to save. Defaults to 1.</p> <code>1</code> <code>output_video</code> <code>Optional[str]</code> <p>The path to the output video file. Defaults to None.</p> <code>None</code> <code>fps</code> <code>int</code> <p>The frames per second for the output video. Defaults to 30.</p> <code>30</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_video_segments_blended(\n    self,\n    output_dir: str,\n    img_ext: str = \"png\",\n    alpha: float = 0.6,\n    dpi: int = 200,\n    frame_stride: int = 1,\n    output_video: Optional[str] = None,\n    fps: int = 30,\n) -&gt; None:\n    \"\"\"Save blended video segments to the output directory and optionally create a video.\n\n    Args:\n        output_dir (str): The directory to save the output images.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n        alpha (float): The alpha value for the blended masks. Defaults to 0.6.\n\n        dpi (int): The DPI (dots per inch) for the output images. Defaults to 200.\n        frame_stride (int): The stride for selecting frames to save. Defaults to 1.\n        output_video (Optional[str]): The path to the output video file. Defaults to None.\n        fps (int): The frames per second for the output video. Defaults to 30.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from PIL import Image\n\n    def show_mask(mask, ax, obj_id=None, random_color=False):\n        if random_color:\n            color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n        else:\n            cmap = plt.get_cmap(\"tab10\")\n            cmap_idx = 0 if obj_id is None else obj_id\n            color = np.array([*cmap(cmap_idx)[:3], alpha])\n        h, w = mask.shape[-2:]\n        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n        ax.imshow(mask_image)\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    plt.close(\"all\")\n\n    video_segments = self.video_segments\n    video_dir = self.video_path\n    frame_names = self._frame_names\n    num_frames = len(frame_names)\n    num_digits = len(str(num_frames))\n\n    # Initialize the tqdm progress bar\n    for out_frame_idx in tqdm(\n        range(0, len(frame_names), frame_stride), desc=\"Rendering frames\"\n    ):\n        image = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n\n        # Get original image dimensions\n        w, h = image.size\n\n        # Set DPI and calculate figure size based on the original image dimensions\n        figsize = (\n            w / dpi,\n            h / dpi,\n        )\n        figsize = (\n            figsize[0] * 1.3,\n            figsize[1] * 1.3,\n        )\n\n        # Create a figure with the exact size and DPI\n        fig = plt.figure(figsize=figsize, dpi=dpi)\n\n        # Disable axis to prevent whitespace\n        plt.axis(\"off\")\n\n        # Display the original image\n        plt.imshow(image)\n\n        # Overlay masks for each object ID\n        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n            show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n\n        # Save the figure with no borders or extra padding\n        filename = f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n        filepath = os.path.join(output_dir, filename)\n        plt.savefig(filepath, dpi=dpi, pad_inches=0, bbox_inches=\"tight\")\n        plt.close(fig)\n\n    if output_video is not None:\n        common.images_to_video(output_dir, output_video, fps=fps)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_image","title":"<code>set_image(image)</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray, Image]</code> <p>The input image as a path, a numpy array, or an Image.</p> required Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.no_grad()\ndef set_image(\n    self,\n    image: Union[str, np.ndarray, Image],\n) -&gt; None:\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (Union[str, np.ndarray, Image]): The input image as a path,\n            a numpy array, or an Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = common.download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray) or isinstance(image, Image):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_image_batch","title":"<code>set_image_batch(image_list)</code>","text":"<p>Set a batch of images for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>List[Union[ndarray, str, Image]]</code> <p>A list of images,</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an input image path does not exist or if the input image type is not supported.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.no_grad()\ndef set_image_batch(\n    self,\n    image_list: List[Union[np.ndarray, str, Image]],\n) -&gt; None:\n    \"\"\"Set a batch of images for prediction.\n\n    Args:\n        image_list (List[Union[np.ndarray, str, Image]]): A list of images,\n        which can be numpy arrays, file paths, or PIL images.\n\n    Raises:\n        ValueError: If an input image path does not exist or if the input\n            image type is not supported.\n    \"\"\"\n    images = []\n    for image in image_list:\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(image, Image):\n            image = np.array(image)\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        images.append(image)\n\n    self.predictor.set_image_batch(images)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_video","title":"<code>set_video(video_path, output_dir=None, frame_rate=None, prefix='')</code>","text":"<p>Set the video path and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>start_frame</code> <code>int</code> <p>The starting frame index. Defaults to 0.</p> required <code>end_frame</code> <code>Optional[int]</code> <p>The ending frame index. Defaults to None.</p> required <code>step</code> <code>int</code> <p>The step size. Defaults to 1.</p> required <code>frame_rate</code> <code>Optional[int]</code> <p>The frame rate. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def set_video(\n    self,\n    video_path: str,\n    output_dir: str = None,\n    frame_rate: Optional[int] = None,\n    prefix: str = \"\",\n) -&gt; None:\n    \"\"\"Set the video path and parameters.\n\n    Args:\n        video_path (str): The path to the video file.\n        start_frame (int, optional): The starting frame index. Defaults to 0.\n        end_frame (Optional[int], optional): The ending frame index. Defaults to None.\n        step (int, optional): The step size. Defaults to 1.\n        frame_rate (Optional[int], optional): The frame rate. Defaults to None.\n    \"\"\"\n\n    if isinstance(video_path, str):\n        if video_path.startswith(\"http\"):\n            video_path = common.download_file(video_path)\n        if os.path.isfile(video_path):\n            if output_dir is None:\n                output_dir = common.make_temp_dir()\n                if not os.path.exists(output_dir):\n                    os.makedirs(output_dir)\n            print(f\"Output directory: {output_dir}\")\n            common.video_to_images(\n                video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n            )\n\n        elif os.path.isdir(video_path):\n            files = sorted(os.listdir(video_path))\n            if len(files) == 0:\n                raise ValueError(f\"No files found in {video_path}.\")\n            elif files[0].endswith(\".tif\"):\n                self._tif_source = os.path.join(video_path, files[0])\n                self._tif_dir = video_path\n                self._tif_names = files\n                video_path = common.geotiff_to_jpg_batch(video_path)\n            output_dir = video_path\n\n        if not os.path.exists(video_path):\n            raise ValueError(f\"Input path {video_path} does not exist.\")\n    else:\n        raise ValueError(\"Input video_path must be a string.\")\n\n    self.video_path = output_dir\n    self._num_images = len(os.listdir(output_dir))\n    self._frame_names = sorted(os.listdir(output_dir))\n    self.inference_state = self.predictor.init_state(video_path=output_dir)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_anns(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    alpha: float = 0.35,\n    output: Optional[str] = None,\n    blend: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n            continue\n        if (\n            hasattr(self, \"_max_size\")\n            and isinstance(self._max_size, int)\n            and ann[\"area\"] &gt; self._max_size\n        ):\n            continue\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    # if \"dpi\" not in kwargs:\n    #     kwargs[\"dpi\"] = 100\n\n    # if \"bbox_inches\" not in kwargs:\n    #     kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = common.blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        common.array_to_image(array, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>fg_color</code> <code>Tuple[int, int, int]</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>Tuple[int, int, int]</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[list, list]</code> <p>Tuple[list, list]: A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_canvas(\n    self,\n    fg_color: Tuple[int, int, int] = (0, 255, 0),\n    bg_color: Tuple[int, int, int] = (0, 0, 255),\n    radius: int = 5,\n) -&gt; Tuple[list, list]:\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        fg_color (Tuple[int, int, int], optional): The color for the foreground points.\n            Defaults to (0, 255, 0).\n        bg_color (Tuple[int, int, int], optional): The color for the background points.\n            Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        Tuple[list, list]: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_images","title":"<code>show_images(path=None)</code>","text":"<p>Show the images in the video.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the images. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_images(self, path: str = None) -&gt; None:\n    \"\"\"Show the images in the video.\n\n    Args:\n        path (str, optional): The path to the images. Defaults to None.\n    \"\"\"\n    if path is None:\n        path = self.video_path\n\n    if path is not None:\n        common.show_image_gui(path)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>Optional[str]</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The map object.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_map(\n    self,\n    basemap: str = \"SATELLITE\",\n    repeat_mode: bool = True,\n    out_dir: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following:\n            SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for\n            draw control. Defaults to True.\n        out_dir (Optional[str], optional): The path to the output directory.\n            Defaults to None.\n\n    Returns:\n        Any: The map object.\n    \"\"\"\n    return common.sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_masks(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    cmap: str = \"binary_r\",\n    axis: str = \"off\",\n    foreground: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only.\n            Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.objects is None:\n        self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_prompts","title":"<code>show_prompts(prompts, frame_idx=0, mask=None, random_color=False, point_crs=None, figsize=(9, 6))</code>","text":"<p>Show the prompts on the image.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Dict[int, Any]</code> <p>A dictionary containing the prompts with points and labels.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index. Defaults to 0.</p> <code>0</code> <code>mask</code> <code>Any</code> <p>The mask. Defaults to None.</p> <code>None</code> <code>random_color</code> <code>bool</code> <p>Whether to use random colors for the masks. Defaults to False.</p> <code>False</code> <code>point_crs</code> <code>Optional[str]</code> <p>The coordinate reference system</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size. Defaults to (9, 6).</p> <code>(9, 6)</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_prompts(\n    self,\n    prompts: Dict[int, Any],\n    frame_idx: int = 0,\n    mask: Any = None,\n    random_color: bool = False,\n    point_crs: Optional[str] = None,\n    figsize: Tuple[int, int] = (9, 6),\n) -&gt; None:\n    \"\"\"Show the prompts on the image.\n\n    Args:\n        prompts (Dict[int, Any]): A dictionary containing the prompts with\n            points and labels.\n        frame_idx (int, optional): The frame index. Defaults to 0.\n        mask (Any, optional): The mask. Defaults to None.\n        random_color (bool, optional): Whether to use random colors for the\n            masks. Defaults to False.\n        point_crs (Optional[str], optional): The coordinate reference system\n        figsize (Tuple[int, int], optional): The figure size. Defaults to (9, 6).\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    from PIL import Image\n\n    def show_mask(mask, ax, obj_id=None, random_color=random_color):\n        if random_color:\n            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n        else:\n            cmap = plt.get_cmap(\"tab10\")\n            cmap_idx = 0 if obj_id is None else obj_id\n            color = np.array([*cmap(cmap_idx)[:3], 0.6])\n        h, w = mask.shape[-2:]\n        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n        ax.imshow(mask_image)\n\n    def show_points(coords, labels, ax, marker_size=200):\n        pos_points = coords[labels == 1]\n        neg_points = coords[labels == 0]\n        ax.scatter(\n            pos_points[:, 0],\n            pos_points[:, 1],\n            color=\"green\",\n            marker=\"*\",\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n        ax.scatter(\n            neg_points[:, 0],\n            neg_points[:, 1],\n            color=\"red\",\n            marker=\"*\",\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n\n    def show_box(box, ax):\n        x0, y0 = box[0], box[1]\n        w, h = box[2] - box[0], box[3] - box[1]\n        ax.add_patch(\n            plt.Rectangle(\n                (x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2\n            )\n        )\n\n    if point_crs is not None and self._tif_source is not None:\n        for prompt in prompts.values():\n            points = prompt.get(\"points\", None)\n            if points is not None:\n                points = common.coords_to_xy(self._tif_source, points, point_crs)\n                prompt[\"points\"] = points\n            box = prompt.get(\"box\", None)\n            if box is not None:\n                box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                prompt[\"box\"] = box\n\n    prompts = self._convert_prompts(prompts)\n    self.prompts = prompts\n    video_dir = self.video_path\n    frame_names = self._frame_names\n    fig = plt.figure(figsize=figsize)\n    fig.canvas.toolbar_visible = True\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = True\n    plt.title(f\"frame {frame_idx}\")\n    plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n\n    for obj_id, prompt in prompts.items():\n        points = prompt.get(\"points\", None)\n        labels = prompt.get(\"labels\", None)\n        box = prompt.get(\"box\", None)\n        anno_frame_idx = prompt.get(\"frame_idx\", None)\n        if anno_frame_idx == frame_idx:\n            if points is not None:\n                show_points(points, labels, plt.gca())\n            if box is not None:\n                show_box(box, plt.gca())\n            if mask is not None:\n                show_mask(mask, plt.gca(), obj_id=obj_id)\n\n    plt.show()\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args=None)</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>str</code> <p>The data type of the output image. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>save_args</code> <code>Optional[Dict[str, Any]]</code> <p>Optional arguments for saving the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The predicted mask as a numpy array, or None if output is specified.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def tensor_to_numpy(\n    self,\n    index: Optional[int] = None,\n    output: Optional[str] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"uint8\",\n    save_args: Optional[Dict[str, Any]] = None,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (Optional[int], optional): The index of the mask to save.\n            Defaults to None, which will save the mask with the highest score.\n        output (Optional[str], optional): The path to the output image.\n            Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1].\n        dtype (str, optional): The data type of the output image. Defaults\n            to \"uint8\".\n        save_args (Optional[Dict[str, Any]], optional): Optional arguments\n            for saving the output image. Defaults to None.\n\n    Returns:\n        Optional[np.ndarray]: The predicted mask as a numpy array, or None\n            if output is specified.\n    \"\"\"\n    if save_args is None:\n        save_args = {}\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 0\n\n    masks = masks[:, index, :, :]\n    if len(masks.shape) == 4 and masks.shape[1] == 1:\n        masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (_, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        common.array_to_image(\n            mask_overlay, output, self.source, dtype=dtype, **save_args\n        )\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"text_sam/","title":"text_sam module","text":"<p>The LangSAM model for segmenting objects from satellite images using text prompts. The source code is adapted from the https://github.com/luca-medeiros/lang-segment-anything repository. Credits to Luca Medeiros for the original implementation.</p>"},{"location":"text_sam/#samgeo.text_sam.LangSAM","title":"<code>LangSAM</code>","text":"<p>A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>class LangSAM:\n    \"\"\"\n    A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.\n    \"\"\"\n\n    def __init__(self, model_type=\"vit_h\", checkpoint=None):\n        \"\"\"Initialize the LangSAM instance.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the SAM 1\n                models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n                sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n        \"\"\"\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.build_groundingdino()\n        self.build_sam(model_type, checkpoint)\n\n        self.source = None\n        self.image = None\n        self.masks = None\n        self.boxes = None\n        self.phrases = None\n        self.logits = None\n        self.prediction = None\n\n    def build_sam(self, model_type, checkpoint_url=None):\n        \"\"\"Build the SAM model.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the SAM 1\n                models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n                sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n        \"\"\"\n        sam1_models = [\"vit_h\", \"vit_l\", \"vit_b\"]\n        sam2_models = [\n            \"sam2-hiera-tiny\",\n            \"sam2-hiera-small\",\n            \"sam2-hiera-base-plus\",\n            \"sam2-hiera-large\",\n        ]\n        if model_type in sam1_models:\n            if checkpoint_url is not None:\n                sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n            else:\n                checkpoint_url = SAM_MODELS[model_type]\n                sam = sam_model_registry[model_type]()\n                state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n                sam.load_state_dict(state_dict, strict=True)\n            sam.to(device=self.device)\n            self.sam = SamPredictor(sam)\n            self._sam_version = 1\n        elif model_type in sam2_models:\n            self.sam = SamGeo2(model_id=model_type, device=self.device, automatic=False)\n            self._sam_version = 2\n\n    def build_groundingdino(self):\n        \"\"\"Build the GroundingDINO model.\"\"\"\n        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n        ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n        self.groundingdino = load_model_hf(\n            ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n        )\n\n    def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n        \"\"\"\n        Run the GroundingDINO model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n\n        Returns:\n            tuple: Tuple containing boxes, logits, and phrases.\n        \"\"\"\n\n        image_trans = transform_image(image)\n        boxes, logits, phrases = predict(\n            model=self.groundingdino,\n            image=image_trans,\n            caption=text_prompt,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n            device=self.device,\n        )\n        W, H = image.size\n        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n        return boxes, logits, phrases\n\n    def predict_sam(self, image, boxes):\n        \"\"\"\n        Run the SAM model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            boxes (torch.Tensor): Tensor of bounding boxes.\n\n        Returns:\n            Masks tensor.\n        \"\"\"\n        if self._sam_version == 1:\n            image_array = np.asarray(image)\n            self.sam.set_image(image_array)\n            transformed_boxes = self.sam.transform.apply_boxes_torch(\n                boxes, image_array.shape[:2]\n            )\n            masks, _, _ = self.sam.predict_torch(\n                point_coords=None,\n                point_labels=None,\n                boxes=transformed_boxes.to(self.sam.device),\n                multimask_output=False,\n            )\n            return masks.cpu()\n        elif self._sam_version == 2:\n            if isinstance(self.source, str):\n                self.sam.set_image(self.source)\n            # If no source is set provide PIL image\n            if self.source is None:\n                self.sam.set_image(image)\n            self.sam.boxes = boxes.numpy().tolist()\n            masks, _, _ = self.sam.predict(\n                boxes=boxes.numpy().tolist(),\n                multimask_output=False,\n                return_results=True,\n            )\n            self.masks = masks\n            return masks\n\n    def set_image(self, image):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n    def predict(\n        self,\n        image,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        output=None,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        return_results=False,\n        return_coords=False,\n        detection_filter=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction.\n\n        Parameters:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            output (str, optional): Output path for the prediction. Defaults to None.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            return_results (bool, optional): Whether to return the results. Defaults to False.\n            detection_filter (callable, optional):\n                Callable with box, mask, logit, phrase, and index args returns a boolean.\n                If provided, the function will be called for each detected object.\n                Defaults to None.\n\n        Returns:\n            tuple: Tuple containing masks, boxes, phrases, and logits.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            # Load the georeferenced image\n            with rasterio.open(image) as src:\n                image_np = src.read().transpose(\n                    (1, 2, 0)\n                )  # Convert rasterio image to numpy array\n                self.transform = src.transform  # Save georeferencing information\n                self.crs = src.crs  # Save the Coordinate Reference System\n\n                if self.crs is None:\n                    warnings.warn(\n                        \"The CRS (Coordinate Reference System) \"\n                        \"of input image is None. \"\n                        \"Please define a projection on the input image \"\n                        \"before running segment-geospatial, \"\n                        \"or manually set CRS on result object \"\n                        \"like `sam.crs = 'EPSG:3857'`.\",\n                        UserWarning,\n                    )\n\n                image_pil = Image.fromarray(\n                    image_np[:, :, :3]\n                )  # Convert numpy array to PIL image, excluding the alpha channel\n        else:\n            image_pil = image\n            image_np = np.array(image_pil)\n\n        self.image = image_pil\n\n        boxes, logits, phrases = self.predict_dino(\n            image_pil, text_prompt, box_threshold, text_threshold\n        )\n        masks = torch.tensor([])\n        if len(boxes) &gt; 0:\n            masks = self.predict_sam(image_pil, boxes)\n            # If masks have 4 dimensions and the second dimension is 1 (e.g., [boxes, 1, height, width]),\n            # squeeze that dimension to reduce it to 3 dimensions ([boxes, height, width]).\n            # If boxes = 1, the mask's shape will be [1, height, width] after squeezing.\n            if masks.ndim == 4 and masks.shape[1] == 1:\n                masks = masks.squeeze(1)\n\n        if boxes.nelement() == 0:  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Create an empty mask overlay\n\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            # Validate the detection_filter argument\n            if detection_filter is not None:\n                if not callable(detection_filter):\n                    raise ValueError(\"detection_filter must be callable.\")\n\n                if not len(inspect.signature(detection_filter).parameters) == 5:\n                    raise ValueError(\n                        \"detection_filter required args: \"\n                        \"box, mask, logit, phrase, and index.\"\n                    )\n\n            for i, (box, mask, logit, phrase) in enumerate(\n                zip(boxes, masks, logits, phrases)\n            ):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n\n                # Apply the user-supplied filtering logic if provided\n                if detection_filter is not None:\n                    if not detection_filter(box, mask, logit, phrase, i):\n                        continue\n\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n        self.masks = masks\n        self.boxes = boxes\n        self.phrases = phrases\n        self.logits = logits\n        self.prediction = mask_overlay\n\n        if return_results:\n            return masks, boxes, phrases, logits\n\n        if return_coords:\n            boxlist = []\n            for box in self.boxes:\n                box = box.cpu().numpy()\n                boxlist.append((box[0], box[1]))\n            return boxlist\n\n    def predict_batch(\n        self,\n        images,\n        out_dir,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        merge=True,\n        verbose=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction for a batch of images.\n\n        Parameters:\n            images (list): List of input PIL Images.\n            out_dir (str): Output directory for the prediction.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n        \"\"\"\n\n        import glob\n\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n        if isinstance(images, str):\n            images = list(glob.glob(os.path.join(images, \"*.tif\")))\n            images.sort()\n\n        if not isinstance(images, list):\n            raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n        for i, image in enumerate(images):\n            basename = os.path.splitext(os.path.basename(image))[0]\n            if verbose:\n                print(\n                    f\"Processing image {str(i + 1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n                )\n            output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n            self.predict(\n                image,\n                text_prompt,\n                box_threshold,\n                text_threshold,\n                output=output,\n                mask_multiplier=mask_multiplier,\n                dtype=dtype,\n                save_args=save_args,\n                **kwargs,\n            )\n\n        if merge:\n            output = os.path.join(out_dir, \"merged.tif\")\n            merge_rasters(out_dir, output)\n            if verbose:\n                print(f\"Saved the merged prediction to {output}.\")\n\n    def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n        \"\"\"Save the bounding boxes to a vector file.\n\n        Args:\n            output (str): The path to the output vector file.\n            dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n            **kwargs: Additional arguments for boxes_to_vector().\n        \"\"\"\n\n        if self.boxes is None:\n            print(\"Please run predict() first.\")\n            return\n        else:\n            boxes = self.boxes.tolist()\n            coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n            if output is None:\n                return boxes_to_vector(coords, self.crs, dst_crs, output)\n            else:\n                boxes_to_vector(coords, self.crs, dst_crs, output)\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        cmap=\"viridis\",\n        alpha=0.4,\n        add_boxes=True,\n        box_color=\"r\",\n        box_linewidth=1,\n        title=None,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n            add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n            box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n            box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n            title (str, optional): The title for the image. Defaults to None.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n            kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n        \"\"\"\n\n        import warnings\n\n        import matplotlib.patches as patches\n        import matplotlib.pyplot as plt\n\n        warnings.filterwarnings(\"ignore\")\n\n        anns = self.prediction\n\n        if anns is None:\n            print(\"Please run predict() first.\")\n            return\n        elif len(anns) == 0:\n            print(\"No objects found in the image.\")\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        if add_boxes:\n            for box in self.boxes:\n                # Draw bounding box\n                box = box.cpu().numpy()  # Convert the tensor to a numpy array\n                rect = patches.Rectangle(\n                    (box[0], box[1]),\n                    box[2] - box[0],\n                    box[3] - box[1],\n                    linewidth=box_linewidth,\n                    edgecolor=box_color,\n                    facecolor=\"none\",\n                )\n                plt.gca().add_patch(rect)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n        if title is not None:\n            plt.title(title)\n        plt.axis(axis)\n\n        if output is not None:\n            if blend:\n                plt.savefig(output, **kwargs)\n            else:\n                array_to_image(self.prediction, output, self.source)\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n\n    def region_groups(\n        self,\n        image: Union[str, \"xr.DataArray\", np.ndarray],\n        connectivity: int = 1,\n        min_size: int = 10,\n        max_size: Optional[int] = None,\n        threshold: Optional[int] = None,\n        properties: Optional[List[str]] = None,\n        intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n        out_csv: Optional[str] = None,\n        out_vector: Optional[str] = None,\n        out_image: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[\n        Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n    ]:\n        \"\"\"\n        Segment regions in an image and filter them based on size.\n\n        Args:\n            image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n                path, xarray DataArray, or numpy array.\n            connectivity (int, optional): Connectivity for labeling. Defaults to 1\n                for 4-connectivity. Use 2 for 8-connectivity.\n            min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n            max_size (Optional[int], optional): Maximum size of regions to keep.\n                Defaults to None.\n            threshold (Optional[int], optional): Threshold for filling holes.\n                Defaults to None, which is equal to min_size.\n            properties (Optional[List[str]], optional): List of properties to measure.\n                See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n                Defaults to None.\n            intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n                Intensity image to use for properties. Defaults to None.\n            out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n                Defaults to None.\n            out_vector (Optional[str], optional): Path to save the vector file.\n                Defaults to None.\n            out_image (Optional[str], optional): Path to save the output image.\n                Defaults to None.\n\n        Returns:\n            Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n        \"\"\"\n        return self.sam.region_groups(\n            image,\n            connectivity=connectivity,\n            min_size=min_size,\n            max_size=max_size,\n            threshold=threshold,\n            properties=properties,\n            intensity_image=intensity_image,\n            out_csv=out_csv,\n            out_vector=out_vector,\n            out_image=out_image,\n            **kwargs,\n        )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.__init__","title":"<code>__init__(model_type='vit_h', checkpoint=None)</code>","text":"<p>Initialize the LangSAM instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the SAM 1 models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny, sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large) Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_url</code> <code>str</code> <p>The URL to the checkpoint file. Defaults to None</p> required Source code in <code>samgeo/text_sam.py</code> <pre><code>def __init__(self, model_type=\"vit_h\", checkpoint=None):\n    \"\"\"Initialize the LangSAM instance.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the SAM 1\n            models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n            sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n    \"\"\"\n\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.build_groundingdino()\n    self.build_sam(model_type, checkpoint)\n\n    self.source = None\n    self.image = None\n    self.masks = None\n    self.boxes = None\n    self.phrases = None\n    self.logits = None\n    self.prediction = None\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_groundingdino","title":"<code>build_groundingdino()</code>","text":"<p>Build the GroundingDINO model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_groundingdino(self):\n    \"\"\"Build the GroundingDINO model.\"\"\"\n    ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n    ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n    ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n    self.groundingdino = load_model_hf(\n        ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n    )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_sam","title":"<code>build_sam(model_type, checkpoint_url=None)</code>","text":"<p>Build the SAM model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the SAM 1 models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny, sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large) Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> required <code>checkpoint_url</code> <code>str</code> <p>The URL to the checkpoint file. Defaults to None</p> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_sam(self, model_type, checkpoint_url=None):\n    \"\"\"Build the SAM model.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the SAM 1\n            models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n            sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n    \"\"\"\n    sam1_models = [\"vit_h\", \"vit_l\", \"vit_b\"]\n    sam2_models = [\n        \"sam2-hiera-tiny\",\n        \"sam2-hiera-small\",\n        \"sam2-hiera-base-plus\",\n        \"sam2-hiera-large\",\n    ]\n    if model_type in sam1_models:\n        if checkpoint_url is not None:\n            sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n        else:\n            checkpoint_url = SAM_MODELS[model_type]\n            sam = sam_model_registry[model_type]()\n            state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n            sam.load_state_dict(state_dict, strict=True)\n        sam.to(device=self.device)\n        self.sam = SamPredictor(sam)\n        self._sam_version = 1\n    elif model_type in sam2_models:\n        self.sam = SamGeo2(model_id=model_type, device=self.device, automatic=False)\n        self._sam_version = 2\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict","title":"<code>predict(image, text_prompt, box_threshold, text_threshold, output=None, mask_multiplier=255, dtype=np.uint8, save_args={}, return_results=False, return_coords=False, detection_filter=None, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>output</code> <code>str</code> <p>Output path for the prediction. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>uint8</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>return_results</code> <code>bool</code> <p>Whether to return the results. Defaults to False.</p> <code>False</code> <code>detection_filter</code> <code>callable</code> <p>Callable with box, mask, logit, phrase, and index args returns a boolean. If provided, the function will be called for each detected object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing masks, boxes, phrases, and logits.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict(\n    self,\n    image,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    output=None,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    return_results=False,\n    return_coords=False,\n    detection_filter=None,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction.\n\n    Parameters:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        output (str, optional): Output path for the prediction. Defaults to None.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        return_results (bool, optional): Whether to return the results. Defaults to False.\n        detection_filter (callable, optional):\n            Callable with box, mask, logit, phrase, and index args returns a boolean.\n            If provided, the function will be called for each detected object.\n            Defaults to None.\n\n    Returns:\n        tuple: Tuple containing masks, boxes, phrases, and logits.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        # Load the georeferenced image\n        with rasterio.open(image) as src:\n            image_np = src.read().transpose(\n                (1, 2, 0)\n            )  # Convert rasterio image to numpy array\n            self.transform = src.transform  # Save georeferencing information\n            self.crs = src.crs  # Save the Coordinate Reference System\n\n            if self.crs is None:\n                warnings.warn(\n                    \"The CRS (Coordinate Reference System) \"\n                    \"of input image is None. \"\n                    \"Please define a projection on the input image \"\n                    \"before running segment-geospatial, \"\n                    \"or manually set CRS on result object \"\n                    \"like `sam.crs = 'EPSG:3857'`.\",\n                    UserWarning,\n                )\n\n            image_pil = Image.fromarray(\n                image_np[:, :, :3]\n            )  # Convert numpy array to PIL image, excluding the alpha channel\n    else:\n        image_pil = image\n        image_np = np.array(image_pil)\n\n    self.image = image_pil\n\n    boxes, logits, phrases = self.predict_dino(\n        image_pil, text_prompt, box_threshold, text_threshold\n    )\n    masks = torch.tensor([])\n    if len(boxes) &gt; 0:\n        masks = self.predict_sam(image_pil, boxes)\n        # If masks have 4 dimensions and the second dimension is 1 (e.g., [boxes, 1, height, width]),\n        # squeeze that dimension to reduce it to 3 dimensions ([boxes, height, width]).\n        # If boxes = 1, the mask's shape will be [1, height, width] after squeezing.\n        if masks.ndim == 4 and masks.shape[1] == 1:\n            masks = masks.squeeze(1)\n\n    if boxes.nelement() == 0:  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Create an empty mask overlay\n\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        # Validate the detection_filter argument\n        if detection_filter is not None:\n            if not callable(detection_filter):\n                raise ValueError(\"detection_filter must be callable.\")\n\n            if not len(inspect.signature(detection_filter).parameters) == 5:\n                raise ValueError(\n                    \"detection_filter required args: \"\n                    \"box, mask, logit, phrase, and index.\"\n                )\n\n        for i, (box, mask, logit, phrase) in enumerate(\n            zip(boxes, masks, logits, phrases)\n        ):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n\n            # Apply the user-supplied filtering logic if provided\n            if detection_filter is not None:\n                if not detection_filter(box, mask, logit, phrase, i):\n                    continue\n\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n    self.masks = masks\n    self.boxes = boxes\n    self.phrases = phrases\n    self.logits = logits\n    self.prediction = mask_overlay\n\n    if return_results:\n        return masks, boxes, phrases, logits\n\n    if return_coords:\n        boxlist = []\n        for box in self.boxes:\n            box = box.cpu().numpy()\n            boxlist.append((box[0], box[1]))\n        return boxlist\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_batch","title":"<code>predict_batch(images, out_dir, text_prompt, box_threshold, text_threshold, mask_multiplier=255, dtype=np.uint8, save_args={}, merge=True, verbose=True, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of input PIL Images.</p> required <code>out_dir</code> <code>str</code> <p>Output directory for the prediction.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>uint8</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>merge</code> <code>bool</code> <p>Whether to merge the predictions into a single GeoTIFF file. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_batch(\n    self,\n    images,\n    out_dir,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    merge=True,\n    verbose=True,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction for a batch of images.\n\n    Parameters:\n        images (list): List of input PIL Images.\n        out_dir (str): Output directory for the prediction.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n    \"\"\"\n\n    import glob\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(images, str):\n        images = list(glob.glob(os.path.join(images, \"*.tif\")))\n        images.sort()\n\n    if not isinstance(images, list):\n        raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n    for i, image in enumerate(images):\n        basename = os.path.splitext(os.path.basename(image))[0]\n        if verbose:\n            print(\n                f\"Processing image {str(i + 1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n            )\n        output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n        self.predict(\n            image,\n            text_prompt,\n            box_threshold,\n            text_threshold,\n            output=output,\n            mask_multiplier=mask_multiplier,\n            dtype=dtype,\n            save_args=save_args,\n            **kwargs,\n        )\n\n    if merge:\n        output = os.path.join(out_dir, \"merged.tif\")\n        merge_rasters(out_dir, output)\n        if verbose:\n            print(f\"Saved the merged prediction to {output}.\")\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_dino","title":"<code>predict_dino(image, text_prompt, box_threshold, text_threshold)</code>","text":"<p>Run the GroundingDINO model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing boxes, logits, and phrases.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n    \"\"\"\n    Run the GroundingDINO model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n\n    Returns:\n        tuple: Tuple containing boxes, logits, and phrases.\n    \"\"\"\n\n    image_trans = transform_image(image)\n    boxes, logits, phrases = predict(\n        model=self.groundingdino,\n        image=image_trans,\n        caption=text_prompt,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n        device=self.device,\n    )\n    W, H = image.size\n    boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n    return boxes, logits, phrases\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_sam","title":"<code>predict_sam(image, boxes)</code>","text":"<p>Run the SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>boxes</code> <code>Tensor</code> <p>Tensor of bounding boxes.</p> required <p>Returns:</p> Type Description <p>Masks tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_sam(self, image, boxes):\n    \"\"\"\n    Run the SAM model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        boxes (torch.Tensor): Tensor of bounding boxes.\n\n    Returns:\n        Masks tensor.\n    \"\"\"\n    if self._sam_version == 1:\n        image_array = np.asarray(image)\n        self.sam.set_image(image_array)\n        transformed_boxes = self.sam.transform.apply_boxes_torch(\n            boxes, image_array.shape[:2]\n        )\n        masks, _, _ = self.sam.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes.to(self.sam.device),\n            multimask_output=False,\n        )\n        return masks.cpu()\n    elif self._sam_version == 2:\n        if isinstance(self.source, str):\n            self.sam.set_image(self.source)\n        # If no source is set provide PIL image\n        if self.source is None:\n            self.sam.set_image(image)\n        self.sam.boxes = boxes.numpy().tolist()\n        masks, _, _ = self.sam.predict(\n            boxes=boxes.numpy().tolist(),\n            multimask_output=False,\n            return_results=True,\n        )\n        self.masks = masks\n        return masks\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to use for properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def region_groups(\n    self,\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[\n    Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to use for properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    return self.sam.region_groups(\n        image,\n        connectivity=connectivity,\n        min_size=min_size,\n        max_size=max_size,\n        threshold=threshold,\n        properties=properties,\n        intensity_image=intensity_image,\n        out_csv=out_csv,\n        out_vector=out_vector,\n        out_image=out_image,\n        **kwargs,\n    )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.save_boxes","title":"<code>save_boxes(output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the bounding boxes to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output vector file.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional arguments for boxes_to_vector().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Save the bounding boxes to a vector file.\n\n    Args:\n        output (str): The path to the output vector file.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional arguments for boxes_to_vector().\n    \"\"\"\n\n    if self.boxes is None:\n        print(\"Please run predict() first.\")\n        return\n    else:\n        boxes = self.boxes.tolist()\n        coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n        if output is None:\n            return boxes_to_vector(coords, self.crs, dst_crs, output)\n        else:\n            boxes_to_vector(coords, self.crs, dst_crs, output)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.set_image","title":"<code>set_image(image)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required Source code in <code>samgeo/text_sam.py</code> <pre><code>def set_image(self, image):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', cmap='viridis', alpha=0.4, add_boxes=True, box_color='r', box_linewidth=1, title=None, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>cmap</code> <code>str</code> <p>The colormap for the annotations. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.4.</p> <code>0.4</code> <code>add_boxes</code> <code>bool</code> <p>Whether to show the bounding boxes. Defaults to True.</p> <code>True</code> <code>box_color</code> <code>str</code> <p>The color for the bounding boxes. Defaults to \"r\".</p> <code>'r'</code> <code>box_linewidth</code> <code>int</code> <p>The line width for the bounding boxes. Defaults to 1.</p> <code>1</code> <code>title</code> <code>str</code> <p>The title for the image. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional arguments for matplotlib.pyplot.savefig().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    cmap=\"viridis\",\n    alpha=0.4,\n    add_boxes=True,\n    box_color=\"r\",\n    box_linewidth=1,\n    title=None,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n        add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n        box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n        box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n        title (str, optional): The title for the image. Defaults to None.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n        kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n    \"\"\"\n\n    import warnings\n\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n\n    warnings.filterwarnings(\"ignore\")\n\n    anns = self.prediction\n\n    if anns is None:\n        print(\"Please run predict() first.\")\n        return\n    elif len(anns) == 0:\n        print(\"No objects found in the image.\")\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    if add_boxes:\n        for box in self.boxes:\n            # Draw bounding box\n            box = box.cpu().numpy()  # Convert the tensor to a numpy array\n            rect = patches.Rectangle(\n                (box[0], box[1]),\n                box[2] - box[0],\n                box[3] - box[1],\n                linewidth=box_linewidth,\n                edgecolor=box_color,\n                facecolor=\"none\",\n            )\n            plt.gca().add_patch(rect)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n    if title is not None:\n        plt.title(title)\n    plt.axis(axis)\n\n    if output is not None:\n        if blend:\n            plt.savefig(output, **kwargs)\n        else:\n            array_to_image(self.prediction, output, self.source)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_map","title":"<code>show_map(basemap='SATELLITE', out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.load_model_hf","title":"<code>load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu')</code>","text":"<p>Loads a model from HuggingFace Model Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Repository ID on HuggingFace Model Hub.</p> required <code>filename</code> <code>str</code> <p>Name of the model file in the repository.</p> required <code>ckpt_config_filename</code> <code>str</code> <p>Name of the config file for the model in the repository.</p> required <code>device</code> <code>str</code> <p>Device to load the model onto. Default is 'cpu'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The loaded model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def load_model_hf(\n    repo_id: str, filename: str, ckpt_config_filename: str, device: str = \"cpu\"\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Loads a model from HuggingFace Model Hub.\n\n    Args:\n        repo_id (str): Repository ID on HuggingFace Model Hub.\n        filename (str): Name of the model file in the repository.\n        ckpt_config_filename (str): Name of the config file for the model in the repository.\n        device (str): Device to load the model onto. Default is 'cpu'.\n\n    Returns:\n        torch.nn.Module: The loaded model.\n    \"\"\"\n\n    cache_config_file = hf_hub_download(\n        repo_id=repo_id,\n        filename=ckpt_config_filename,\n        force_filename=ckpt_config_filename,\n    )\n    args = SLConfig.fromfile(cache_config_file)\n    model = build_model(args)\n    model.to(device)\n    cache_file = hf_hub_download(\n        repo_id=repo_id, filename=filename, force_filename=filename\n    )\n    checkpoint = torch.load(cache_file, map_location=\"cpu\")\n    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n    model.eval()\n    return model\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.transform_image","title":"<code>transform_image(image)</code>","text":"<p>Transforms an image using standard transformations for image-based models.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL Image to be transformed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The transformed image as a tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def transform_image(image: Image) -&gt; torch.Tensor:\n    \"\"\"\n    Transforms an image using standard transformations for image-based models.\n\n    Args:\n        image (Image): The PIL Image to be transformed.\n\n    Returns:\n        torch.Tensor: The transformed image as a tensor.\n    \"\"\"\n    transform = T.Compose(\n        [\n            T.RandomResize([800], max_size=1333),\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n    image_transformed, _ = transform(image, None)\n    return image_transformed\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use segment-geospatial in a project:</p> <pre><code>import samgeo\n</code></pre> <p>Here is a simple example of using segment-geospatial to generate a segmentation mask from a satellite image:</p> <pre><code>import os\nimport torch\nfrom samgeo import SamGeo, tms_to_geotiff\n\nbbox = [-95.3704, 29.6762, -95.368, 29.6775]\nimage = 'satellite.tif'\ntms_to_geotiff(output=image, bbox=bbox, zoom=20, source='Satellite')\n\nout_dir = os.path.join(os.path.expanduser('~'), 'Downloads')\ncheckpoint = os.path.join(out_dir, 'sam_vit_h_4b8939.pth')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nsam = SamGeo(\n    checkpoint=checkpoint,\n    model_type='vit_h',\n    device=device,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    sam_kwargs=None,\n)\n\nmask = 'segment.tif'\nsam.generate(image, mask)\n\nvector = 'segment.gpkg'\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</code></pre> <p></p>"},{"location":"assets/","title":"Credits","text":"<p>Credits to Khalil Misbah for the original design of the leafmap logo. The samgeo logo is a derivative of the leafmap logo.</p> <p></p>"},{"location":"examples/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install dependencies.</p> <p><code>conda create esri::python esri::arcpy conda-forge::segment-geospatial --name geo</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap geo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import overlay_images, tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.hq_sam import SamGeo\nfrom samgeo.common import overlay_images, tms_to_geotiff\n</pre> import leafmap from samgeo.hq_sam import SamGeo from samgeo.common import overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from box prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.hq_sam import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo.hq_sam import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import raster_to_vector, overlay_images\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/sam2_automatic/","title":"Sam2 automatic","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.6768, -95.3692], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.6768, -95.3692], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True)\n</pre> sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image)\n</pre> sam2.generate(image) In\u00a0[\u00a0]: Copied! <pre>sam2.save_masks(output=\"masks.tif\")\n</pre> sam2.save_masks(output=\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"binary_r\")\n</pre> sam2.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7)\nm\n</pre> m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7) m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.gpkg\", layer_name=\"Objects\")\n</pre> m.add_vector(\"masks.gpkg\", layer_name=\"Objects\") In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    apply_postprocessing=False,\n    points_per_side=32,\n    points_per_batch=64,\n    pred_iou_thresh=0.7,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.7,\n    crop_n_layers=1,\n    box_nms_thresh=0.7,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=25.0,\n    use_m2m=True,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     apply_postprocessing=False,     points_per_side=32,     points_per_batch=64,     pred_iou_thresh=0.7,     stability_score_thresh=0.92,     stability_score_offset=0.7,     crop_n_layers=1,     box_nms_thresh=0.7,     crop_n_points_downscale_factor=2,     min_mask_region_area=25.0,     use_m2m=True, ) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image, output=\"masks2.tif\")\n</pre> sam2.generate(image, output=\"masks2.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations2.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations2.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", )"},{"location":"examples/sam2_automatic/#automatic-mask-generation-with-sam-2","title":"Automatic Mask Generation with SAM 2\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model 2 (SAM2) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_automatic/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_automatic/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_automatic/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/sam2_automatic/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/sam2_automatic/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/sam2_automatic/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/sam2_box_prompts/","title":"Sam2 box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\nfrom samgeo.common import raster_to_vector, regularize\n</pre> import leafmap from samgeo import SamGeo2 from samgeo.common import raster_to_vector, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-117.5995, 47.6518, -117.5988, 47.652],\n        [-117.5987, 47.6518, -117.5979, 47.652],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-117.5995, 47.6518, -117.5988, 47.652],         [-117.5987, 47.6518, -117.5979, 47.652],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\"\ngeojson = \"building_bboxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\" geojson = \"building_bboxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict(     boxes=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_vector = \"building_vector.geojson\"\nraster_to_vector(output_masks, output_vector)\n</pre> output_vector = \"building_vector.geojson\" raster_to_vector(output_masks, output_vector) In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(output_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(output_vector, output_regularized) In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\n</pre> m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) <p></p>"},{"location":"examples/sam2_box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from box prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/sam2_box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/sam2_box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/sam2_box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/sam2_box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/sam2_box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/sam2_box_prompts/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/sam2_box_prompts/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/sam2_point_prompts/","title":"Sam2 point prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\nfrom samgeo.common import regularize\n</pre> import leafmap from samgeo import SamGeo2 from samgeo.common import regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    point_coords_batch = m.user_rois\nelse:\n    point_coords_batch = [\n        [-117.599896, 47.655345],\n        [-117.59992, 47.655167],\n        [-117.599928, 47.654974],\n        [-117.599518, 47.655337],\n    ]\n</pre> if m.user_rois is not None:     point_coords_batch = m.user_rois else:     point_coords_batch = [         [-117.599896, 47.655345],         [-117.59992, 47.655167],         [-117.599928, 47.654974],         [-117.599518, 47.655337],     ] <p>Segment the objects using the point prompts and save the output masks.</p> In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=point_coords_batch,\n    point_crs=\"EPSG:4326\",\n    output=\"mask.tif\",\n    dtype=\"uint8\",\n)\n</pre> sam.predict_by_points(     point_coords_batch=point_coords_batch,     point_crs=\"EPSG:4326\",     output=\"mask.tif\",     dtype=\"uint8\", ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n</pre> geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\" <p>Display the vector dataawr on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.add_circle_markers_from_xy(\n    geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.add_circle_markers_from_xy(     geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8 ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict_by_points(     point_coords_batch=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>out_vector = \"building_vector.geojson\"\nout_image = \"buildings.tif\"\n</pre> out_vector = \"building_vector.geojson\" out_image = \"buildings.tif\" In\u00a0[\u00a0]: Copied! <pre>array, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() <p></p> In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(out_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(out_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\")\nm.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\") m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/sam2_point_prompts/#segmenting-remote-sensing-imagery-with-point-prompts","title":"Segmenting remote sensing imagery with point prompts\u00b6","text":"<p>This notebook shows how to generate object masks from point prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_point_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_point_prompts/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam2_point_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_point_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"examples/sam2_point_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/sam2_point_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict_by_points()</code> method to segment the image with specified point coordinates. You can use the draw tools to add place markers on the map. If no point is added, the default sample points will be used.</p>"},{"location":"examples/sam2_point_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/sam2_point_prompts/#use-an-existing-vector-dataset-as-points-prompts","title":"Use an existing vector dataset as points prompts\u00b6","text":"<p>Alternatively, you can specify a file path or HTTP URL to a vector dataset containing point geometries.</p>"},{"location":"examples/sam2_point_prompts/#segment-image-with-a-vector-dataset","title":"Segment image with a vector dataset\u00b6","text":"<p>Segment the image using the specified file path to the vector dataset.</p>"},{"location":"examples/sam2_point_prompts/#clean-up-the-result","title":"Clean up the result\u00b6","text":"<p>Remove small objects from the segmented masks, fill holes, and compute geometric properties.</p>"},{"location":"examples/sam2_point_prompts/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"examples/sam2_point_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/sam2_predictor/","title":"Sam2 predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15) m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to use the predictor mode rather than the automatic mode.</p> In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.set_image(image)\n</pre> sam2.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam2.show_map()\nm\n</pre> m = sam2.show_map() m <p></p>"},{"location":"examples/sam2_predictor/#generating-object-masks-from-input-prompts-with-sam-2","title":"Generating object masks from input prompts with SAM 2\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_predictor/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_predictor/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/sam2_predictor/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/sam2_predictor/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/sam2_text_prompts/","title":"Sam2 text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM(model_type=\"sam2-hiera-large\")\n</pre> sam = LangSAM(model_type=\"sam2-hiera-large\") In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam.region_groups(\n    image=\"trees.tif\",\n    min_size=100,\n    out_csv=\"objects.csv\",\n    out_image=\"objects.tif\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam.region_groups(     image=\"trees.tif\",     min_size=100,     out_csv=\"objects.csv\",     out_image=\"objects.tif\",     out_vector=\"objects.gpkg\", ) <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/sam2_text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-2-sam-2","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model 2 (SAM 2)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/sam2_text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/sam2_text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/sam2_text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/sam2_text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/sam2_video/","title":"Sam2 video","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url)\n</pre> leafmap.download_file(url) In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>video_path = \"landsat_ts\"\npredictor.set_video(video_path)\n</pre> video_path = \"landsat_ts\" predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[1582, 933], [1287, 905], [1473, 998]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[1582, 933], [1287, 905], [1473, 998]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p>Althernatively, prompts can be provided in lon/lat coordinates. The model will automatically convert the lon/lat coordinates to pixel coordinates when the <code>point_crs</code> parameter is set to the coordinate reference system of the lon/lat coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\npredictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\")\n</pre> prompts = {     1: {         \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } predictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video()\n</pre> predictor.predict_video() In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments(\"segments\")\n</pre> predictor.save_video_segments(\"segments\") <p>To save the results as blended images and MP4 video:</p> In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\n    \"blended\", fps=5, output_video=\"segments_blended.mp4\"\n)\n</pre> predictor.save_video_segments_blended(     \"blended\", fps=5, output_video=\"segments_blended.mp4\" ) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" In\u00a0[\u00a0]: Copied! <pre>video_path = url\npredictor.set_video(video_path)\n</pre> video_path = url predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[335, 203]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n    2: {\n        \"points\": [[420, 201]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[335, 203]],         \"labels\": [1],         \"frame_idx\": 0,     },     2: {         \"points\": [[420, 201]],         \"labels\": [1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video(prompts)\n</pre> predictor.predict_video(prompts) In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25)\n</pre> predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25) <p></p>"},{"location":"examples/sam2_video/#segmenting-objects-from-timeseries-images-with-sam-2","title":"Segmenting objects from timeseries images with SAM 2\u00b6","text":"<p>This notebook shows how to segment objects from timeseries with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_video/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_video/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam2_video/#download-sample-data","title":"Download sample data\u00b6","text":"<p>For now, SamGeo2 supports remote sensing data in the form of RGB images, 8-bit integer. Make sure all images are in the same width and height.</p>"},{"location":"examples/sam2_video/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/sam2_video/#specify-the-input-data","title":"Specify the input data\u00b6","text":"<p>Point to the directory containing the images or the video file.</p>"},{"location":"examples/sam2_video/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":"<p>The prompts can be points and boxes. The points are represented as a list of tuples, where each tuple contains the x and y coordinates of the point. The boxes are represented as a list of tuples, where each tuple contains the x, y, width, and height of the box.</p>"},{"location":"examples/sam2_video/#segment-the-objects","title":"Segment the objects\u00b6","text":""},{"location":"examples/sam2_video/#save-results","title":"Save results\u00b6","text":"<p>To save the results as gray-scale GeoTIFFs with the same georeference as the input images:</p>"},{"location":"examples/sam2_video/#segment-the-objects-from-a-video","title":"Segment the objects from a video\u00b6","text":""},{"location":"examples/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor\nfrom samgeo.common import tms_to_geotiff\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor from samgeo.common import tms_to_geotiff from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/text_swimming_pools/","title":"Text swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/text_swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/text_swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/tree_mapping/","title":"Tree mapping","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>geojson = (\n    \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_boxes.geojson\"\n)\n</pre> geojson = (     \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_boxes.geojson\" ) <p>Display the bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(\n    geojson,\n    style=style,\n    zoom_to_layer=True,\n    layer_name=\"Bounding boxes\",\n    info_mode=None,\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(     geojson,     style=style,     zoom_to_layer=True,     layer_name=\"Bounding boxes\",     info_mode=None, ) m In\u00a0[\u00a0]: Copied! <pre>output_masks = \"mask2.tif\"\nsam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=output_masks, dtype=\"uint8\")\n</pre> output_masks = \"mask2.tif\" sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=output_masks, dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(output_masks, nodata=0, opacity=0.5, layer_name=\"Tree masks\")\n</pre> m.add_raster(output_masks, nodata=0, opacity=0.5, layer_name=\"Tree masks\") In\u00a0[\u00a0]: Copied! <pre>out_image = \"tree_masks.tif\"\nout_vector = \"tree_vector.geojson\"\narray, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> out_image = \"tree_masks.tif\" out_vector = \"tree_vector.geojson\" array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(\n    out_image, colormap=\"tab20\", nodata=0, opacity=0.7, layer_name=\"Tree masks\"\n)\nm.add_vector(out_vector, style=style, zoom_to_layer=True, layer_name=\"Tree vector\")\nm.add_vector(\n    geojson,\n    style={\"color\": \"blue\", \"fillOpacity\": 0},\n    layer_name=\"Bounding boxes\",\n    info_mode=None,\n)\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(     out_image, colormap=\"tab20\", nodata=0, opacity=0.7, layer_name=\"Tree masks\" ) m.add_vector(out_vector, style=style, zoom_to_layer=True, layer_name=\"Tree vector\") m.add_vector(     geojson,     style={\"color\": \"blue\", \"fillOpacity\": 0},     layer_name=\"Bounding boxes\",     info_mode=None, ) m.add_layer_manager() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.split_map(\n    out_image,\n    image,\n    left_label=\"Tree masks\",\n    right_label=\"Aerial imagery\",\n    left_args={\"colormap\": \"tab20\", \"nodata\": 0, \"opacity\": 0.7},\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.split_map(     out_image,     image,     left_label=\"Tree masks\",     right_label=\"Aerial imagery\",     left_args={\"colormap\": \"tab20\", \"nodata\": 0, \"opacity\": 0.7}, ) m <p></p>"},{"location":"examples/tree_mapping/#tree-mapping-with-samgeo-and-segment-anything-model-2-sam-2","title":"Tree Mapping with SAMGeo and Segment Anything Model 2 (SAM 2)\u00b6","text":"<p>This notebook shows how to segment trees from aerial imagery with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/tree_mapping/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/tree_mapping/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/tree_mapping/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/tree_mapping/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/tree_mapping/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/tree_mapping/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/tree_mapping/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/tree_mapping/#use-an-existing-vector-dataset-as-box-prompts","title":"Use an existing vector dataset as box prompts\u00b6","text":"<p>You can also use an existing vector dataset as box prompts. The following example uses an existing dataset of tree bounding boxes from GitHub.</p>"},{"location":"examples/tree_mapping/#segment-trees-with-box-prompts","title":"Segment trees with box prompts\u00b6","text":"<p>Segment trees using the bounding boxes from the vector dataset.</p>"},{"location":"examples/tree_mapping/#post-processing","title":"Post-processing\u00b6","text":"<p>You can use the <code>region_groups()</code> method to clean up the segmentation results, such as removing small regions, and filling holes. In addition, you can compute geometric properties of the regions, such as area, perimeter, eccentricity, and solidity.</p>"},{"location":"examples/tree_mapping/#display-the-cleaned-masks","title":"Display the cleaned masks\u00b6","text":""},{"location":"examples/tree_mapping/#create-a-split-map","title":"Create a split map\u00b6","text":""},{"location":"workshops/AIforGood_2025/","title":"AIforGood 2025","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install segment-geospatial groundingdino-py\n</pre> %pip install segment-geospatial groundingdino-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2, regularize\n</pre> import leafmap from samgeo import SamGeo2, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>Important note: The code is provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.</p> <p>Alternatively, you can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    point_coords_batch = m.user_rois\nelse:\n    point_coords_batch = [\n        [-117.599896, 47.655345],\n        [-117.59992, 47.655167],\n        [-117.599928, 47.654974],\n        [-117.599518, 47.655337],\n    ]\n</pre> if m.user_rois is not None:     point_coords_batch = m.user_rois else:     point_coords_batch = [         [-117.599896, 47.655345],         [-117.59992, 47.655167],         [-117.599928, 47.654974],         [-117.599518, 47.655337],     ] <p>Segment the objects using the point prompts and save the output masks.</p> In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=point_coords_batch,\n    point_crs=\"EPSG:4326\",\n    output=\"mask.tif\",\n    dtype=\"uint8\",\n)\n</pre> sam.predict_by_points(     point_coords_batch=point_coords_batch,     point_crs=\"EPSG:4326\",     output=\"mask.tif\",     dtype=\"uint8\", ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n</pre> geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\" <p>Display the vector dataawr on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.add_circle_markers_from_xy(\n    geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.add_circle_markers_from_xy(     geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8 ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict_by_points(     point_coords_batch=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>out_vector = \"building_vector.geojson\"\nout_image = \"buildings.tif\"\n</pre> out_vector = \"building_vector.geojson\" out_image = \"buildings.tif\" In\u00a0[\u00a0]: Copied! <pre>array, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() <p></p> In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(out_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(out_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\")\nm.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\") m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2, raster_to_vector, regularize\n</pre> import leafmap from samgeo import SamGeo2, raster_to_vector, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-117.5995, 47.6518, -117.5988, 47.652],\n        [-117.5987, 47.6518, -117.5979, 47.652],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-117.5995, 47.6518, -117.5988, 47.652],         [-117.5987, 47.6518, -117.5979, 47.652],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\"\ngeojson = \"building_bboxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\" geojson = \"building_bboxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector dataset on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict(     boxes=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_vector = \"building_vector.geojson\"\nraster_to_vector(output_masks, output_vector)\n</pre> output_vector = \"building_vector.geojson\" raster_to_vector(output_masks, output_vector) In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(output_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(output_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\n</pre> m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM(model_type=\"sam2-hiera-large\")\n</pre> sam = LangSAM(model_type=\"sam2-hiera-large\") In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam.region_groups(\n    image=\"trees.tif\",\n    min_size=100,\n    out_csv=\"objects.csv\",\n    out_image=\"objects.tif\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam.region_groups(     image=\"trees.tif\",     min_size=100,     out_csv=\"objects.csv\",     out_image=\"objects.tif\",     out_vector=\"objects.gpkg\", ) <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\"\nleafmap.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\" leafmap.download_file(url) In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>video_path = \"landsat_ts\"\npredictor.set_video(video_path)\n</pre> video_path = \"landsat_ts\" predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[1582, 933], [1287, 905], [1473, 998]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[1582, 933], [1287, 905], [1473, 998]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> <p>Althernatively, prompts can be provided in lon/lat coordinates. The model will automatically convert the lon/lat coordinates to pixel coordinates when the <code>point_crs</code> parameter is set to the coordinate reference system of the lon/lat coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\npredictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\")\n</pre> prompts = {     1: {         \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } predictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video()\n</pre> predictor.predict_video() In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments(\"segments\")\n</pre> predictor.save_video_segments(\"segments\") <p>To save the results as blended images and MP4 video:</p> In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\n    \"blended\", fps=5, output_video=\"segments_blended.mp4\"\n)\n</pre> predictor.save_video_segments_blended(     \"blended\", fps=5, output_video=\"segments_blended.mp4\" ) <p></p> <p>Preview the video.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Video\n\nVideo(\"segments_blended.mp4\", embed=True, width=600, height=400)\n</pre> from IPython.display import Video  Video(\"segments_blended.mp4\", embed=True, width=600, height=400) In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" In\u00a0[\u00a0]: Copied! <pre>video_path = url\npredictor.set_video(video_path)\n</pre> video_path = url predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[335, 203]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n    2: {\n        \"points\": [[420, 201]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[335, 203]],         \"labels\": [1],         \"frame_idx\": 0,     },     2: {         \"points\": [[420, 201]],         \"labels\": [1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video(prompts)\n</pre> predictor.predict_video(prompts) In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25)\n</pre> predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25) <p></p> <p>Preview the video.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Video\n\nVideo(\"cars_blended.mp4\", embed=True, width=600, height=400)\n</pre> from IPython.display import Video  Video(\"cars_blended.mp4\", embed=True, width=600, height=400)"},{"location":"workshops/AIforGood_2025/#ai-for-good-workshop-2025","title":"AI for Good Workshop 2025\u00b6","text":"<p>Join us for the AI for Good Workshop 2025, part of the UN's AI for Good workshop series! This workshop will take place online on February 5, 2025, from 9:00 AM to 10:30 AM EST. It is free and open to the public. Please register using this link: Mastering Remote Sensing Image Segmentation with AI: A Hands-On Workshop with the Segment Anything Model.</p>"},{"location":"workshops/AIforGood_2025/#overview","title":"Overview\u00b6","text":"<p>Built upon Meta\u2019s Segment Anything Model (SAM), the SAMGeo Python package brings advanced segmentation capabilities to geospatial data. This hands-on workshop is tailored for geospatial enthusiasts, researchers, and professionals eager to unlock the potential of GeoAI in their projects.</p> <p>Participants will explore how to leverage SAMGeo for accurate and efficient image segmentation of satellite and aerial imagery. The workshop includes step-by-step demonstrations and practical exercises covering:</p> <ul> <li>Introduction to SAM and SAMGeo: Learn the architecture and functionality of SAM and its transformative applications in geospatial analysis.</li> <li>Data Preparation: Prepare geospatial datasets with multi-spectral channels for segmentation tasks.</li> <li>Hands-On with SAMGeo: Leverage SAMGeo to segment geospatial features (e.g., buildings, trees, water bodies) using prompts such as point coordinates, bounding boxes, and text.</li> <li>Postprocessing Techniques: Calculate geometric properties of segmented features, filter results, and extract meaningful insights.</li> <li>Data Visualization: Visualize object masks and segmented features in standard geospatial formats for analysis and reporting.</li> </ul> <p>By the end of the workshop, participants will gain practical experience applying SAMGeo to real-world geospatial challenges and leave equipped with new tools to elevate their geospatial data workflows.</p>"},{"location":"workshops/AIforGood_2025/#target-audience","title":"Target audience\u00b6","text":"<p>This workshop is ideal for geospatial data scientists, remote sensing analysts, researchers, and anyone interested in applying AI to geospatial data.</p>"},{"location":"workshops/AIforGood_2025/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>A Google Colab account</li> <li>Basic understanding of Python programming and geospatial data concepts is recommended</li> </ul>"},{"location":"workshops/AIforGood_2025/#recording","title":"Recording\u00b6","text":"<p>The recording of the workshop is available on YouTube: https://www.youtube.com/watch?v=pTlIIr-ZS4s</p>"},{"location":"workshops/AIforGood_2025/#introduction-to-sam-and-samgeo","title":"Introduction to SAM and SAMGeo\u00b6","text":"<p>The Segment Anything Model (SAM), introduced by Meta AI in April 2023, represents a significant advancement in computer vision, particularly in the field of image segmentation. Designed as a promptable segmentation model, SAM is capable of generating accurate segmentation masks based on various prompts, such as points, bounding boxes, or textual inputs. A notable feature of SAM is its zero-shot transfer ability, allowing it to adapt to new image distributions and tasks without additional training. This adaptability is largely attributed to its training on the extensive SA-1B dataset, which comprises over 1 billion segmentation masks across 11 million images.</p> <p>Building upon the foundation laid by SAM, Meta AI released Segment Anything Model 2 (SAM 2) in August 2024. SAM 2 extends the capabilities of its predecessor by introducing real-time, promptable object segmentation in both images and videos. This unified model achieves state-of-the-art performance, enabling fast and precise selection of any object in any visual context. Key enhancements in SAM 2 include improved accuracy and processing speed, advanced prompting techniques, and the ability to handle video segmentation tasks seamlessly.</p> <p>Building on the success of SAM and SAM 2, the SAMGeo Python package extends these capabilities to geospatial data. SAMGeo empowers users to perform advanced image segmentation tasks on satellite and aerial imagery, enabling the extraction of valuable insights from geospatial datasets. By leveraging the power of SAMGeo, geospatial professionals can streamline their workflows, enhance data analysis, and unlock new possibilities in remote sensing applications.</p> <p>For more information on SAM and SAMGeo, please check out the slides from here: https://bit.ly/aiforgood-samgeo.</p>"},{"location":"workshops/AIforGood_2025/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/AIforGood_2025/#install-the-required-packages-locally","title":"Install the required packages locally\u00b6","text":"<p>If you are running this notebook locally, you can install the required packages using the following commands:</p> <pre>conda create -n sam python=3.12\nconda activate sam\nconda install -c conda-forge mamba\nmamba install -c conda-forge segment-geospatial groundingdino-py gdal\n</pre>"},{"location":"workshops/AIforGood_2025/#use-google-colab","title":"Use Google Colab\u00b6","text":"<p>If you are using Google Colab, make sure you use GPU runtime for this notebook. Go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator. Then you can run the following cell to install the required packages.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-point-prompts","title":"Image segmentation with point prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using point prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"workshops/AIforGood_2025/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict_by_points()</code> method to segment the image with specified point coordinates. You can use the draw tools to add place markers on the map. If no point is added, the default sample points will be used.</p>"},{"location":"workshops/AIforGood_2025/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/AIforGood_2025/#use-an-existing-vector-dataset-as-point-prompts","title":"Use an existing vector dataset as point prompts\u00b6","text":"<p>Alternatively, you can specify a file path or HTTP URL to a vector dataset containing point geometries.</p>"},{"location":"workshops/AIforGood_2025/#segment-image-with-a-vector-dataset","title":"Segment image with a vector dataset\u00b6","text":"<p>Segment the image using the specified file path to the vector dataset.</p>"},{"location":"workshops/AIforGood_2025/#clean-up-the-result","title":"Clean up the result\u00b6","text":"<p>Remove small objects from the segmented masks, fill holes, and compute geometric properties.</p>"},{"location":"workshops/AIforGood_2025/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"workshops/AIforGood_2025/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Place markers on the map to segment the objects interactively.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-box-prompts","title":"Image segmentation with box prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using box prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"workshops/AIforGood_2025/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"workshops/AIforGood_2025/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/AIforGood_2025/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/AIforGood_2025/#use-an-existing-vector-dataset-as-box-prompts","title":"Use an existing vector dataset as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector dataset. Let's download a sample vector dataset from GitHub.</p>"},{"location":"workshops/AIforGood_2025/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"workshops/AIforGood_2025/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":"<p>Convert the segmented masks to a vector format.</p>"},{"location":"workshops/AIforGood_2025/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-text-prompts","title":"Image segmentation with text prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using text prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map.</p>"},{"location":"workshops/AIforGood_2025/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/AIforGood_2025/#specify-text-prompts","title":"Specify text prompts\u00b6","text":"<p>Specify the text prompt to segment the objects in the image. The text prompt can be a single word or a phrase that describes the object you want to segment.</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/AIforGood_2025/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/AIforGood_2025/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"workshops/AIforGood_2025/#timeseries-images-segmentation","title":"Timeseries images segmentation\u00b6","text":""},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>For now, SamGeo2 supports remote sensing data in the form of RGB images, 8-bit integer. Make sure all images are in the same width and height. Let's download a sample timeseries dataset from GitHub.</p>"},{"location":"workshops/AIforGood_2025/#initialize-the-model","title":"Initialize the model\u00b6","text":"<p>Initialize the SamGeo2 class with the model ID and set the <code>video</code> parameter to <code>True</code>.</p>"},{"location":"workshops/AIforGood_2025/#specify-the-input-data","title":"Specify the input data\u00b6","text":"<p>Point to the directory containing the images or the video file.</p>"},{"location":"workshops/AIforGood_2025/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":"<p>The prompts can be points and boxes. The points are represented as a list of tuples, where each tuple contains the x and y coordinates of the point. The boxes are represented as a list of tuples, where each tuple contains the x, y, width, and height of the box.</p>"},{"location":"workshops/AIforGood_2025/#segment-objects","title":"Segment objects\u00b6","text":"<p>Segment the objects from the video or timeseries images.</p>"},{"location":"workshops/AIforGood_2025/#save-results","title":"Save results\u00b6","text":"<p>To save the results as gray-scale GeoTIFFs with the same georeference as the input images:</p>"},{"location":"workshops/AIforGood_2025/#video-segmentation","title":"Video segmentation\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from a video using the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#specify-the-input-data","title":"Specify the input data\u00b6","text":""},{"location":"workshops/AIforGood_2025/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":""},{"location":"workshops/AIforGood_2025/#segment-objects","title":"Segment objects\u00b6","text":""},{"location":"workshops/AIforGood_2025/#save-results","title":"Save results\u00b6","text":""},{"location":"workshops/IPPN_2024/","title":"IPPN 2024","text":"<p>Open Source Pipeline for UAS and satellite based High Throughput Phenotyping Applications - Part 1</p> <p>This notebook is designed for workshop presented at the International Plant Phenotyping Network (IPPN) conference on October 7, 2024. Click the Open in Colab button above to run this notebook interactively in the cloud. For Part 2 of the workshop, please click here.</p> <ul> <li>Registration: https://www.plant-phenotyping.org/index.php?index=935</li> <li>Notebook: https://samgeo.gishub.org/workshops/IPPN_2024</li> <li>Earth Engine: https://earthengine.google.com</li> <li>Geemap: https://geemap.org</li> <li>Leafmap: https://leafmap.org</li> <li>Samgeo: https://samgeo.gishub.org</li> <li>Data to Science (D2S): https://ps2.d2s.org</li> <li>D2S Python API: https://py.d2s.org</li> </ul> In\u00a0[\u00a0]: Copied! <pre># %pip install -U \"leafmap[raster]\" segment-geospatial d2spy\n</pre> # %pip install -U \"leafmap[raster]\" segment-geospatial d2spy In\u00a0[\u00a0]: Copied! <pre># %pip install numpy==1.26.4\n</pre> # %pip install numpy==1.26.4 In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n</pre> m = leafmap.Map() <p>To display it in a Jupyter notebook, simply ask for the object representation:</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m <p>To customize the map, you can specify various keyword arguments, such as <code>center</code> ([lat, lon]), <code>zoom</code>, <code>width</code>, and <code>height</code>. The default <code>width</code> is <code>100%</code>, which takes up the entire cell width of the Jupyter notebook. The <code>height</code> argument accepts a number or a string. If a number is provided, it represents the height of the map in pixels. If a string is provided, the string must be in the format of a number followed by <code>px</code>, e.g., <code>600px</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\")\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\") m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(basemap=\"Esri.WorldImagery\")\nm\n</pre> m = leafmap.Map(basemap=\"Esri.WorldImagery\") m <p>You can add as many basemaps as you like to the map. For example, the following code adds the <code>OpenTopoMap</code> basemap to the map above:</p> In\u00a0[\u00a0]: Copied! <pre>m.add_basemap(\"OpenTopoMap\")\n</pre> m.add_basemap(\"OpenTopoMap\") <p>You can also add an XYZ tile layer to the map.</p> In\u00a0[\u00a0]: Copied! <pre>basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\"\nm.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\")\n</pre> basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\" m.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\") <p>You can also change basemaps interactively using the basemap GUI.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap_gui()\nm\n</pre> m = leafmap.Map() m.add_basemap_gui() m In\u00a0[\u00a0]: Copied! <pre>from d2spy.workspace import Workspace\n\n# Replace with URL to a D2S instance\nd2s_url = \"https://ps2.d2s.org\"\n\n# Login and connect to workspace with your email address\nworkspace = Workspace.connect(d2s_url, \"workshop@d2s.org\")\n</pre> from d2spy.workspace import Workspace  # Replace with URL to a D2S instance d2s_url = \"https://ps2.d2s.org\"  # Login and connect to workspace with your email address workspace = Workspace.connect(d2s_url, \"workshop@d2s.org\") In\u00a0[\u00a0]: Copied! <pre># Check for API key\napi_key = workspace.api_key\nif not api_key:\n    print(\n        \"No API key. Please request one from the D2S profile page and re-run this cell.\"\n    )\n</pre> # Check for API key api_key = workspace.api_key if not api_key:     print(         \"No API key. Please request one from the D2S profile page and re-run this cell.\"     ) In\u00a0[\u00a0]: Copied! <pre>import os\nfrom datetime import date\n\nos.environ[\"D2S_API_KEY\"] = api_key\nos.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\"\n</pre> import os from datetime import date  os.environ[\"D2S_API_KEY\"] = api_key os.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\" In\u00a0[\u00a0]: Copied! <pre># Get list of all your projects\nprojects = workspace.get_projects()\nfor project in projects:\n    print(project)\n</pre> # Get list of all your projects projects = workspace.get_projects() for project in projects:     print(project) <p>The <code>projects</code> variable is a <code>ProjectCollection</code>. The collection can be filtered by either the project descriptions or titles using the methods <code>filter_by_title</code> or <code>filter_by_name</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title\nfiltered_projects = projects.filter_by_title(\"Citrus Orchard\")\nprint(filtered_projects)\n</pre> # Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title filtered_projects = projects.filter_by_title(\"Citrus Orchard\") print(filtered_projects) <p>Now you can choose a specific project to work with. In this case, the filtered projects returned only one project, so we will use that project.</p> In\u00a0[\u00a0]: Copied! <pre>project = filtered_projects[0]\n</pre> project = filtered_projects[0] <p><code>get_project_boundary</code> method of the <code>Project</code> class will retrieve a GeoJSON object of the project boundary.</p> In\u00a0[\u00a0]: Copied! <pre># Get project boundary as Python dictionary in GeoJSON structure\nproject_boundary = project.get_project_boundary()\nproject_boundary\n</pre> # Get project boundary as Python dictionary in GeoJSON structure project_boundary = project.get_project_boundary() project_boundary In\u00a0[\u00a0]: Copied! <pre># Get list of all flights for a project\nflights = project.get_flights()\n# Print first flight object (if one exists)\nfor flight in flights:\n    print(flight)\n</pre> # Get list of all flights for a project flights = project.get_flights() # Print first flight object (if one exists) for flight in flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only flights from June 2022\nfiltered_flights = flights.filter_by_date(\n    start_date=date(2022, 6, 1), end_date=date(2022, 7, 1)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> # Example of creating new collection of only flights from June 2022 filtered_flights = flights.filter_by_date(     start_date=date(2022, 6, 1), end_date=date(2022, 7, 1) ) for flight in filtered_flights:     print(flight) <p>Now, we can choose a flight from the filtered flight. Let's choose the flight on June 9, 2022.</p> In\u00a0[\u00a0]: Copied! <pre>flight = filtered_flights[0]\nflight\n</pre> flight = filtered_flights[0] flight In\u00a0[\u00a0]: Copied! <pre># Get list of data products from a flight\ndata_products = flight.get_data_products()\n\nfor data_product in data_products:\n    print(data_product)\n</pre> # Get list of data products from a flight data_products = flight.get_data_products()  for data_product in data_products:     print(data_product) <p>The <code>data_products</code> variable is a <code>DataProductCollection</code>. The collection can be filtered by data type using the method <code>filter_by_data_type</code>. This method will return all data products that match the requested data type.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"ortho\" data type\northo_data_products = data_products.filter_by_data_type(\"ortho\")\nprint(ortho_data_products)\n</pre> # Example of creating new collection of data products with the \"ortho\" data type ortho_data_products = data_products.filter_by_data_type(\"ortho\") print(ortho_data_products) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"HYBRID\", show=False)\northo_data = ortho_data_products[0]\northo_url_202206 = ortho_data.url\northo_url_202206 = leafmap.d2s_tile(ortho_url_202206)\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_basemap(\"HYBRID\", show=False) ortho_data = ortho_data_products[0] ortho_url_202206 = ortho_data.url ortho_url_202206 = leafmap.d2s_tile(ortho_url_202206) m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"dsm\" data type\ndsm_data_products = data_products.filter_by_data_type(\"dsm\")\nprint(dsm_data_products)\n</pre> # Example of creating new collection of data products with the \"dsm\" data type dsm_data_products = data_products.filter_by_data_type(\"dsm\") print(dsm_data_products) In\u00a0[\u00a0]: Copied! <pre>dsm_data = dsm_data_products[0]\ndsm_url_202206 = dsm_data.url\ndsm_url_202206 = leafmap.d2s_tile(dsm_url_202206)\nm.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\")\n</pre> dsm_data = dsm_data_products[0] dsm_url_202206 = dsm_data.url dsm_url_202206 = leafmap.d2s_tile(dsm_url_202206) m.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(dsm_url_202206)\n</pre> leafmap.cog_stats(dsm_url_202206) <p>Add a colorbar to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"chm\" data type\nchm_data_products = data_products.filter_by_data_type(\"chm\")\nprint(chm_data_products)\n</pre> # Example of creating new collection of data products with the \"chm\" data type chm_data_products = data_products.filter_by_data_type(\"chm\") print(chm_data_products) In\u00a0[\u00a0]: Copied! <pre>chm_data = chm_data_products[0]\nchm_url_202206 = chm_data.url\nchm_url_202206 = leafmap.d2s_tile(chm_url_202206)\nm.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\")\n</pre> chm_data = chm_data_products[0] chm_url_202206 = chm_data.url chm_url_202206 = leafmap.d2s_tile(chm_url_202206) m.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(chm_url_202206)\n</pre> leafmap.cog_stats(chm_url_202206) In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\") m <p>Add the project boundary to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(project_boundary, layer_name=\"Project Boundary\")\n</pre> m.add_geojson(project_boundary, layer_name=\"Project Boundary\") <p>Add tree boundaries to the map.</p> In\u00a0[\u00a0]: Copied! <pre>map_layers = project.get_map_layers()\ntree_boundaries = map_layers[0]\n</pre> map_layers = project.get_map_layers() tree_boundaries = map_layers[0] In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\n</pre> m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") In\u00a0[\u00a0]: Copied! <pre>filtered_flights = flights.filter_by_date(\n    start_date=date(2022, 12, 1), end_date=date(2022, 12, 31)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> filtered_flights = flights.filter_by_date(     start_date=date(2022, 12, 1), end_date=date(2022, 12, 31) ) for flight in filtered_flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre>flight_202212 = filtered_flights[0]\ndata_products = flight_202212.get_data_products()\northo_data_products = data_products.filter_by_data_type(\"ortho\")\northo_data = ortho_data_products[0]\northo_url_202212 = ortho_data.url\northo_url_202212 = leafmap.d2s_tile(ortho_url_202212)\n</pre> flight_202212 = filtered_flights[0] data_products = flight_202212.get_data_products() ortho_data_products = data_products.filter_by_data_type(\"ortho\") ortho_data = ortho_data_products[0] ortho_url_202212 = ortho_data.url ortho_url_202212 = leafmap.d2s_tile(ortho_url_202212) In\u00a0[\u00a0]: Copied! <pre>from ipyleaflet import TileLayer\n\nm = leafmap.Map()\nleft_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\"\n)\nright_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\"\n)\nm.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\")\nm.set_center(-97.955281, 26.165595, 18)\nm\n</pre> from ipyleaflet import TileLayer  m = leafmap.Map() left_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\" ) right_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\" ) m.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\") m.set_center(-97.955281, 26.165595, 18) m In\u00a0[\u00a0]: Copied! <pre>import rioxarray as rxr\n</pre> import rioxarray as rxr In\u00a0[\u00a0]: Copied! <pre>data = rxr.open_rasterio(ortho_url_202206)\ndata\n</pre> data = rxr.open_rasterio(ortho_url_202206) data In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m <p>Draw an area of interest (AOI) on the map. If an AOI is not provided, a default AOI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-97.956252, 26.165315, -97.954992, 26.165883] In\u00a0[\u00a0]: Copied! <pre>geojson = leafmap.bbox_to_geojson(bbox)\ngdf = leafmap.geojson_to_gdf(geojson)\nm.add_gdf(gdf, layer_name=\"AOI\", info_mode=None)\n</pre> geojson = leafmap.bbox_to_geojson(bbox) gdf = leafmap.geojson_to_gdf(geojson) m.add_gdf(gdf, layer_name=\"AOI\", info_mode=None) In\u00a0[\u00a0]: Copied! <pre>crs = data.rio.crs.to_string()\nprint(crs)\n</pre> crs = data.rio.crs.to_string() print(crs) In\u00a0[\u00a0]: Copied! <pre>gdf = gdf.to_crs(crs)\nprint(gdf.crs)\n</pre> gdf = gdf.to_crs(crs) print(gdf.crs) <p>Resample the ortho imagery from 1 cm to 10 cm resolution.</p> In\u00a0[\u00a0]: Copied! <pre>resampled_data = data.rio.reproject(crs, resolution=(0.1, 0.1))\nresampled_data.shape\n</pre> resampled_data = data.rio.reproject(crs, resolution=(0.1, 0.1)) resampled_data.shape <p>Clip the ortho image to the AOI.</p> In\u00a0[\u00a0]: Copied! <pre>clipped_data = resampled_data.rio.clip(gdf.geometry, gdf.crs)\nclipped_data.shape\n</pre> clipped_data = resampled_data.rio.clip(gdf.geometry, gdf.crs) clipped_data.shape <p>Save the clipped ortho image to a GeoTIFF file.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"ortho_image_202206.tif\"\nclipped_data.sel(band=[1, 2, 3]).rio.to_raster(image)\n</pre> image = \"ortho_image_202206.tif\" clipped_data.sel(band=[1, 2, 3]).rio.to_raster(image) <p>Read the CHM dataset from D2S as a DataArray.</p> In\u00a0[\u00a0]: Copied! <pre>chm_data = rxr.open_rasterio(chm_url_202206)\nchm_data\n</pre> chm_data = rxr.open_rasterio(chm_url_202206) chm_data In\u00a0[\u00a0]: Copied! <pre>resampled_chm_data = chm_data.rio.reproject_match(resampled_data)\nresampled_chm_data.shape\n</pre> resampled_chm_data = chm_data.rio.reproject_match(resampled_data) resampled_chm_data.shape In\u00a0[\u00a0]: Copied! <pre>clipped_chm_data = resampled_chm_data.rio.clip(gdf.geometry, gdf.crs)\nclipped_chm_data.shape\n</pre> clipped_chm_data = resampled_chm_data.rio.clip(gdf.geometry, gdf.crs) clipped_chm_data.shape In\u00a0[\u00a0]: Copied! <pre>chm_image = \"chm_202206.tif\"\nclipped_chm_data.sel(band=[1]).rio.to_raster(chm_image)\n</pre> chm_image = \"chm_202206.tif\" clipped_chm_data.sel(band=[1]).rio.to_raster(chm_image) <p>Visualize the clipped ortho image.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Ortho Image 202206\")\nm.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Ortho Image 202206\") m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo import SamGeo, SamGeo2\n</pre> from samgeo import SamGeo, SamGeo2 In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True)\n</pre> sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image)\n</pre> sam2.generate(image) In\u00a0[\u00a0]: Copied! <pre>sam2.save_masks(output=\"masks.tif\")\n</pre> sam2.save_masks(output=\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"binary_r\")\n</pre> sam2.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Drone Imagery\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Drone Imagery\",     label2=\"Image Segmentation\", ) <p>Add segmentation result to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7)\nm\n</pre> m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7) m <p>Convert the object masks to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.gpkg\", layer_name=\"Objects\")\n</pre> m.add_vector(\"masks.gpkg\", layer_name=\"Objects\") In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    apply_postprocessing=False,\n    points_per_side=64,\n    points_per_batch=128,\n    pred_iou_thresh=0.7,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.7,\n    crop_n_layers=1,\n    box_nms_thresh=0.7,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=25,\n    use_m2m=True,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     apply_postprocessing=False,     points_per_side=64,     points_per_batch=128,     pred_iou_thresh=0.7,     stability_score_thresh=0.92,     stability_score_offset=0.7,     crop_n_layers=1,     box_nms_thresh=0.7,     crop_n_points_downscale_factor=2,     min_mask_region_area=25,     use_m2m=True, ) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image, output=\"masks2.tif\")\n</pre> sam2.generate(image, output=\"masks2.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations2.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations2.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Remove small objects.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam2.region_groups(\n    \"masks2.tif\",\n    connectivity=1,\n    min_size=10,\n    max_size=2000,\n    intensity_image=\"chm_202206.tif\",\n    out_image=\"objects.tif\",\n    out_csv=\"objects.csv\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam2.region_groups(     \"masks2.tif\",     connectivity=1,     min_size=10,     max_size=2000,     intensity_image=\"chm_202206.tif\",     out_image=\"objects.tif\",     out_csv=\"objects.csv\",     out_vector=\"objects.gpkg\", ) In\u00a0[\u00a0]: Copied! <pre>gdf = leafmap.geojson_to_gdf(tree_boundaries)\ngdf.head()\n</pre> gdf = leafmap.geojson_to_gdf(tree_boundaries) gdf.head() In\u00a0[\u00a0]: Copied! <pre>geojson = \"tree_boundaries.geojson\"\ngdf.to_file(geojson)\n</pre> geojson = \"tree_boundaries.geojson\" gdf.to_file(geojson) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\"\n)\n</pre> sam.predict(     boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\" ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\"\n)\nm\n</pre> m.add_raster(     \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\" ) m <p></p>"},{"location":"workshops/IPPN_2024/#introduction","title":"Introduction\u00b6","text":"<p>Recent advances in sensor technology have revolutionized the assessment of crop health by providing fine spatial and high temporal resolutions at affordable costs. As plant scientists gain access to increasingly larger volumes of Unmanned Aerial Systems (UAS) and satellite High Throughput Phenotyping (HTP) data, there is a growing need to extract biologically informative and quantitative phenotypic information from the vast amount of freely available geospatial data. However, the lack of specialized software packages tailored for processing such data makes it challenging to develop transdisciplinary research collaboration around these data. This workshop aims to bridge the gap between big data and agricultural research scientists by providing training on an open-source online platform for managing big UAS HTP data known as Data to Science. Additionally, attendees will be introduced to powerful Python packages, namely leafmap and Leafmap, designed for the seamless integration and analysis of UAS and satellite images in various agricultural applications. By participating in this workshop, attendees will acquire the skills necessary to efficiently search, visualize, and analyze geospatial data within a Jupyter environment, even with minimal coding experience. The workshop provides a hands-on learning experience through practical examples and interactive exercises, enabling participants to enhance their proficiency and gain valuable insights into leveraging geospatial data for agricultural research purposes.</p>"},{"location":"workshops/IPPN_2024/#agenda","title":"Agenda\u00b6","text":"<p>The main topics to be covered in this workshop include:</p> <ul> <li>Create interactive maps using leafmap</li> <li>Visualize drone imagery from D2S</li> <li>Segment drone imagery using samgeo</li> <li>Calculate zonal statistics from drone imagery</li> <li>Visualize Earth Engine data</li> <li>Create timelapse animations</li> </ul>"},{"location":"workshops/IPPN_2024/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/IPPN_2024/#change-colab-dark-theme","title":"Change Colab dark theme\u00b6","text":"<p>Currently, ipywidgets does not work well with Colab dark theme. Some of the leafmap widgets may not display properly in Colab dark theme.It is recommended that you change Colab to the light theme.</p> <p></p>"},{"location":"workshops/IPPN_2024/#change-runtime-type-to-gpu","title":"Change runtime type to GPU\u00b6","text":"<p>To speed up the processing, you can change the Colab runtime type to GPU. Go to the \"Runtime\" menu, select \"Change runtime type\", and choose \"T4 GPU\" from the \"Hardware accelerator\" dropdown menu.</p> <p></p>"},{"location":"workshops/IPPN_2024/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following code to install the required packages.</p>"},{"location":"workshops/IPPN_2024/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the necessary libraries for this workshop.</p>"},{"location":"workshops/IPPN_2024/#creating-interactive-maps","title":"Creating interactive maps\u00b6","text":"<p>Let's create an interactive map using the <code>ipyleaflet</code> plotting backend. The <code>leafmap.Map</code> class inherits the <code>ipyleaflet.Map</code> class. Therefore, you can use the same syntax to create an interactive map as you would with <code>ipyleaflet.Map</code>.</p>"},{"location":"workshops/IPPN_2024/#adding-basemaps","title":"Adding basemaps\u00b6","text":"<p>There are several ways to add basemaps to a map. You can specify the basemap to use in the <code>basemap</code> keyword argument when creating the map. Alternatively, you can add basemap layers to the map using the <code>add_basemap</code> method. leafmap has hundreds of built-in basemaps available that can be easily added to the map with only one line of code.</p> <p>Create a map by specifying the basemap to use as follows. For example, the <code>Esri.WorldImagery</code> basemap represents the Esri world imagery basemap.</p>"},{"location":"workshops/IPPN_2024/#visualizing-drone-imagery-from-d2s","title":"Visualizing Drone Imagery from D2S\u00b6","text":"<p>The Data to Science (D2S) platform (https://ps2.d2s.org) hosts a large collection of drone imagery that can be accessed through the D2S API (https://py.d2s.org). To visualize drone imagery from D2S, you need to sign up for a free account on the D2S platform and obtain an API key.</p>"},{"location":"workshops/IPPN_2024/#login-to-d2s","title":"Login to D2S\u00b6","text":"<p>Login and connect to your D2S workspace in one go using the d2spy.</p>"},{"location":"workshops/IPPN_2024/#choose-a-project-to-work-with","title":"Choose a project to work with\u00b6","text":"<p>The Workspace <code>get_projects</code> method will retrieve a collection of the projects your account can currently access on the D2S instance.</p>"},{"location":"workshops/IPPN_2024/#get-the-project-boundary","title":"Get the project boundary\u00b6","text":""},{"location":"workshops/IPPN_2024/#get-project-flights","title":"Get project flights\u00b6","text":"<p>The <code>Project</code> <code>get_flights</code> method will retrieve a list of flights associated with the project.</p>"},{"location":"workshops/IPPN_2024/#filter-flights-by-date","title":"Filter flights by date\u00b6","text":"<p>The <code>flights</code> variable is a <code>FlightCollection</code>. The collection can be filtered by the acquisition date using the method <code>filter_by_date</code>. This method will return all flights with an acquisition date between the provided start and end dates.</p>"},{"location":"workshops/IPPN_2024/#get-data-products","title":"Get data products\u00b6","text":"<p>The Flight <code>get_data_products</code> method will retrieve a list of data products associated with the flight.</p>"},{"location":"workshops/IPPN_2024/#visualize-ortho-imagery","title":"Visualize ortho imagery\u00b6","text":"<p>Now we can grab the ortho URL to display it using leafmap.</p>"},{"location":"workshops/IPPN_2024/#visualize-dsm","title":"Visualize DSM\u00b6","text":"<p>Similarly, you can visualize the Digital Surface Model (DSM) from D2S using the code below.</p>"},{"location":"workshops/IPPN_2024/#visualize-chm","title":"Visualize CHM\u00b6","text":"<p>Similarly, you can visualize the Canopy Height Model (CHM) from D2S using the code below.</p>"},{"location":"workshops/IPPN_2024/#get-another-flight","title":"Get another flight\u00b6","text":"<p>Retrieve the Ortho data product for the December 2022 flight.</p>"},{"location":"workshops/IPPN_2024/#compare-two-ortho-images","title":"Compare two ortho images\u00b6","text":"<p>Create a split map for comparing the 2022 and 2024 ortho images.</p>"},{"location":"workshops/IPPN_2024/#download-data-from-d2s","title":"Download data from D2S\u00b6","text":"<p>Read the ortho image from D2S as a DataArray.</p>"},{"location":"workshops/IPPN_2024/#segmenting-drone-imagery-using-samgeo","title":"Segmenting Drone Imagery using Samgeo\u00b6","text":""},{"location":"workshops/IPPN_2024/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/IPPN_2024/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/IPPN_2024/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/IPPN_2024/#using-box-prompts","title":"Using box prompts\u00b6","text":""},{"location":"workshops/cn_workshop/","title":"Cn workshop","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/cn_workshop/#samgeo-workshop","title":"SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the \u7b2c\u4e03\u5c4a\u5730\u7403\u7a7a\u95f4\u5927\u6570\u636e\u4e0e\u4e91\u8ba1\u7b97\u524d\u6cbf\u4f1a\u8bae\u4e0e\u96c6\u4e2d\u5b66\u4e60.</p>"},{"location":"workshops/cn_workshop/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/cn_workshop/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/cn_workshop/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/cn_workshop/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/cn_workshop/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/cn_workshop/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/cn_workshop/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/cn_workshop/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/cn_workshop/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"workshops/purdue/","title":"Purdue","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/purdue/#purdue-samgeo-workshop","title":"Purdue SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the Purdue GIS Day 2023.</p> <ul> <li>Slides: https://bit.ly/purdue-samgeo</li> <li>Notebook: https://samgeo.gishub.org/workshops/purdue</li> </ul>"},{"location":"workshops/purdue/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/purdue/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/purdue/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/purdue/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/purdue/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/purdue/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/purdue/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/purdue/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/purdue/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/purdue/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/purdue/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""}]}
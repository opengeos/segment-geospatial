{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SamGeo","text":"<p>A Python package for segmenting geospatial data with the Segment Anything Model (SAM)</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The SamGeo package draws its inspiration from segment-anything-eo repository authored by Aliaksandr Hancharenka. The primary objective of SamGeo is to simplify the process of leveraging SAM for geospatial data analysis by enabling users to achieve this with minimal coding effort. The source code of SamGeo was adapted from the segment-anything-eo repository, and credit for its original version goes to Aliaksandr Hancharenka.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://samgeo.gishub.org</li> </ul>"},{"location":"#citations","title":"Citations","text":"<ul> <li>Wu, Q., &amp; Osco, L. (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). Journal of Open Source Software, 8(89), 5663. https://doi.org/10.21105/joss.05663</li> <li>Osco, L. P., Wu, Q., de Lemos, E. L., Gon\u00e7alves, W. N., Ramos, A. P. M., Li, J., &amp; Junior, J. M. (2023). The Segment Anything Model (SAM) for remote sensing applications: From zero to one shot. International Journal of Applied Earth Observation and Geoinformation, 124, 103540. https://doi.org/10.1016/j.jag.2023.103540</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Download map tiles from Tile Map Service (TMS) servers and create GeoTIFF files</li> <li>Segment GeoTIFF files using the Segment Anything Model (SAM) and HQ-SAM</li> <li>Segment remote sensing imagery with text prompts</li> <li>Create foreground and background markers interactively</li> <li>Load existing markers from vector datasets</li> <li>Save segmentation results as common vector formats (GeoPackage, Shapefile, GeoJSON)</li> <li>Save input prompts as GeoJSON files</li> <li>Visualize segmentation results on interactive maps</li> <li>Segment objects from timeseries remote sensing imagery</li> </ul>"},{"location":"#qgis-plugin","title":"QGIS Plugin","text":"<p>SamGeo is also available as a QGIS plugin. Check out this short video demo and full video tutorial on how to use the plugin.</p> <p></p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<p>segment-geospatial is available on PyPI and can be installed in several ways so that its dependencies can be controlled more granularly. This reduces package size for CI environments, since not every time all of the models will be used.</p> <p>Depending on what tools you need to use, you might want to do:</p> <ul> <li><code>segment-geospatial</code> or <code>segment-geospatial[samgeo]</code>: Installs only the minimum required dependencies to run SAMGeo</li> <li><code>segment-geospatial[samgeo2]</code>: Installs the dependencies to run SAMGeo 2</li> <li><code>segment-geospatial[samgeo3]</code>: Installs the dependencies to run SAMGeo 3</li> <li><code>segment-geospatial[fast]</code>: Installs the dependencies to run Fast SAM</li> <li><code>segment-geospatial[hq]</code>: Installs the dependencies to run HQ-SAM</li> <li><code>segment-geospatial[text]</code>: Installs Grounding DINO to use SAMGeo 1 and 2 with text prompts</li> <li><code>segment-geospatial[fer]</code>: Installs the dependencies to run the feature     edge reconstruction algorithm</li> </ul> <p>Additionally, these other two optional imports are defined:</p> <ul> <li><code>segment-geospatial[all]</code>: Installs the dependencies to run all of the SAMGeo models</li> <li><code>segment-geospatial[extra]</code>: Installs the dependencies to run all of the SAMGeo models and other utilities to run the examples like Jupyter notebook support, <code>leafmap</code>, etc.</li> </ul> <p>Simply running the following should install the dependencies for each use case:</p> <pre><code>pip install \"segment-geospatial[samgeo3]\" # Or any other choice of the above\n</code></pre> <p>To see more in detail what packages come with each choice, please refer to <code>pyproject.toml</code>.</p>"},{"location":"#install-from-conda-forge","title":"Install from conda-forge","text":"<p>segment-geospatial is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install segment-geospatial using the following commands. It is recommended to create a fresh conda environment for segment-geospatial. The following commands will create a new conda environment named <code>geo</code> and install segment-geospatial and its dependencies:</p> <pre><code>conda create -n geo python\nconda activate geo\nconda install -c conda-forge segment-geospatial\n</code></pre> <p>If your system has a GPU, but the above commands do not install the GPU version of pytorch, you can force the installation of the GPU version of pytorch using the following command:</p> <pre><code>conda install -c conda-forge segment-geospatial \"pytorch=*=cuda*\"\n</code></pre> <p>Samgeo-geospatial has some optional dependencies that are not included in the default conda environment. To install these dependencies, run the following command:</p> <pre><code>conda install -c conda-forge groundingdino-py segment-anything-fast\n</code></pre>"},{"location":"#install-sam-3-on-windows","title":"Install SAM 3 on Windows","text":"<p>It is a bit tricky to install SAM 3 on Windows. Run the following commands on Windows to install SamGeo:</p> <pre><code>conda create -n geo python=3.12\nconda activate geo\nconda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\npip install \"segment-geospatial[samgeo3]\"\npip install triton-windows ipykernel jupyterlab\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Segmenting remote sensing imagery</li> <li>Automatically generating object masks</li> <li>Segmenting remote sensing imagery with input prompts</li> <li>Segmenting remote sensing imagery with box prompts</li> <li>Segmenting remote sensing imagery with text prompts</li> <li>Batch segmentation with text prompts</li> <li>Using segment-geospatial with ArcGIS Pro</li> <li>Segmenting swimming pools with text prompts</li> <li>Segmenting satellite imagery from the Maxar Open Data Program</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Automatic mask generator</li> </ul> <ul> <li>Interactive segmentation with input prompts</li> </ul> <ul> <li>Input prompts from existing files</li> </ul> <ul> <li>Interactive segmentation with text prompts</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Video tutorials are available on my YouTube Channel.</p> <ul> <li>Automatic mask generation</li> </ul> <p></p> <ul> <li>Using SAM with ArcGIS Pro</li> </ul> <p></p> <ul> <li>Interactive segmentation with text prompts</li> </ul> <p></p>"},{"location":"#using-sam-with-desktop-gis","title":"Using SAM with Desktop GIS","text":"<ul> <li>QGIS: Check out the SamGeo QGIS Plugin.</li> <li>ArcGIS: Check out the Segment Anything Model (SAM) Toolbox for ArcGIS and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS.</li> </ul>"},{"location":"#computing-resources","title":"Computing Resources","text":"<p>The Segment Anything Model is computationally intensive, and a powerful GPU is recommended to process large datasets. It is recommended to have a GPU with at least 8 GB of GPU memory. You can utilize the free GPU resources provided by Google Colab. Alternatively, you can apply for AWS Cloud Credit for Research, which offers cloud credits to support academic research. If you are in the Greater China region, apply for the AWS Cloud Credit here.</p>"},{"location":"#legal-notice","title":"Legal Notice","text":"<p>This repository and its content are provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Please refer to the contributing guidelines for more information.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is based upon work partially supported by the National Aeronautics and Space Administration (NASA) under Grant No. 80NSSC22K1742 issued through the Open Source Tools, Frameworks, and Libraries 2020 Program.</p> <p>This project is also supported by Amazon Web Services (AWS). In addition, this package was made possible by the following open source projects. Credit goes to the developers of these projects.</p> <ul> <li>segment-anything</li> <li>SAM 3</li> <li>segment-anything-eo</li> <li>tms2geotiff</li> <li>GroundingDINO</li> <li>lang-segment-anything</li> </ul>"},{"location":"caption/","title":"caption module","text":"<p>Image captioning and feature extraction module using BLIP and spaCy.</p> <p>This module provides functionality to generate captions for images using the BLIP model and extract relevant features from the captions using spaCy NLP.</p>"},{"location":"caption/#samgeo.caption.ImageCaptioner","title":"<code>ImageCaptioner</code>","text":"<p>Image captioning and feature extraction using BLIP and spaCy.</p> <p>This class provides functionality to generate captions for images using the BLIP model and extract relevant features from the captions using spaCy NLP processing.</p> <p>Parameters:</p> Name Type Description Default <code>blip_model_name</code> <code>str</code> <p>Name or path of the BLIP model to use for captioning. Defaults to \"Salesforce/blip-image-captioning-base\".</p> <code>DEFAULT_BLIP_MODEL</code> <code>spacy_model_name</code> <code>str</code> <p>Name of the spaCy model to use for NLP processing. Defaults to \"en_core_web_sm\".</p> <code>DEFAULT_SPACY_MODEL</code> <code>device</code> <code>Optional[str]</code> <p>Device to run the BLIP model on. If None, automatically detects the best available device (CUDA, MPS, or CPU).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>blip_model_name</code> <p>The name of the loaded BLIP model.</p> <code>spacy_model_name</code> <p>The name of the loaded spaCy model.</p> <code>device</code> <p>The device the model is running on.</p> <code>processor</code> <p>The BLIP processor for image preprocessing.</p> <code>blip_model</code> <p>The BLIP model for caption generation.</p> <code>nlp</code> <p>The spaCy NLP pipeline.</p> Example <p>captioner = ImageCaptioner() caption, features = captioner.analyze(\"path/to/image.jpg\") print(caption) \"an aerial view of a parking lot with cars\" print(features) [\"parking_lot\", \"car\"]</p> Source code in <code>samgeo/caption.py</code> <pre><code>class ImageCaptioner:\n    \"\"\"Image captioning and feature extraction using BLIP and spaCy.\n\n    This class provides functionality to generate captions for images using\n    the BLIP model and extract relevant features from the captions using\n    spaCy NLP processing.\n\n    Args:\n        blip_model_name: Name or path of the BLIP model to use for captioning.\n            Defaults to \"Salesforce/blip-image-captioning-base\".\n        spacy_model_name: Name of the spaCy model to use for NLP processing.\n            Defaults to \"en_core_web_sm\".\n        device: Device to run the BLIP model on. If None, automatically\n            detects the best available device (CUDA, MPS, or CPU).\n\n    Attributes:\n        blip_model_name: The name of the loaded BLIP model.\n        spacy_model_name: The name of the loaded spaCy model.\n        device: The device the model is running on.\n        processor: The BLIP processor for image preprocessing.\n        blip_model: The BLIP model for caption generation.\n        nlp: The spaCy NLP pipeline.\n\n    Example:\n        &gt;&gt;&gt; captioner = ImageCaptioner()\n        &gt;&gt;&gt; caption, features = captioner.analyze(\"path/to/image.jpg\")\n        &gt;&gt;&gt; print(caption)\n        \"an aerial view of a parking lot with cars\"\n        &gt;&gt;&gt; print(features)\n        [\"parking_lot\", \"car\"]\n    \"\"\"\n\n    def __init__(\n        self,\n        blip_model_name: str = DEFAULT_BLIP_MODEL,\n        spacy_model_name: str = DEFAULT_SPACY_MODEL,\n        device: Optional[str] = None,\n    ):\n        \"\"\"Initialize the ImageCaptioner with specified models.\n\n        Args:\n            blip_model_name: Name or path of the BLIP model to use.\n                Defaults to \"Salesforce/blip-image-captioning-base\".\n            spacy_model_name: Name of the spaCy model to use.\n                Defaults to \"en_core_web_sm\".\n            device: Device to run the model on ('cuda', 'mps', 'cpu').\n                If None, automatically detects the best available device.\n        \"\"\"\n        self.blip_model_name = blip_model_name\n        self.spacy_model_name = spacy_model_name\n        self.device = device if device else get_device()\n\n        # Load spaCy model\n        ensure_spacy_model(spacy_model_name)\n        self.nlp = spacy.load(spacy_model_name)\n\n        # Load BLIP model\n        self.processor = BlipProcessor.from_pretrained(\n            blip_model_name,\n            use_fast=True,\n        )\n        self.blip_model = BlipForConditionalGeneration.from_pretrained(\n            blip_model_name\n        ).to(self.device)\n        self.blip_model.eval()\n\n    @torch.inference_mode()\n    def generate_caption(self, image_source: Union[str, Image.Image]) -&gt; str:\n        \"\"\"Generate a caption for the given image.\n\n        Args:\n            image_source: The image to caption. Can be a local file path,\n                an HTTP(S) URL, or a PIL Image object.\n\n        Returns:\n            Generated caption string describing the image content.\n\n        Example:\n            &gt;&gt;&gt; captioner = ImageCaptioner()\n            &gt;&gt;&gt; caption = captioner.generate_caption(\"path/to/aerial.jpg\")\n            &gt;&gt;&gt; print(caption)\n            \"an aerial view of a building with a parking lot\"\n        \"\"\"\n        img = load_image(image_source)\n        inputs = self.processor(img, return_tensors=\"pt\").to(self.device)\n        out = self.blip_model.generate(**inputs)\n        caption = self.processor.decode(out[0], skip_special_tokens=True)\n        return caption\n\n    def extract_features(\n        self,\n        caption: str,\n        include_features: Optional[Union[str, List[str]]] = None,\n        exclude_features: Optional[List[str]] = None,\n    ) -&gt; List[str]:\n        \"\"\"Extract features from a caption using NLP processing.\n\n        Uses spaCy to parse the caption and extract relevant noun features\n        based on the provided inclusion/exclusion criteria.\n\n        Args:\n            caption: The caption text to extract features from.\n            include_features: Controls which features to extract:\n                - None: Extract any noun (excluding large-scale terms\n                  and custom excludes).\n                - \"default\" or [\"default\"]: Use the aerial_features.json\n                  vocabulary for matching.\n                - List of strings: Custom allowed features (with or without\n                  underscores).\n            exclude_features: List of noun lemmas to exclude in addition\n                to the built-in large-scale terms.\n\n        Returns:\n            Sorted list of extracted feature names (canonical keys or\n            noun lemmas).\n\n        Example:\n            &gt;&gt;&gt; captioner = ImageCaptioner()\n            &gt;&gt;&gt; features = captioner.extract_features(\n            ...     \"a parking lot with several cars\",\n            ...     include_features=[\"default\"]\n            ... )\n            &gt;&gt;&gt; print(features)\n            [\"car\", \"parking_lot\"]\n        \"\"\"\n        doc = self.nlp(caption)\n        detected = set()\n\n        # ----------------------- exclusions -----------------------\n        active_exclude = set(LARGE_SCALE)\n        if exclude_features:\n            active_exclude.update(\n                ex.lower().replace(\"_\", \" \") for ex in exclude_features\n            )\n\n        # Normalize include_features semantics\n        use_default_vocab = False\n        user_include_list: Optional[List[str]] = None\n\n        if include_features is None:\n            use_default_vocab = False\n        else:\n            # allow include_features=\"default\" or [\"default\"]\n            if isinstance(include_features, str):\n                include_features = [include_features]\n\n            if any(f.lower() == \"default\" for f in include_features):\n                use_default_vocab = True\n            else:\n                user_include_list = include_features\n\n        # Build maps for matching\n        if use_default_vocab:\n            single_map = SINGLE_WORD_FEATURES\n            multi_map = MULTIWORD_FEATURES\n        elif user_include_list:\n            norm = [f.lower().replace(\"_\", \" \") for f in user_include_list]\n            single_map = {\n                phrase: phrase.replace(\" \", \"_\") for phrase in norm if \" \" not in phrase\n            }\n            multi_map = {\n                phrase: phrase.replace(\" \", \"_\") for phrase in norm if \" \" in phrase\n            }\n        else:\n            single_map = None\n            multi_map = None\n\n        # ----------------------- multi-word (noun chunks) -----------------------\n        if use_default_vocab or user_include_list:\n            for chunk in doc.noun_chunks:\n                chunk_text = chunk.text.lower()\n                chunk_lemma = \" \".join(tok.lemma_.lower() for tok in chunk)\n\n                for candidate in {chunk_text, chunk_lemma}:\n                    if multi_map and candidate in multi_map:\n                        detected.add(multi_map[candidate])\n\n        # ----------------------- single-word nouns -----------------------\n        original_include = include_features\n        for token in doc:\n            if token.pos_ != \"NOUN\":\n                continue\n\n            lemma = token.lemma_.lower()\n            if lemma in active_exclude:\n                continue\n\n            # Case: no include list \u2192 accept any noun lemma\n            if original_include is None:\n                detected.add(lemma)\n            else:\n                if single_map and lemma in single_map:\n                    detected.add(single_map[lemma])\n\n        # ----------------------- fallback if using include list -----------------------\n        if (use_default_vocab or user_include_list) and not detected:\n            fallback = {\n                token.lemma_.lower()\n                for token in doc\n                if token.pos_ == \"NOUN\" and token.lemma_.lower() not in active_exclude\n            }\n            return sorted(fallback)\n\n        return sorted(detected)\n\n    @torch.inference_mode()\n    def analyze(\n        self,\n        image_source: Union[str, Image.Image],\n        include_features: Optional[Union[str, List[str]]] = None,\n        exclude_features: Optional[List[str]] = None,\n    ) -&gt; Tuple[str, List[str]]:\n        \"\"\"Analyze an image by generating a caption and extracting features.\n\n        This is the main entry point that combines caption generation and\n        feature extraction into a single call.\n\n        Args:\n            image_source: The image to analyze. Can be a local file path,\n                an HTTP(S) URL, or a PIL Image object.\n            include_features: Controls which features to extract:\n                - None: Extract any noun (excluding large-scale terms\n                  and custom excludes).\n                - \"default\" or [\"default\"]: Use the aerial_features.json\n                  vocabulary for matching.\n                - List of strings: Custom allowed features (with or without\n                  underscores).\n            exclude_features: List of noun lemmas to exclude in addition\n                to the built-in large-scale terms.\n\n        Returns:\n            A tuple containing:\n                - caption: The BLIP-generated caption string.\n                - features: Sorted list of extracted feature names.\n\n        Example:\n            &gt;&gt;&gt; captioner = ImageCaptioner()\n            &gt;&gt;&gt; caption, features = captioner.analyze(\n            ...     \"https://example.com/aerial.jpg\",\n            ...     include_features=[\"default\"],\n            ...     exclude_features=[\"building\"]\n            ... )\n            &gt;&gt;&gt; print(caption)\n            \"an aerial view of a residential area\"\n            &gt;&gt;&gt; print(features)\n            [\"house\", \"road\", \"tree\"]\n        \"\"\"\n        caption = self.generate_caption(image_source)\n        features = self.extract_features(\n            caption,\n            include_features=include_features,\n            exclude_features=exclude_features,\n        )\n        return caption, features\n</code></pre>"},{"location":"caption/#samgeo.caption.ImageCaptioner.__init__","title":"<code>__init__(blip_model_name=DEFAULT_BLIP_MODEL, spacy_model_name=DEFAULT_SPACY_MODEL, device=None)</code>","text":"<p>Initialize the ImageCaptioner with specified models.</p> <p>Parameters:</p> Name Type Description Default <code>blip_model_name</code> <code>str</code> <p>Name or path of the BLIP model to use. Defaults to \"Salesforce/blip-image-captioning-base\".</p> <code>DEFAULT_BLIP_MODEL</code> <code>spacy_model_name</code> <code>str</code> <p>Name of the spaCy model to use. Defaults to \"en_core_web_sm\".</p> <code>DEFAULT_SPACY_MODEL</code> <code>device</code> <code>Optional[str]</code> <p>Device to run the model on ('cuda', 'mps', 'cpu'). If None, automatically detects the best available device.</p> <code>None</code> Source code in <code>samgeo/caption.py</code> <pre><code>def __init__(\n    self,\n    blip_model_name: str = DEFAULT_BLIP_MODEL,\n    spacy_model_name: str = DEFAULT_SPACY_MODEL,\n    device: Optional[str] = None,\n):\n    \"\"\"Initialize the ImageCaptioner with specified models.\n\n    Args:\n        blip_model_name: Name or path of the BLIP model to use.\n            Defaults to \"Salesforce/blip-image-captioning-base\".\n        spacy_model_name: Name of the spaCy model to use.\n            Defaults to \"en_core_web_sm\".\n        device: Device to run the model on ('cuda', 'mps', 'cpu').\n            If None, automatically detects the best available device.\n    \"\"\"\n    self.blip_model_name = blip_model_name\n    self.spacy_model_name = spacy_model_name\n    self.device = device if device else get_device()\n\n    # Load spaCy model\n    ensure_spacy_model(spacy_model_name)\n    self.nlp = spacy.load(spacy_model_name)\n\n    # Load BLIP model\n    self.processor = BlipProcessor.from_pretrained(\n        blip_model_name,\n        use_fast=True,\n    )\n    self.blip_model = BlipForConditionalGeneration.from_pretrained(\n        blip_model_name\n    ).to(self.device)\n    self.blip_model.eval()\n</code></pre>"},{"location":"caption/#samgeo.caption.ImageCaptioner.analyze","title":"<code>analyze(image_source, include_features=None, exclude_features=None)</code>","text":"<p>Analyze an image by generating a caption and extracting features.</p> <p>This is the main entry point that combines caption generation and feature extraction into a single call.</p> <p>Parameters:</p> Name Type Description Default <code>image_source</code> <code>Union[str, Image]</code> <p>The image to analyze. Can be a local file path, an HTTP(S) URL, or a PIL Image object.</p> required <code>include_features</code> <code>Optional[Union[str, List[str]]]</code> <p>Controls which features to extract: - None: Extract any noun (excluding large-scale terms   and custom excludes). - \"default\" or [\"default\"]: Use the aerial_features.json   vocabulary for matching. - List of strings: Custom allowed features (with or without   underscores).</p> <code>None</code> <code>exclude_features</code> <code>Optional[List[str]]</code> <p>List of noun lemmas to exclude in addition to the built-in large-scale terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, List[str]]</code> <p>A tuple containing: - caption: The BLIP-generated caption string. - features: Sorted list of extracted feature names.</p> Example <p>captioner = ImageCaptioner() caption, features = captioner.analyze( ...     \"https://example.com/aerial.jpg\", ...     include_features=[\"default\"], ...     exclude_features=[\"building\"] ... ) print(caption) \"an aerial view of a residential area\" print(features) [\"house\", \"road\", \"tree\"]</p> Source code in <code>samgeo/caption.py</code> <pre><code>@torch.inference_mode()\ndef analyze(\n    self,\n    image_source: Union[str, Image.Image],\n    include_features: Optional[Union[str, List[str]]] = None,\n    exclude_features: Optional[List[str]] = None,\n) -&gt; Tuple[str, List[str]]:\n    \"\"\"Analyze an image by generating a caption and extracting features.\n\n    This is the main entry point that combines caption generation and\n    feature extraction into a single call.\n\n    Args:\n        image_source: The image to analyze. Can be a local file path,\n            an HTTP(S) URL, or a PIL Image object.\n        include_features: Controls which features to extract:\n            - None: Extract any noun (excluding large-scale terms\n              and custom excludes).\n            - \"default\" or [\"default\"]: Use the aerial_features.json\n              vocabulary for matching.\n            - List of strings: Custom allowed features (with or without\n              underscores).\n        exclude_features: List of noun lemmas to exclude in addition\n            to the built-in large-scale terms.\n\n    Returns:\n        A tuple containing:\n            - caption: The BLIP-generated caption string.\n            - features: Sorted list of extracted feature names.\n\n    Example:\n        &gt;&gt;&gt; captioner = ImageCaptioner()\n        &gt;&gt;&gt; caption, features = captioner.analyze(\n        ...     \"https://example.com/aerial.jpg\",\n        ...     include_features=[\"default\"],\n        ...     exclude_features=[\"building\"]\n        ... )\n        &gt;&gt;&gt; print(caption)\n        \"an aerial view of a residential area\"\n        &gt;&gt;&gt; print(features)\n        [\"house\", \"road\", \"tree\"]\n    \"\"\"\n    caption = self.generate_caption(image_source)\n    features = self.extract_features(\n        caption,\n        include_features=include_features,\n        exclude_features=exclude_features,\n    )\n    return caption, features\n</code></pre>"},{"location":"caption/#samgeo.caption.ImageCaptioner.extract_features","title":"<code>extract_features(caption, include_features=None, exclude_features=None)</code>","text":"<p>Extract features from a caption using NLP processing.</p> <p>Uses spaCy to parse the caption and extract relevant noun features based on the provided inclusion/exclusion criteria.</p> <p>Parameters:</p> Name Type Description Default <code>caption</code> <code>str</code> <p>The caption text to extract features from.</p> required <code>include_features</code> <code>Optional[Union[str, List[str]]]</code> <p>Controls which features to extract: - None: Extract any noun (excluding large-scale terms   and custom excludes). - \"default\" or [\"default\"]: Use the aerial_features.json   vocabulary for matching. - List of strings: Custom allowed features (with or without   underscores).</p> <code>None</code> <code>exclude_features</code> <code>Optional[List[str]]</code> <p>List of noun lemmas to exclude in addition to the built-in large-scale terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>Sorted list of extracted feature names (canonical keys or</p> <code>List[str]</code> <p>noun lemmas).</p> Example <p>captioner = ImageCaptioner() features = captioner.extract_features( ...     \"a parking lot with several cars\", ...     include_features=[\"default\"] ... ) print(features) [\"car\", \"parking_lot\"]</p> Source code in <code>samgeo/caption.py</code> <pre><code>def extract_features(\n    self,\n    caption: str,\n    include_features: Optional[Union[str, List[str]]] = None,\n    exclude_features: Optional[List[str]] = None,\n) -&gt; List[str]:\n    \"\"\"Extract features from a caption using NLP processing.\n\n    Uses spaCy to parse the caption and extract relevant noun features\n    based on the provided inclusion/exclusion criteria.\n\n    Args:\n        caption: The caption text to extract features from.\n        include_features: Controls which features to extract:\n            - None: Extract any noun (excluding large-scale terms\n              and custom excludes).\n            - \"default\" or [\"default\"]: Use the aerial_features.json\n              vocabulary for matching.\n            - List of strings: Custom allowed features (with or without\n              underscores).\n        exclude_features: List of noun lemmas to exclude in addition\n            to the built-in large-scale terms.\n\n    Returns:\n        Sorted list of extracted feature names (canonical keys or\n        noun lemmas).\n\n    Example:\n        &gt;&gt;&gt; captioner = ImageCaptioner()\n        &gt;&gt;&gt; features = captioner.extract_features(\n        ...     \"a parking lot with several cars\",\n        ...     include_features=[\"default\"]\n        ... )\n        &gt;&gt;&gt; print(features)\n        [\"car\", \"parking_lot\"]\n    \"\"\"\n    doc = self.nlp(caption)\n    detected = set()\n\n    # ----------------------- exclusions -----------------------\n    active_exclude = set(LARGE_SCALE)\n    if exclude_features:\n        active_exclude.update(\n            ex.lower().replace(\"_\", \" \") for ex in exclude_features\n        )\n\n    # Normalize include_features semantics\n    use_default_vocab = False\n    user_include_list: Optional[List[str]] = None\n\n    if include_features is None:\n        use_default_vocab = False\n    else:\n        # allow include_features=\"default\" or [\"default\"]\n        if isinstance(include_features, str):\n            include_features = [include_features]\n\n        if any(f.lower() == \"default\" for f in include_features):\n            use_default_vocab = True\n        else:\n            user_include_list = include_features\n\n    # Build maps for matching\n    if use_default_vocab:\n        single_map = SINGLE_WORD_FEATURES\n        multi_map = MULTIWORD_FEATURES\n    elif user_include_list:\n        norm = [f.lower().replace(\"_\", \" \") for f in user_include_list]\n        single_map = {\n            phrase: phrase.replace(\" \", \"_\") for phrase in norm if \" \" not in phrase\n        }\n        multi_map = {\n            phrase: phrase.replace(\" \", \"_\") for phrase in norm if \" \" in phrase\n        }\n    else:\n        single_map = None\n        multi_map = None\n\n    # ----------------------- multi-word (noun chunks) -----------------------\n    if use_default_vocab or user_include_list:\n        for chunk in doc.noun_chunks:\n            chunk_text = chunk.text.lower()\n            chunk_lemma = \" \".join(tok.lemma_.lower() for tok in chunk)\n\n            for candidate in {chunk_text, chunk_lemma}:\n                if multi_map and candidate in multi_map:\n                    detected.add(multi_map[candidate])\n\n    # ----------------------- single-word nouns -----------------------\n    original_include = include_features\n    for token in doc:\n        if token.pos_ != \"NOUN\":\n            continue\n\n        lemma = token.lemma_.lower()\n        if lemma in active_exclude:\n            continue\n\n        # Case: no include list \u2192 accept any noun lemma\n        if original_include is None:\n            detected.add(lemma)\n        else:\n            if single_map and lemma in single_map:\n                detected.add(single_map[lemma])\n\n    # ----------------------- fallback if using include list -----------------------\n    if (use_default_vocab or user_include_list) and not detected:\n        fallback = {\n            token.lemma_.lower()\n            for token in doc\n            if token.pos_ == \"NOUN\" and token.lemma_.lower() not in active_exclude\n        }\n        return sorted(fallback)\n\n    return sorted(detected)\n</code></pre>"},{"location":"caption/#samgeo.caption.ImageCaptioner.generate_caption","title":"<code>generate_caption(image_source)</code>","text":"<p>Generate a caption for the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image_source</code> <code>Union[str, Image]</code> <p>The image to caption. Can be a local file path, an HTTP(S) URL, or a PIL Image object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated caption string describing the image content.</p> Example <p>captioner = ImageCaptioner() caption = captioner.generate_caption(\"path/to/aerial.jpg\") print(caption) \"an aerial view of a building with a parking lot\"</p> Source code in <code>samgeo/caption.py</code> <pre><code>@torch.inference_mode()\ndef generate_caption(self, image_source: Union[str, Image.Image]) -&gt; str:\n    \"\"\"Generate a caption for the given image.\n\n    Args:\n        image_source: The image to caption. Can be a local file path,\n            an HTTP(S) URL, or a PIL Image object.\n\n    Returns:\n        Generated caption string describing the image content.\n\n    Example:\n        &gt;&gt;&gt; captioner = ImageCaptioner()\n        &gt;&gt;&gt; caption = captioner.generate_caption(\"path/to/aerial.jpg\")\n        &gt;&gt;&gt; print(caption)\n        \"an aerial view of a building with a parking lot\"\n    \"\"\"\n    img = load_image(image_source)\n    inputs = self.processor(img, return_tensors=\"pt\").to(self.device)\n    out = self.blip_model.generate(**inputs)\n    caption = self.processor.decode(out[0], skip_special_tokens=True)\n    return caption\n</code></pre>"},{"location":"caption/#samgeo.caption.blip_analyze_image","title":"<code>blip_analyze_image(image_source, include_features=None, exclude_features=None, blip_model_name=None, spacy_model_name=None)</code>","text":"<p>Analyze an image by generating a caption and extracting features.</p> <p>This is a convenience function that provides the full pipeline for image analysis. For repeated use or custom model configurations, consider creating an ImageCaptioner instance directly.</p> <p>Parameters:</p> Name Type Description Default <code>image_source</code> <code>Union[str, Image]</code> <p>The image to analyze. Can be a local file path, an HTTP(S) URL, or a PIL Image object.</p> required <code>include_features</code> <code>Optional[Union[str, List[str]]]</code> <p>Controls which features to extract: - None: Extract any noun (excluding large-scale terms   and custom excludes). - \"default\" or [\"default\"]: Use the aerial_features.json   vocabulary for matching. - List of strings: Custom allowed features (with or without   underscores).</p> <code>None</code> <code>exclude_features</code> <code>Optional[List[str]]</code> <p>List of noun lemmas to exclude in addition to the built-in large-scale terms.</p> <code>None</code> <code>blip_model_name</code> <code>Optional[str]</code> <p>Name or path of the BLIP model to use. If None, uses the default \"Salesforce/blip-image-captioning-base\".</p> <code>None</code> <code>spacy_model_name</code> <code>Optional[str]</code> <p>Name of the spaCy model to use. If None, uses the default \"en_core_web_sm\".</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, List[str]]</code> <p>A tuple containing: - caption: The BLIP-generated caption string. - features: Sorted list of extracted feature names.</p> Example <p>caption, features = blip_analyze_image( ...     \"path/to/image.jpg\", ...     include_features=[\"default\"], ...     blip_model_name=\"Salesforce/blip-image-captioning-large\" ... ) print(caption) \"an aerial view of a parking lot with cars\" print(features) [\"car\", \"parking_lot\"]</p> Source code in <code>samgeo/caption.py</code> <pre><code>@torch.inference_mode()\ndef blip_analyze_image(\n    image_source: Union[str, Image.Image],\n    include_features: Optional[Union[str, List[str]]] = None,\n    exclude_features: Optional[List[str]] = None,\n    blip_model_name: Optional[str] = None,\n    spacy_model_name: Optional[str] = None,\n) -&gt; Tuple[str, List[str]]:\n    \"\"\"Analyze an image by generating a caption and extracting features.\n\n    This is a convenience function that provides the full pipeline for\n    image analysis. For repeated use or custom model configurations,\n    consider creating an ImageCaptioner instance directly.\n\n    Args:\n        image_source: The image to analyze. Can be a local file path,\n            an HTTP(S) URL, or a PIL Image object.\n        include_features: Controls which features to extract:\n            - None: Extract any noun (excluding large-scale terms\n              and custom excludes).\n            - \"default\" or [\"default\"]: Use the aerial_features.json\n              vocabulary for matching.\n            - List of strings: Custom allowed features (with or without\n              underscores).\n        exclude_features: List of noun lemmas to exclude in addition\n            to the built-in large-scale terms.\n        blip_model_name: Name or path of the BLIP model to use.\n            If None, uses the default \"Salesforce/blip-image-captioning-base\".\n        spacy_model_name: Name of the spaCy model to use.\n            If None, uses the default \"en_core_web_sm\".\n\n    Returns:\n        A tuple containing:\n            - caption: The BLIP-generated caption string.\n            - features: Sorted list of extracted feature names.\n\n    Example:\n        &gt;&gt;&gt; caption, features = blip_analyze_image(\n        ...     \"path/to/image.jpg\",\n        ...     include_features=[\"default\"],\n        ...     blip_model_name=\"Salesforce/blip-image-captioning-large\"\n        ... )\n        &gt;&gt;&gt; print(caption)\n        \"an aerial view of a parking lot with cars\"\n        &gt;&gt;&gt; print(features)\n        [\"car\", \"parking_lot\"]\n    \"\"\"\n    # Use custom models if specified, otherwise use default captioner\n    if blip_model_name is not None or spacy_model_name is not None:\n        captioner = ImageCaptioner(\n            blip_model_name=blip_model_name or DEFAULT_BLIP_MODEL,\n            spacy_model_name=spacy_model_name or DEFAULT_SPACY_MODEL,\n        )\n    else:\n        captioner = _get_default_captioner()\n\n    return captioner.analyze(\n        image_source,\n        include_features=include_features,\n        exclude_features=exclude_features,\n    )\n</code></pre>"},{"location":"caption/#samgeo.caption.ensure_spacy_model","title":"<code>ensure_spacy_model(model_name=DEFAULT_SPACY_MODEL)</code>","text":"<p>Download spaCy model only if it's missing.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the spaCy model to ensure is installed. Defaults to \"en_core_web_sm\".</p> <code>DEFAULT_SPACY_MODEL</code> Source code in <code>samgeo/caption.py</code> <pre><code>def ensure_spacy_model(model_name: str = DEFAULT_SPACY_MODEL) -&gt; None:\n    \"\"\"Download spaCy model only if it's missing.\n\n    Args:\n        model_name: Name of the spaCy model to ensure is installed.\n            Defaults to \"en_core_web_sm\".\n    \"\"\"\n    try:\n        importlib.import_module(model_name)\n    except ImportError:\n        print(f\"\u2193 Model '{model_name}' not found. Installing...\")\n        try:\n            download(model_name)\n            print(f\"\u2713 Model '{model_name}' installed.\")\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download spaCy model '{model_name}'. \"\n                f\"You may need to install it manually with: python -m spacy download {model_name}. \"\n                f\"Error: {e}\"\n            ) from e\n</code></pre>"},{"location":"caption/#samgeo.caption.extract_features_from_caption","title":"<code>extract_features_from_caption(caption, include_features=None, exclude_features=None)</code>","text":"<p>Extract features from a caption using NLP processing.</p> <p>This is a convenience function that uses the default ImageCaptioner instance. For more control over models, create an ImageCaptioner instance directly.</p> <p>Parameters:</p> Name Type Description Default <code>caption</code> <code>str</code> <p>The caption text to extract features from.</p> required <code>include_features</code> <code>Optional[Union[str, List[str]]]</code> <p>Controls which features to extract: - None: Extract any noun (excluding large-scale terms   and custom excludes). - \"default\" or [\"default\"]: Use the aerial_features.json   vocabulary for matching. - List of strings: Custom allowed features (with or without   underscores).</p> <code>None</code> <code>exclude_features</code> <code>Optional[List[str]]</code> <p>List of noun lemmas to exclude in addition to the built-in large-scale terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>Sorted list of extracted feature names (canonical keys or</p> <code>List[str]</code> <p>noun lemmas).</p> Example <p>features = extract_features_from_caption( ...     \"a parking lot with several cars\", ...     include_features=[\"default\"] ... ) print(features) [\"car\", \"parking_lot\"]</p> Source code in <code>samgeo/caption.py</code> <pre><code>def extract_features_from_caption(\n    caption: str,\n    include_features: Optional[Union[str, List[str]]] = None,\n    exclude_features: Optional[List[str]] = None,\n) -&gt; List[str]:\n    \"\"\"Extract features from a caption using NLP processing.\n\n    This is a convenience function that uses the default ImageCaptioner\n    instance. For more control over models, create an ImageCaptioner\n    instance directly.\n\n    Args:\n        caption: The caption text to extract features from.\n        include_features: Controls which features to extract:\n            - None: Extract any noun (excluding large-scale terms\n              and custom excludes).\n            - \"default\" or [\"default\"]: Use the aerial_features.json\n              vocabulary for matching.\n            - List of strings: Custom allowed features (with or without\n              underscores).\n        exclude_features: List of noun lemmas to exclude in addition\n            to the built-in large-scale terms.\n\n    Returns:\n        Sorted list of extracted feature names (canonical keys or\n        noun lemmas).\n\n    Example:\n        &gt;&gt;&gt; features = extract_features_from_caption(\n        ...     \"a parking lot with several cars\",\n        ...     include_features=[\"default\"]\n        ... )\n        &gt;&gt;&gt; print(features)\n        [\"car\", \"parking_lot\"]\n    \"\"\"\n    captioner = _get_default_captioner()\n    return captioner.extract_features(\n        caption,\n        include_features=include_features,\n        exclude_features=exclude_features,\n    )\n</code></pre>"},{"location":"caption/#samgeo.caption.load_aerial_feature_vocab","title":"<code>load_aerial_feature_vocab(url=AERIAL_FEATURES_URL)</code>","text":"<p>Load the nested aerial_features.json and flatten to a list of feature keys.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to the aerial features JSON file. Defaults to the Hugging Face hosted version.</p> <code>AERIAL_FEATURES_URL</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>Sorted list of feature keys extracted from the JSON file.</p> Source code in <code>samgeo/caption.py</code> <pre><code>def load_aerial_feature_vocab(url: str = AERIAL_FEATURES_URL) -&gt; List[str]:\n    \"\"\"Load the nested aerial_features.json and flatten to a list of feature keys.\n\n    Args:\n        url: URL to the aerial features JSON file. Defaults to the\n            Hugging Face hosted version.\n\n    Returns:\n        Sorted list of feature keys extracted from the JSON file.\n    \"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    data = resp.json()\n\n    features = set()\n\n    # data is two-level nested: {category: list OR {subcat: list}}\n    for _, val in data.items():\n        if isinstance(val, list):\n            features.update(val)\n        elif isinstance(val, dict):\n            for _, sublist in val.items():\n                if isinstance(sublist, list):\n                    features.update(sublist)\n\n    return sorted(features)\n</code></pre>"},{"location":"caption/#samgeo.caption.load_image","title":"<code>load_image(source)</code>","text":"<p>Load a PIL image from various sources.</p> <p>Supports loading from local file paths, HTTP(S) URLs, or returns the image directly if it's already a PIL.Image.Image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Image]</code> <p>The image source. Can be a local file path (str), an HTTP(S) URL (str), or an existing PIL Image object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image object converted to RGB mode.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the source type is not supported.</p> <code>HTTPError</code> <p>If downloading from URL fails.</p> <code>FileNotFoundError</code> <p>If local file path doesn't exist.</p> Source code in <code>samgeo/caption.py</code> <pre><code>def load_image(source: Union[str, Image.Image]) -&gt; Image.Image:\n    \"\"\"Load a PIL image from various sources.\n\n    Supports loading from local file paths, HTTP(S) URLs, or returns\n    the image directly if it's already a PIL.Image.Image.\n\n    Args:\n        source: The image source. Can be a local file path (str),\n            an HTTP(S) URL (str), or an existing PIL Image object.\n\n    Returns:\n        PIL Image object converted to RGB mode.\n\n    Raises:\n        TypeError: If the source type is not supported.\n        requests.HTTPError: If downloading from URL fails.\n        FileNotFoundError: If local file path doesn't exist.\n    \"\"\"\n    if isinstance(source, Image.Image):\n        return source.convert(\"RGB\")\n\n    if isinstance(source, str):\n        if source.startswith(\"http://\") or source.startswith(\"https://\"):\n            resp = requests.get(source, stream=True)\n            resp.raise_for_status()\n            return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n        else:\n            return Image.open(source).convert(\"RGB\")\n\n    raise TypeError(f\"Unsupported image source type: {type(source)}\")\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0102-nov-7-2023","title":"v0.10.2 - Nov 7, 2023","text":"<p>What's Changed</p> <ul> <li>Add JOSS paper by @giswqs in #197</li> <li>Add notebook for using Maxar Open Data by @giswqs in #198</li> <li>Add checkpoint to textsam.LangSAM() by @forestbat in #204</li> <li>Add workshop notebook by @giswqs in #209</li> </ul> <p>New Contributors</p> <ul> <li>@forestbat made their first contribution in #204</li> </ul> <p>Full Changelog: v0.10.1...v0.10.2</p>"},{"location":"changelog/#v0101-sep-1-2023","title":"v0.10.1 - Sep 1, 2023","text":"<p>What's Changed</p> <ul> <li>Fix basemap issue by @giswqs in #190</li> </ul> <p>Full Changelog: v0.10.0...v0.10.1)</p>"},{"location":"changelog/#v0100-aug-24-2023","title":"v0.10.0 - Aug 24, 2023","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>Added fastsam module by @giswqs #167</li> <li>Update optional dependencies by @giswqs in #68</li> <li>Improve contributing guidelines by @giswqs in #169</li> <li>[FIX] Added missing conversions from BGR to RGB by @lbferreira in #171</li> <li>Address JOSS review comments by @giswqs in #175</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<ul> <li>@lbferreira made their first contribution in #171</li> </ul>"},{"location":"changelog/#v091-aug-14-2023","title":"v0.9.1 - Aug 14, 2023","text":"<p>New Features</p> <ul> <li>Added support for HQ-SAM (#161)</li> <li>Added HQ-SAM notebooks (#162)</li> </ul>"},{"location":"changelog/#v090-aug-6-2023","title":"v0.9.0 - Aug 6, 2023","text":"<p>New Features</p> <ul> <li>Added support for multiple input boxes (#159)</li> </ul> <p>Improvements</p> <ul> <li>UpdateD groundingdino installation (#147)</li> <li>Updated README (#152)</li> </ul>"},{"location":"changelog/#v085-jul-19-2023","title":"v0.8.5 - Jul 19, 2023","text":"<p>Improvements</p> <ul> <li>Updated installation docs (#146)</li> <li>Updated leafmap and localtileserver to dependencies (#146)</li> <li>Added info about libgl1 dependency install on Linux systems (#141)</li> <li>Fixed save_masks bug without source image (#139)</li> </ul>"},{"location":"changelog/#v084-jul-5-2023","title":"v0.8.4 - Jul 5, 2023","text":"<p>Improvements</p> <ul> <li>Fixed model download bug (#136)</li> <li>Added legal notice (#133)</li> <li>Fixed image source bug for show_anns (#131)</li> <li>Improved exception handling for LangSAM GUI (#130)</li> <li>Added to return pixel coordinates of masks (#129)</li> <li>Added text_sam to docs (#123)</li> <li>Fixed file deletion error on Windows (#122)</li> <li>Fixed mask bug in text_sam/predict when the input is PIL image (#117)</li> </ul>"},{"location":"changelog/#v083-jun-20-2023","title":"v0.8.3 - Jun 20, 2023","text":"<p>New Features</p> <ul> <li>Added support for batch segmentation (#116)</li> <li>Added swimming pools example (#106)</li> </ul> <p>Improvements</p> <ul> <li>Removed 'flag' and 'param' arguments (#112)</li> <li>Used sorted function instead of if statements (#109)</li> </ul>"},{"location":"changelog/#v082-jun-14-2023","title":"v0.8.2 - Jun 14, 2023","text":"<p>New Features</p> <ul> <li>Added regularized option for vector output (#104)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Added more deep learning resources (#90)</li> <li>Use the force_filename parameter with hf_hub_download() (#93)</li> <li>Fixed typo (#94)</li> </ul>"},{"location":"changelog/#v081-may-24-2023","title":"v0.8.1 - May 24, 2023","text":"<p>Improvements</p> <ul> <li>Added huggingface_hub and remove onnx (#87)</li> <li>Added more demos to docs (#82)</li> </ul>"},{"location":"changelog/#v080-may-24-2023","title":"v0.8.0 - May 24, 2023","text":"<p>New Features</p> <ul> <li>Added support for using text prompts with SAM (#73)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Improved text prompt notebook (#79)</li> <li>Fixed notebook typos (#78)</li> <li>Added ArcGIS tutorial to docs (#72)</li> </ul>"},{"location":"changelog/#v070-may-20-2023","title":"v0.7.0 - May 20, 2023","text":"<p>New Features</p> <ul> <li>Added unittest (#58)</li> <li>Added JOSS paper draft (#61)</li> <li>Added ArcGIS notebook example (#63)</li> <li>Added text prompting segmentation (#65)</li> <li>Added support for segmenting non-georeferenced imagery (#66)</li> </ul> <p>Improvements</p> <ul> <li>Added blend option for show_anns method (#59)</li> <li>Updated ArcGIS installation instructions (#68, #70)</li> </ul> <p>Contributors</p> <p>@p-vdp @LucasOsco</p>"},{"location":"changelog/#v062-may-17-2023","title":"v0.6.2 - May 17, 2023","text":"<p>Improvements</p> <ul> <li>Added jupyter-server-proxy to Dockerfile for supporting add_raster (#57)</li> </ul>"},{"location":"changelog/#v061-may-16-2023","title":"v0.6.1 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added Dockerfile (#51)</li> </ul>"},{"location":"changelog/#v060-may-16-2023","title":"v0.6.0 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added interactive GUI for creating foreground and background markers (#44)</li> <li>Added support for custom projection bbox (#39)</li> </ul> <p>Improvements</p> <ul> <li>Fixed Colab Marker AwesomeIcon bug (#50)</li> <li>Added info about using SAM with Desktop GIS (#48)</li> <li>Use proper extension in the usage documentation (#43)</li> </ul> <p>Demos</p> <ul> <li>Interactive segmentation with input prompts</li> </ul> <p></p> <ul> <li>Input prompts from existing files</li> </ul> <p></p>"},{"location":"changelog/#v050-may-10-2023","title":"v0.5.0 - May 10, 2023","text":"<p>New Features</p> <ul> <li>Added support for input prompts (#30)</li> </ul> <p>Improvements</p> <ul> <li>Fixed the batch processing bug (#29)</li> </ul> <p>Demos</p> <p></p>"},{"location":"changelog/#v040-may-6-2023","title":"v0.4.0 - May 6, 2023","text":"<p>New Features</p> <ul> <li>Added new methods to <code>SamGeo</code> class, including <code>show_masks</code>, <code>save_masks</code>, <code>show_anns</code>, making it much easier to save segmentation results in GeoTIFF and vector formats.</li> <li>Added new functions to <code>common</code> module, including <code>array_to_image</code>, <code>show_image</code>, <code>download_file</code>, <code>overlay_images</code>, <code>blend_images</code>, and <code>update_package</code></li> <li>Added tow more notebooks, including automatic_mask_generator and satellite-predictor</li> <li>Added <code>SamGeoPredictor</code> class</li> </ul> <p>Improvements</p> <ul> <li>Improved <code>SamGeo.generate()</code> method</li> <li>Improved docstrings and API reference</li> <li>Added demos to docs</li> </ul> <p>Demos</p> <ul> <li>Automatic mask generator</li> </ul> <p></p> <p>Contributors</p> <p>@darrenwiens</p>"},{"location":"changelog/#v030-apr-26-2023","title":"v0.3.0 - Apr 26, 2023","text":"<p>New Features</p> <ul> <li>Added several new functions, including <code>get_basemaps</code>, <code>reproject</code>, <code>tiff_to_shp</code>, and <code>tiff_to_geojson</code></li> <li>Added hundereds of new basemaps through xyzservices</li> </ul> <p>Improvement</p> <ul> <li>Fixed <code>tiff_to_vector</code> crs bug #12</li> <li>Add <code>crs</code> parameter to <code>tms_to_geotiff</code></li> </ul>"},{"location":"changelog/#v020-apr-21-2023","title":"v0.2.0 - Apr 21, 2023","text":"<p>New Features</p> <ul> <li>Added notebook example</li> <li>Added <code>SamGeo.generate</code> method</li> <li>Added <code>SamGeo.tiff_to_vector</code> method</li> </ul>"},{"location":"changelog/#v010-apr-19-2023","title":"v0.1.0 - Apr 19, 2023","text":"<p>New Features</p> <ul> <li>Added <code>SamGeo</code> class</li> <li>Added GitHub Actions</li> <li>Added notebook example</li> </ul>"},{"location":"changelog/#v001-apr-18-2023","title":"v0.0.1 - Apr 18, 2023","text":"<p>Initial release</p>"},{"location":"changelog_update/","title":"Changelog update","text":"In\u00a0[\u00a0]: Copied! <pre>import re\n</pre> import re In\u00a0[\u00a0]: Copied! <pre># Copy the release notes from the GitHub release page\nmarkdown_text = \"\"\"\n## What's Changed\n* Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197\n* Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198\n* Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204\n* Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209\n\n## New Contributors\n* @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204\n\n**Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2\n\"\"\"\n</pre> # Copy the release notes from the GitHub release page markdown_text = \"\"\" ## What's Changed * Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197 * Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198 * Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204 * Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209  ## New Contributors * @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204  **Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2 \"\"\" In\u00a0[\u00a0]: Copied! <pre># Regular expression pattern to match the Markdown hyperlinks\npattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\"\n</pre> # Regular expression pattern to match the Markdown hyperlinks pattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\" In\u00a0[\u00a0]: Copied! <pre># Function to replace matched URLs with the desired format\ndef replace_url(match):\n    pr_number = match.group(1)\n    return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\"\n</pre> # Function to replace matched URLs with the desired format def replace_url(match):     pr_number = match.group(1)     return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\" In\u00a0[\u00a0]: Copied! <pre># Use re.sub to replace URLs with the desired format\nformatted_text = re.sub(pattern, replace_url, markdown_text)\n</pre> # Use re.sub to replace URLs with the desired format formatted_text = re.sub(pattern, replace_url, markdown_text) In\u00a0[\u00a0]: Copied! <pre>for line in formatted_text.splitlines():\n    if \"Full Changelog\" in line:\n        prefix = line.split(\": \")[0]\n        link = line.split(\": \")[1]\n        version = line.split(\"/\")[-1]\n        formatted_text = (\n            formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")\n            .replace(\"## What's Changed\", \"**What's Changed**\")\n            .replace(\"## New Contributors\", \"**New Contributors**\")\n        )\n</pre> for line in formatted_text.splitlines():     if \"Full Changelog\" in line:         prefix = line.split(\": \")[0]         link = line.split(\": \")[1]         version = line.split(\"/\")[-1]         formatted_text = (             formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")             .replace(\"## What's Changed\", \"**What's Changed**\")             .replace(\"## New Contributors\", \"**New Contributors**\")         ) In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/changelog_update.md\", \"w\") as f:\n    f.write(formatted_text)\n</pre> with open(\"docs/changelog_update.md\", \"w\") as f:     f.write(formatted_text) In\u00a0[\u00a0]: Copied! <pre># Print the formatted text\nprint(formatted_text)\n</pre> # Print the formatted text print(formatted_text) <p>Copy the formatted text and paste it to the CHANGELOG.md file</p>"},{"location":"common/","title":"common module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"common/#samgeo.common.array_to_image","title":"<code>array_to_image(array, output, source=None, dtype=None, compress='deflate', **kwargs)</code>","text":"<p>Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The NumPy array to be saved as a GeoTIFF.</p> required <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>source</code> <code>str</code> <p>The path to an existing GeoTIFF file with map projection information. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output array. Defaults to None.</p> <code>None</code> <code>compress</code> <code>str</code> <p>The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".</p> <code>'deflate'</code> Source code in <code>samgeo/common.py</code> <pre><code>def array_to_image(\n    array, output, source=None, dtype=None, compress=\"deflate\", **kwargs\n):\n    \"\"\"Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.\n\n    Args:\n        array (np.ndarray): The NumPy array to be saved as a GeoTIFF.\n        output (str): The path to the output image.\n        source (str, optional): The path to an existing GeoTIFF file with map projection information. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output array. Defaults to None.\n        compress (str, optional): The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".\n    \"\"\"\n\n    from PIL import Image\n\n    if isinstance(array, str) and os.path.exists(array):\n        array = cv2.imread(array)\n        array = cv2.cvtColor(array, cv2.COLOR_BGR2RGB)\n\n    if output.endswith(\".tif\"):\n        if source is not None:\n            with rasterio.open(source) as src:\n                crs = src.crs\n                transform = src.transform\n                if compress is None:\n                    compress = src.compression\n        else:\n            crs = kwargs.get(\"crs\", None)\n            transform = kwargs.get(\"transform\", None)\n\n        # Determine the minimum and maximum values in the array\n\n        min_value = np.min(array)\n        max_value = np.max(array)\n\n        if dtype is None:\n            # Determine the best dtype for the array\n            if min_value &gt;= 0 and max_value &lt;= 1:\n                dtype = np.float32\n            elif min_value &gt;= 0 and max_value &lt;= 255:\n                dtype = np.uint8\n            elif min_value &gt;= -128 and max_value &lt;= 127:\n                dtype = np.int8\n            elif min_value &gt;= 0 and max_value &lt;= 65535:\n                dtype = np.uint16\n            elif min_value &gt;= -32768 and max_value &lt;= 32767:\n                dtype = np.int16\n            else:\n                dtype = np.float64\n\n        # Convert the array to the best dtype\n        array = array.astype(dtype)\n\n        # Define the GeoTIFF metadata\n        if array.ndim == 2:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": 1,\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n        elif array.ndim == 3:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": array.shape[2],\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n\n        if compress is not None:\n            metadata[\"compress\"] = compress\n        else:\n            raise ValueError(\"Array must be 2D or 3D.\")\n\n        # Create a new GeoTIFF file and write the array to it\n        with rasterio.open(output, \"w\", **metadata) as dst:\n            if array.ndim == 2:\n                dst.write(array, 1)\n            elif array.ndim == 3:\n                for i in range(array.shape[2]):\n                    dst.write(array[:, :, i], i + 1)\n\n    else:\n        img = Image.fromarray(array)\n        img.save(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>samgeo/common.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: list, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"common/#samgeo.common.blend_images","title":"<code>blend_images(img1, img2, alpha=0.5, output=False, show=True, figsize=(12, 10), axis='off', **kwargs)</code>","text":"<p>Blends two images together using the addWeighted function from the OpenCV library.</p> <p>Parameters:</p> Name Type Description Default <code>img1</code> <code>ndarray</code> <p>The first input image on top represented as a NumPy array.</p> required <code>img2</code> <code>ndarray</code> <p>The second input image at the bottom represented as a NumPy array.</p> required <code>alpha</code> <code>float</code> <p>The weighting factor for the first image in the blend. By default, this is set to 0.5.</p> <code>0.5</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether to display the blended image. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>The axis of the figure. Defaults to \"off\".</p> <code>'off'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the cv2.addWeighted() function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>numpy.ndarray: The blended image as a NumPy array.</p> Source code in <code>samgeo/common.py</code> <pre><code>def blend_images(\n    img1,\n    img2,\n    alpha=0.5,\n    output=False,\n    show=True,\n    figsize=(12, 10),\n    axis=\"off\",\n    **kwargs,\n):\n    \"\"\"\n    Blends two images together using the addWeighted function from the OpenCV library.\n\n    Args:\n        img1 (numpy.ndarray): The first input image on top represented as a NumPy array.\n        img2 (numpy.ndarray): The second input image at the bottom represented as a NumPy array.\n        alpha (float): The weighting factor for the first image in the blend. By default, this is set to 0.5.\n        output (str, optional): The path to the output image. Defaults to False.\n        show (bool, optional): Whether to display the blended image. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (12, 10).\n        axis (str, optional): The axis of the figure. Defaults to \"off\".\n        **kwargs: Additional keyword arguments to pass to the cv2.addWeighted() function.\n\n    Returns:\n        numpy.ndarray: The blended image as a NumPy array.\n    \"\"\"\n    # Resize the images to have the same dimensions\n    if isinstance(img1, str):\n        if img1.startswith(\"http\"):\n            img1 = download_file(img1)\n\n        if not os.path.exists(img1):\n            raise ValueError(f\"Input path {img1} does not exist.\")\n\n        img1 = cv2.imread(img1)\n\n    if isinstance(img2, str):\n        if img2.startswith(\"http\"):\n            img2 = download_file(img2)\n\n        if not os.path.exists(img2):\n            raise ValueError(f\"Input path {img2} does not exist.\")\n\n        img2 = cv2.imread(img2)\n\n    if img1.dtype == np.float32:\n        img1 = (img1 * 255).astype(np.uint8)\n\n    if img2.dtype == np.float32:\n        img2 = (img2 * 255).astype(np.uint8)\n\n    if img1.dtype != img2.dtype:\n        img2 = img2.astype(img1.dtype)\n\n    img1 = cv2.resize(img1, (img2.shape[1], img2.shape[0]))\n\n    # Blend the images using the addWeighted function\n    beta = 1 - alpha\n    blend_img = cv2.addWeighted(img1, alpha, img2, beta, 0, **kwargs)\n\n    if output:\n        array_to_image(blend_img, output, img2)\n\n    if show:\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=figsize)\n        plt.imshow(blend_img)\n        plt.axis(axis)\n        plt.show()\n    else:\n        return blend_img\n</code></pre>"},{"location":"common/#samgeo.common.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <p>geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def boxes_to_vector(coords, src_crs, dst_crs=\"EPSG:4326\", output=None, **kwargs):\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"common/#samgeo.common.check_file_path","title":"<code>check_file_path(file_path, make_dirs=True)</code>","text":"<p>Gets the absolute file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file.</p> required <code>make_dirs</code> <code>bool</code> <p>Whether to create the directory if it does not exist. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory could not be found.</p> <code>TypeError</code> <p>If the input directory path is not a string.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The absolute path to the file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def check_file_path(file_path, make_dirs=True):\n    \"\"\"Gets the absolute file path.\n\n    Args:\n        file_path (str): The path to the file.\n        make_dirs (bool, optional): Whether to create the directory if it does not exist. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If the directory could not be found.\n        TypeError: If the input directory path is not a string.\n\n    Returns:\n        str: The absolute path to the file.\n    \"\"\"\n    if isinstance(file_path, str):\n        if file_path.startswith(\"~\"):\n            file_path = os.path.expanduser(file_path)\n        else:\n            file_path = os.path.abspath(file_path)\n\n        file_dir = os.path.dirname(file_path)\n        if not os.path.exists(file_dir) and make_dirs:\n            os.makedirs(file_dir)\n\n        return file_path\n\n    else:\n        raise TypeError(\"The provided file path must be a string.\")\n</code></pre>"},{"location":"common/#samgeo.common.choose_device","title":"<code>choose_device(empty_cache=True, quiet=True)</code>","text":"<p>Choose a device (CPU or GPU) for deep learning.</p> <p>Parameters:</p> Name Type Description Default <code>empty_cache</code> <code>bool</code> <p>Whether to empty the CUDA cache if a GPU is used. Defaults to True.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress device information printout. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The device name.</p> Source code in <code>samgeo/common.py</code> <pre><code>def choose_device(empty_cache: bool = True, quiet: bool = True) -&gt; str:\n    \"\"\"Choose a device (CPU or GPU) for deep learning.\n\n    Args:\n        empty_cache (bool): Whether to empty the CUDA cache if a GPU is used. Defaults to True.\n        quiet (bool): Whether to suppress device information printout. Defaults to True.\n\n    Returns:\n        str: The device name.\n    \"\"\"\n    import torch\n\n    # if using Apple MPS, fall back to CPU for unsupported ops\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n    # select the device for computation\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    if not quiet:\n        print(f\"Using device: {device}\")\n\n    if device.type == \"cuda\":\n        if empty_cache:\n            torch.cuda.empty_cache()\n        # use bfloat16 for the entire notebook\n        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n        if torch.cuda.get_device_properties(0).major &gt;= 8:\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n    elif device.type == \"mps\":\n        if not quiet:\n            print(\n                \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n                \"give numerically different outputs and sometimes degraded performance on MPS. \"\n                \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n            )\n    return device\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_geojson","title":"<code>coords_to_geojson(coords, output=None)</code>","text":"<p>Convert a list of coordinates (lon, lat) to a GeoJSON string or file.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of coordinates (lon, lat).</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A GeoJSON dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_geojson(coords, output=None):\n    \"\"\"Convert a list of coordinates (lon, lat) to a GeoJSON string or file.\n\n    Args:\n        coords (list): A list of coordinates (lon, lat).\n        output (str, optional): The output file path. Defaults to None.\n\n    Returns:\n        dict: A GeoJSON dictionary.\n    \"\"\"\n\n    import json\n\n    if len(coords) == 0:\n        return\n    # Create a GeoJSON FeatureCollection object\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": []}\n\n    # Iterate through the coordinates list and create a GeoJSON Feature object for each coordinate\n    for coord in coords:\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": {\"type\": \"Point\", \"coordinates\": coord},\n            \"properties\": {},\n        }\n        feature_collection[\"features\"].append(feature)\n\n    # Convert the FeatureCollection object to a JSON string\n    geojson_str = json.dumps(feature_collection)\n\n    if output is not None:\n        with open(output, \"w\") as f:\n            f.write(geojson_str)\n    else:\n        return geojson_str\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>ndarray</code> <p>A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]     or [[[x1, y1]], [[x2, y2]], ...].</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <p>Whether to return out-of-bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D or 3D array of pixel coordinates in the same format as the input.</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: np.ndarray,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds=False,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]\n                or [[[x1, y1]], [[x2, y2]], ...].\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out-of-bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A 2D or 3D array of pixel coordinates in the same format as the input.\n    \"\"\"\n    from rasterio.warp import transform as transform_coords\n\n    out_of_bounds = []\n    if isinstance(coords, np.ndarray):\n        input_is_3d = coords.ndim == 3  # Check if the input is a 3D array\n    else:\n        input_is_3d = False\n\n    # Flatten the 3D array to 2D if necessary\n    if input_is_3d:\n        original_shape = coords.shape  # Store the original shape\n        coords = coords.reshape(-1, 2)  # Flatten to 2D\n\n    # Convert ndarray to a list if necessary\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(coord_crs, src.crs, xs, ys, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n    all_coords = []  # Include all coordinates for return_out_of_bounds=True\n\n    for i, (x, y) in enumerate(result):\n        all_coords.append([x, y])\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # Convert the output back to the original shape if input was 3D\n    output = np.array(output)\n    all_coords = np.array(all_coords)\n    if input_is_3d:\n        if len(output) &gt; 0:\n            output = output.reshape(original_shape)\n        if len(all_coords) &gt; 0:\n            all_coords = all_coords.reshape(original_shape)\n\n    # Handle cases where no valid pixel coordinates are found\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        # Return all coordinates (including out-of-bounds) when return_out_of_bounds=True\n        # This allows callers to handle out-of-bounds coordinates themselves\n        return all_coords, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint","title":"<code>download_checkpoint(model_type='vit_h', checkpoint_dir=None, hq=False)</code>","text":"<p>Download the SAM model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. Can be one of ['vit_h', 'vit_l', 'vit_b']. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_dir</code> <code>str</code> <p>The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.</p> <code>False</code> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint(model_type=\"vit_h\", checkpoint_dir=None, hq=False):\n    \"\"\"Download the SAM model checkpoint.\n\n    Args:\n        model_type (str, optional): The model type. Can be one of ['vit_h', 'vit_l', 'vit_b'].\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_dir (str, optional): The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".\n        hq (bool, optional): Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.\n    \"\"\"\n\n    if not hq:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_vit_h_4b8939.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n            },\n            \"vit_l\": {\n                \"name\": \"sam_vit_l_0b3195.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_vit_b_01ec64.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n            },\n        }\n    else:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_hq_vit_h.pth\",\n                \"url\": [\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.zip\",\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.z01\",\n                ],\n            },\n            \"vit_l\": {\n                \"name\": \"sam_hq_vit_l.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_l.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_hq_vit_b.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_b.pth\",\n            },\n            \"vit_tiny\": {\n                \"name\": \"sam_hq_vit_tiny.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_tiny.pth\",\n            },\n        }\n\n    if model_type not in model_types:\n        raise ValueError(\n            f\"Invalid model_type: {model_type}. It must be one of {', '.join(model_types)}\"\n        )\n\n    if checkpoint_dir is None:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    checkpoint = os.path.join(checkpoint_dir, model_types[model_type][\"name\"])\n    if not os.path.exists(checkpoint):\n        print(f\"Model checkpoint for {model_type} not found.\")\n        url = model_types[model_type][\"url\"]\n        if isinstance(url, str):\n            download_file(url, checkpoint)\n        elif isinstance(url, list):\n            download_files(url, checkpoint_dir, multi_part=True)\n    return checkpoint\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint_legacy","title":"<code>download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs)</code>","text":"<p>Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The checkpoint URL. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs):\n    \"\"\"Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n\n    Args:\n        url (str, optional): The checkpoint URL. Defaults to None.\n        output (str, optional): The output file path. Defaults to None.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    checkpoints = {\n        \"sam_vit_h_4b8939.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        \"sam_vit_l_0b3195.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n        \"sam_vit_b_01ec64.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n    }\n\n    if isinstance(url, str) and url in checkpoints:\n        url = checkpoints[url]\n\n    if url is None:\n        url = checkpoints[\"sam_vit_h_4b8939.pth\"]\n\n    if output is None:\n        output = os.path.basename(url)\n\n    return download_file(url, output, overwrite=overwrite, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.download_file","title":"<code>download_file(url=None, output=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False)</code>","text":"<p>Download a file from URL, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Google Drive URL is also supported. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_file(\n    url=None,\n    output=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n):\n    \"\"\"Download a file from URL, including Google Drive shared URL.\n\n    Args:\n        url (str, optional): Google Drive URL is also supported. Defaults to None.\n        output (str, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string,\n            in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    import zipfile\n\n    try:\n        import gdown\n    except ImportError:\n        print(\n            \"The gdown package is required for this function. Use `pip install gdown` to install it.\"\n        )\n        return\n\n    if output is None:\n        if isinstance(url, str) and url.startswith(\"http\"):\n            output = os.path.basename(url)\n\n    out_dir = os.path.abspath(os.path.dirname(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(url, str):\n        if os.path.exists(os.path.abspath(output)) and (not overwrite):\n            print(\n                f\"{output} already exists. Skip downloading. Set overwrite=True to overwrite.\"\n            )\n            return os.path.abspath(output)\n        else:\n            url = github_raw_url(url)\n\n    if \"https://drive.google.com/file/d/\" in url:\n        fuzzy = True\n\n    output = gdown.download(\n        url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume\n    )\n\n    if unzip and output.endswith(\".zip\"):\n        with zipfile.ZipFile(output, \"r\") as zip_ref:\n            if not quiet:\n                print(\"Extracting files...\")\n            if subfolder:\n                basename = os.path.splitext(os.path.basename(output))[0]\n\n                output = os.path.join(out_dir, basename)\n                if not os.path.exists(output):\n                    os.makedirs(output)\n                zip_ref.extractall(output)\n            else:\n                zip_ref.extractall(os.path.dirname(output))\n\n    return os.path.abspath(output)\n</code></pre>"},{"location":"common/#samgeo.common.download_files","title":"<code>download_files(urls, out_dir=None, filenames=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False, multi_part=False)</code>","text":"<p>Download files from URLs, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>The list of urls to download. Google Drive URL is also supported.</p> required <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>filenames</code> <code>list</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <code>multi_part</code> <code>bool</code> <p>If the file is a multi-part file. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\nbase_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\nurls = [base_url + f for f in files]\nleafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n</code></pre> Source code in <code>samgeo/common.py</code> <pre><code>def download_files(\n    urls,\n    out_dir=None,\n    filenames=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n    multi_part=False,\n):\n    \"\"\"Download files from URLs, including Google Drive shared URL.\n\n    Args:\n        urls (list): The list of urls to download. Google Drive URL is also supported.\n        out_dir (str, optional): The output directory. Defaults to None.\n        filenames (list, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n        multi_part (bool, optional): If the file is a multi-part file. Defaults to False.\n\n    Examples:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n    \"\"\"\n\n    if out_dir is None:\n        out_dir = os.getcwd()\n\n    if filenames is None:\n        filenames = [None] * len(urls)\n\n    filepaths = []\n    for url, output in zip(urls, filenames):\n        if output is None:\n            filename = os.path.join(out_dir, os.path.basename(url))\n        else:\n            filename = os.path.join(out_dir, output)\n\n        filepaths.append(filename)\n        if multi_part:\n            unzip = False\n\n        download_file(\n            url,\n            filename,\n            quiet,\n            proxy,\n            speed,\n            use_cookies,\n            verify,\n            id,\n            fuzzy,\n            resume,\n            unzip,\n            overwrite,\n            subfolder,\n        )\n\n    if multi_part:\n        archive = os.path.splitext(filename)[0] + \".zip\"\n        out_dir = os.path.dirname(filename)\n        extract_archive(archive, out_dir)\n\n        for file in filepaths:\n            os.remove(file)\n</code></pre>"},{"location":"common/#samgeo.common.extract_archive","title":"<code>extract_archive(archive, outdir=None, **kwargs)</code>","text":"<p>Extracts a multipart archive.</p> <p>This function uses the patoolib library to extract a multipart archive. If the patoolib library is not installed, it attempts to install it. If the archive does not end with \".zip\", it appends \".zip\" to the archive name. If the extraction fails (for example, if the files already exist), it skips the extraction.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>str</code> <p>The path to the archive file.</p> required <code>outdir</code> <code>str</code> <p>The directory where the archive should be extracted.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for the patoolib.extract_archive function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>An exception is raised if the extraction fails for reasons other than the files already existing.</p> <p>Example:</p> <pre><code>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\nbase_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\nurls = [base_url + f for f in files]\nleafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n</code></pre> Source code in <code>samgeo/common.py</code> <pre><code>def extract_archive(archive, outdir=None, **kwargs):\n    \"\"\"\n    Extracts a multipart archive.\n\n    This function uses the patoolib library to extract a multipart archive.\n    If the patoolib library is not installed, it attempts to install it.\n    If the archive does not end with \".zip\", it appends \".zip\" to the archive name.\n    If the extraction fails (for example, if the files already exist), it skips the extraction.\n\n    Args:\n        archive (str): The path to the archive file.\n        outdir (str): The directory where the archive should be extracted.\n        **kwargs: Arbitrary keyword arguments for the patoolib.extract_archive function.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: An exception is raised if the extraction fails for reasons other than the files already existing.\n\n    Example:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n\n    \"\"\"\n    try:\n        import patoolib\n    except ImportError:\n        install_package(\"patool\")\n        import patoolib\n\n    if not archive.endswith(\".zip\"):\n        archive = archive + \".zip\"\n\n    if outdir is None:\n        outdir = os.path.dirname(archive)\n\n    try:\n        patoolib.extract_archive(archive, outdir=outdir, **kwargs)\n    except Exception as e:\n        print(\"The unzipped files might already exist. Skipping extraction.\")\n        return\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.geotiff_to_jpg","title":"<code>geotiff_to_jpg(geotiff_path, output_path, bands=None)</code>","text":"<p>Convert a GeoTIFF file to a JPG file.</p> <p>Parameters:</p> Name Type Description Default <code>geotiff_path</code> <code>str</code> <p>The path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>The path to the output JPG file.</p> required <code>bands</code> <code>List[int]</code> <p>List of band indices (1-based) to use for RGB. For example, [4, 3, 2] for NIR-R-G false color composite. If None, uses the first 3 bands. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def geotiff_to_jpg(\n    geotiff_path: str,\n    output_path: str,\n    bands: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"Convert a GeoTIFF file to a JPG file.\n\n    Args:\n        geotiff_path (str): The path to the input GeoTIFF file.\n        output_path (str): The path to the output JPG file.\n        bands (List[int], optional): List of band indices (1-based) to use for RGB.\n            For example, [4, 3, 2] for NIR-R-G false color composite.\n            If None, uses the first 3 bands. Defaults to None.\n    \"\"\"\n\n    from PIL import Image\n\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_path) as src:\n        if bands is not None:\n            # Validate band indices (1-based)\n            if len(bands) != 3:\n                raise ValueError(\"bands must contain exactly 3 band indices for RGB.\")\n            for band in bands:\n                if band &lt; 1 or band &gt; src.count:\n                    raise ValueError(\n                        f\"Band index {band} is out of range. \"\n                        f\"Image has {src.count} bands (1-indexed).\"\n                    )\n            # Read specified bands (rasterio uses 1-based indexing)\n            array = np.stack([src.read(b) for b in bands], axis=0)\n        else:\n            # Read all bands\n            array = src.read()\n\n            # If the array has more than 3 bands, reduce it to the first 3 (RGB)\n            if array.shape[0] &gt;= 3:\n                array = array[:3, :, :]  # Select the first 3 bands (R, G, B)\n            elif array.shape[0] == 1:\n                # For single-band images, repeat the band to create a grayscale RGB\n                array = np.repeat(array, 3, axis=0)\n            elif array.shape[0] == 2:\n                # For two-band images, repeat the first band to create a 3-band image\n                array = np.concatenate([array, array[0:1, :, :]], axis=0)\n\n        # Transpose the array from (bands, height, width) to (height, width, bands)\n        array = np.transpose(array, (1, 2, 0))\n\n        # Normalize the array to 8-bit (0-255) range for JPG\n        array = array.astype(np.float32)\n        array -= array.min()\n        array /= array.max()\n        array *= 255\n        array = array.astype(np.uint8)\n\n        # Convert to a PIL Image and save as JPG\n        image = Image.fromarray(array)\n        image.save(output_path)\n</code></pre>"},{"location":"common/#samgeo.common.geotiff_to_jpg_batch","title":"<code>geotiff_to_jpg_batch(input_folder, output_folder=None, bands=None)</code>","text":"<p>Convert all GeoTIFF files in a folder to JPG files.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The path to the folder containing GeoTIFF files.</p> required <code>output_folder</code> <code>str</code> <p>The path to the folder to save the output JPG files.</p> <code>None</code> <code>bands</code> <code>List[int]</code> <p>List of band indices (1-based) to use for RGB. For example, [4, 3, 2] for NIR-R-G false color composite. If None, uses the first 3 bands. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the output folder containing the JPG files.</p> Source code in <code>samgeo/common.py</code> <pre><code>def geotiff_to_jpg_batch(\n    input_folder: str,\n    output_folder: str = None,\n    bands: Optional[List[int]] = None,\n) -&gt; str:\n    \"\"\"Convert all GeoTIFF files in a folder to JPG files.\n\n    Args:\n        input_folder (str): The path to the folder containing GeoTIFF files.\n        output_folder (str): The path to the folder to save the output JPG files.\n        bands (List[int], optional): List of band indices (1-based) to use for RGB.\n            For example, [4, 3, 2] for NIR-R-G false color composite.\n            If None, uses the first 3 bands. Defaults to None.\n\n    Returns:\n        str: The path to the output folder containing the JPG files.\n    \"\"\"\n\n    if output_folder is None:\n        output_folder = make_temp_dir()\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    geotiff_files = [\n        f for f in os.listdir(input_folder) if f.endswith(\".tif\") or f.endswith(\".tiff\")\n    ]\n\n    # Initialize tqdm progress bar\n    for filename in tqdm(geotiff_files, desc=\"Converting GeoTIFF to JPG\"):\n        geotiff_path = os.path.join(input_folder, filename)\n        jpg_filename = os.path.splitext(filename)[0] + \".jpg\"\n        output_path = os.path.join(output_folder, jpg_filename)\n        geotiff_to_jpg(geotiff_path, output_path, bands=bands)\n\n    return output_folder\n</code></pre>"},{"location":"common/#samgeo.common.get_basemaps","title":"<code>get_basemaps(free_only=True)</code>","text":"<p>Returns a dictionary of xyz basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of xyz basemaps.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_basemaps(free_only=True):\n    \"\"\"Returns a dictionary of xyz basemaps.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz basemaps.\n    \"\"\"\n\n    basemaps = {}\n    xyz_dict = get_xyz_dict(free_only=free_only)\n    for item in xyz_dict:\n        name = xyz_dict[item].name\n        url = xyz_dict[item].build_url()\n        basemaps[name] = url\n\n    return basemaps\n</code></pre>"},{"location":"common/#samgeo.common.get_device","title":"<code>get_device()</code>","text":"<p>Returns the best available device for deep learning in the order: CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Returns the best available device for deep learning in the order:\n    CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"common/#samgeo.common.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_raster_info(raster_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"common/#samgeo.common.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel values unchanged.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing lists of statistics with keys: - 'min': List of minimum values for each band - 'max': List of maximum values for each band - 'mean': List of mean values for each band - 'std': List of standard deviation values for each band</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_raster_stats(raster_path: str, divide_by: float = 1.0) -&gt; Dict[str, Any]:\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel values unchanged.\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"common/#samgeo.common.get_vector_crs","title":"<code>get_vector_crs(filename, **kwargs)</code>","text":"<p>Gets the CRS of a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The CRS of the vector file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_vector_crs(filename, **kwargs):\n    \"\"\"Gets the CRS of a vector file.\n\n    Args:\n        filename (str): The vector file path.\n\n    Returns:\n        str: The CRS of the vector file.\n    \"\"\"\n    gdf = gpd.read_file(filename, **kwargs)\n    epsg = gdf.crs.to_epsg()\n    if epsg is None:\n        return gdf.crs\n    else:\n        return f\"EPSG:{epsg}\"\n</code></pre>"},{"location":"common/#samgeo.common.get_xyz_dict","title":"<code>get_xyz_dict(free_only=True)</code>","text":"<p>Returns a dictionary of xyz services.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of xyz services.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_xyz_dict(free_only=True):\n    \"\"\"Returns a dictionary of xyz services.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz services.\n    \"\"\"\n    import collections\n\n    import xyzservices.providers as xyz\n\n    def _unpack_sub_parameters(var, param):\n        temp = var\n        for sub_param in param.split(\".\"):\n            temp = getattr(temp, sub_param)\n        return temp\n\n    xyz_dict = {}\n    for item in xyz.values():\n        try:\n            name = item[\"name\"]\n            tile = _unpack_sub_parameters(xyz, name)\n            if _unpack_sub_parameters(xyz, name).requires_token():\n                if free_only:\n                    pass\n                else:\n                    xyz_dict[name] = tile\n            else:\n                xyz_dict[name] = tile\n\n        except Exception:\n            for sub_item in item:\n                name = item[sub_item][\"name\"]\n                tile = _unpack_sub_parameters(xyz, name)\n                if _unpack_sub_parameters(xyz, name).requires_token():\n                    if free_only:\n                        pass\n                    else:\n                        xyz_dict[name] = tile\n                else:\n                    xyz_dict[name] = tile\n\n    xyz_dict = collections.OrderedDict(sorted(xyz_dict.items()))\n    return xyz_dict\n</code></pre>"},{"location":"common/#samgeo.common.github_raw_url","title":"<code>github_raw_url(url)</code>","text":"<p>Get the raw URL for a GitHub file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The GitHub URL.</p> required <p>Returns:     str: The raw URL.</p> Source code in <code>samgeo/common.py</code> <pre><code>def github_raw_url(url):\n    \"\"\"Get the raw URL for a GitHub file.\n\n    Args:\n        url (str): The GitHub URL.\n    Returns:\n        str: The raw URL.\n    \"\"\"\n    if isinstance(url, str) and url.startswith(\"https://github.com/\") and \"blob\" in url:\n        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n    return url\n</code></pre>"},{"location":"common/#samgeo.common.image_to_cog","title":"<code>image_to_cog(source, dst_path=None, profile='deflate', **kwargs)</code>","text":"<p>Converts an image to a COG file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A dataset path, URL or rasterio.io.DatasetReader object.</p> required <code>dst_path</code> <code>str</code> <p>An output dataset path or or PathLike object. Defaults to None.</p> <code>None</code> <code>profile</code> <code>str</code> <p>COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".</p> <code>'deflate'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If rio-cogeo is not installed.</p> <code>FileNotFoundError</code> <p>If the source file could not be found.</p> Source code in <code>samgeo/common.py</code> <pre><code>def image_to_cog(source, dst_path=None, profile=\"deflate\", **kwargs):\n    \"\"\"Converts an image to a COG file.\n\n    Args:\n        source (str): A dataset path, URL or rasterio.io.DatasetReader object.\n        dst_path (str, optional): An output dataset path or or PathLike object. Defaults to None.\n        profile (str, optional): COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".\n\n    Raises:\n        ImportError: If rio-cogeo is not installed.\n        FileNotFoundError: If the source file could not be found.\n    \"\"\"\n    try:\n        from rio_cogeo.cogeo import cog_translate\n        from rio_cogeo.profiles import cog_profiles\n\n    except ImportError:\n        raise ImportError(\n            \"The rio-cogeo package is not installed. Please install it with `pip install rio-cogeo` or `conda install rio-cogeo -c conda-forge`.\"\n        )\n\n    if not source.startswith(\"http\"):\n        source = check_file_path(source)\n\n        if not os.path.exists(source):\n            raise FileNotFoundError(\"The provided input file could not be found.\")\n\n    if dst_path is None:\n        if not source.startswith(\"http\"):\n            dst_path = os.path.splitext(source)[0] + \"_cog.tif\"\n        else:\n            dst_path = temp_file_path(extension=\".tif\")\n\n    dst_path = check_file_path(dst_path)\n\n    dst_profile = cog_profiles.get(profile)\n    cog_translate(source, dst_path, dst_profile, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.images_to_video","title":"<code>images_to_video(images, output_video, fps=30, video_size=None)</code>","text":"<p>Converts a series of images into a video. The input can be either a directory containing the images or a list of image file paths.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Union[str, List[str]]</code> <p>A directory containing images or a list of image file paths.</p> required <code>output_video</code> <code>str</code> <p>The filename of the output video (e.g., 'output.mp4').</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video. Default is 30.</p> <code>30</code> <code>video_size</code> <code>Optional[tuple]</code> <p>The size (width, height) of the video. If not provided, the size of the first image is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided path is not a directory, if the images list is empty, or if the first image cannot be read.</p> Example usage <p>images_to_video('path_to_image_directory', 'output_video.mp4', fps=30, video_size=(1280, 720)) images_to_video(['image1.jpg', 'image2.jpg', 'image3.jpg'], 'output_video.mp4', fps=30)</p> Source code in <code>samgeo/common.py</code> <pre><code>def images_to_video(\n    images: Union[str, List[str]],\n    output_video: str,\n    fps: int = 30,\n    video_size: Optional[tuple] = None,\n) -&gt; None:\n    \"\"\"\n    Converts a series of images into a video. The input can be either a directory\n    containing the images or a list of image file paths.\n\n    Args:\n        images (Union[str, List[str]]): A directory containing images or a list\n            of image file paths.\n        output_video (str): The filename of the output video (e.g., 'output.mp4').\n        fps (int, optional): Frames per second for the output video. Default is 30.\n        video_size (Optional[tuple], optional): The size (width, height) of the\n            video. If not provided, the size of the first image is used.\n\n    Raises:\n        ValueError: If the provided path is not a directory, if the images list\n            is empty, or if the first image cannot be read.\n\n    Example usage:\n        images_to_video('path_to_image_directory', 'output_video.mp4', fps=30, video_size=(1280, 720))\n        images_to_video(['image1.jpg', 'image2.jpg', 'image3.jpg'], 'output_video.mp4', fps=30)\n    \"\"\"\n    if isinstance(images, str):\n        if not os.path.isdir(images):\n            raise ValueError(f\"The provided path {images} is not a valid directory.\")\n\n        # Get all image files in the directory (sorted by filename)\n\n        files = sorted(os.listdir(images))\n        if len(files) == 0:\n            raise ValueError(f\"No image files found in the directory {images}\")\n        elif files[0].endswith(\".tif\"):\n            images = geotiff_to_jpg_batch(images)\n\n        images = [\n            os.path.join(images, img)\n            for img in sorted(os.listdir(images))\n            if img.endswith((\".jpg\", \".png\"))\n        ]\n\n    if not isinstance(images, list) or not images:\n        raise ValueError(\n            \"The images parameter should either be a non-empty list of image paths or a valid directory.\"\n        )\n\n    # Read the first image to get the dimensions if video_size is not provided\n    first_image_path = images[0]\n    frame = cv2.imread(first_image_path)\n\n    if frame is None:\n        raise ValueError(f\"Error reading the first image {first_image_path}\")\n\n    if video_size is None:\n        height, width, _ = frame.shape\n        video_size = (width, height)\n\n    # Try different codecs for compatibility across platforms\n    codecs = [\"avc1\", \"mp4v\", \"XVID\"]\n    video_writer = None\n\n    for codec in codecs:\n        fourcc = cv2.VideoWriter_fourcc(*codec)\n        video_writer = cv2.VideoWriter(output_video, fourcc, fps, video_size)\n        if video_writer.isOpened():\n            break\n        video_writer.release()\n\n    if not video_writer or not video_writer.isOpened():\n        raise RuntimeError(\n            f\"Failed to initialize video writer for {output_video}. \"\n            \"Please ensure you have the necessary codecs installed.\"\n        )\n\n    for image_path in images:\n        frame = cv2.imread(image_path)\n        if frame is None:\n            print(f\"Warning: Could not read image {image_path}. Skipping.\")\n            continue\n\n        if video_size != (frame.shape[1], frame.shape[0]):\n            frame = cv2.resize(frame, video_size)\n\n        video_writer.write(frame)\n\n    video_writer.release()\n\n    # Verify the video file was created\n    if not os.path.exists(output_video):\n        raise RuntimeError(\n            f\"Failed to create video file {output_video}. \"\n            \"The video writer completed but the file was not saved to disk.\"\n        )\n\n    # Verify the file has content\n    if os.path.getsize(output_video) == 0:\n        raise RuntimeError(\n            f\"Video file {output_video} was created but is empty. \"\n            \"This may indicate a codec problem.\"\n        )\n\n    print(f\"Video saved as {output_video}\")\n</code></pre>"},{"location":"common/#samgeo.common.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>samgeo/common.py</code> <pre><code>def install_package(package):\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n\n    for package in packages:\n        if package.startswith(\"https://github.com\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"common/#samgeo.common.is_colab","title":"<code>is_colab()</code>","text":"<p>Tests if the code is being executed within Google Colab.</p> Source code in <code>samgeo/common.py</code> <pre><code>def is_colab():\n    \"\"\"Tests if the code is being executed within Google Colab.\"\"\"\n    import sys\n\n    return \"google.colab\" in sys.modules\n</code></pre>"},{"location":"common/#samgeo.common.make_temp_dir","title":"<code>make_temp_dir(**kwargs)</code>","text":"<p>Create a temporary directory and return the path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the temporary directory.</p> Source code in <code>samgeo/common.py</code> <pre><code>def make_temp_dir(**kwargs) -&gt; str:\n    \"\"\"Create a temporary directory and return the path.\n\n    Returns:\n        str: The path to the temporary directory.\n    \"\"\"\n    import tempfile\n\n    temp_dir = tempfile.mkdtemp(**kwargs)\n    return temp_dir\n</code></pre>"},{"location":"common/#samgeo.common.merge_rasters","title":"<code>merge_rasters(input_dir, output, input_pattern='*.tif', output_format='GTiff', output_nodata=None, output_options=['COMPRESS=DEFLATE'])</code>","text":"<p>Merge a directory of rasters into a single raster.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>The path to the input directory.</p> required <code>output</code> <code>str</code> <p>The path to the output raster.</p> required <code>input_pattern</code> <code>str</code> <p>The pattern to match the input files. Defaults to \"*.tif\".</p> <code>'*.tif'</code> <code>output_format</code> <code>str</code> <p>The output format. Defaults to \"GTiff\".</p> <code>'GTiff'</code> <code>output_nodata</code> <code>float</code> <p>The output nodata value. Defaults to None.</p> <code>None</code> <code>output_options</code> <code>list</code> <p>A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].</p> <code>['COMPRESS=DEFLATE']</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def merge_rasters(\n    input_dir,\n    output,\n    input_pattern=\"*.tif\",\n    output_format=\"GTiff\",\n    output_nodata=None,\n    output_options=[\"COMPRESS=DEFLATE\"],\n):\n    \"\"\"Merge a directory of rasters into a single raster.\n\n    Args:\n        input_dir (str): The path to the input directory.\n        output (str): The path to the output raster.\n        input_pattern (str, optional): The pattern to match the input files. Defaults to \"*.tif\".\n        output_format (str, optional): The output format. Defaults to \"GTiff\".\n        output_nodata (float, optional): The output nodata value. Defaults to None.\n        output_options (list, optional): A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    import glob\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n\n    # Get a list of all the input files\n    input_files = glob.glob(os.path.join(input_dir, input_pattern))\n\n    # Prepare the gdal.Warp options\n    warp_options = gdal.WarpOptions(\n        format=output_format, dstNodata=output_nodata, creationOptions=output_options\n    )\n\n    # Merge the input files into a single output file\n    gdal.Warp(\n        destNameOrDestDS=output,\n        srcDSOrSrcDSTab=input_files,\n        options=warp_options,\n    )\n</code></pre>"},{"location":"common/#samgeo.common.overlay_images","title":"<code>overlay_images(image1, image2, alpha=0.5, backend='TkAgg', height_ratios=[10, 1], show_args1={}, show_args2={})</code>","text":"<p>Overlays two images using a slider to control the opacity of the top image.</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>str | ndarray</code> <p>The first input image at the bottom represented as a NumPy array or the path to the image.</p> required <code>image2</code> <code>_type_</code> <p>The second input image on top represented as a NumPy array or the path to the image.</p> required <code>alpha</code> <code>float</code> <p>The alpha value of the top image. Defaults to 0.5.</p> <code>0.5</code> <code>backend</code> <code>str</code> <p>The backend of the matplotlib plot. Defaults to \"TkAgg\".</p> <code>'TkAgg'</code> <code>height_ratios</code> <code>list</code> <p>The height ratios of the two subplots. Defaults to [10, 1].</p> <code>[10, 1]</code> <code>show_args1</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.</p> <code>{}</code> <code>show_args2</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def overlay_images(\n    image1,\n    image2,\n    alpha=0.5,\n    backend=\"TkAgg\",\n    height_ratios=[10, 1],\n    show_args1={},\n    show_args2={},\n):\n    \"\"\"Overlays two images using a slider to control the opacity of the top image.\n\n    Args:\n        image1 (str | np.ndarray): The first input image at the bottom represented as a NumPy array or the path to the image.\n        image2 (_type_): The second input image on top represented as a NumPy array or the path to the image.\n        alpha (float, optional): The alpha value of the top image. Defaults to 0.5.\n        backend (str, optional): The backend of the matplotlib plot. Defaults to \"TkAgg\".\n        height_ratios (list, optional): The height ratios of the two subplots. Defaults to [10, 1].\n        show_args1 (dict, optional): The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.\n        show_args2 (dict, optional): The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.\n\n    \"\"\"\n    import sys\n\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import matplotlib.widgets as mpwidgets\n\n    if \"google.colab\" in sys.modules:\n        backend = \"inline\"\n        print(\n            \"The TkAgg backend is not supported in Google Colab. The overlay_images function will not work on Colab.\"\n        )\n        return\n\n    matplotlib.use(backend)\n\n    if isinstance(image1, str):\n        if image1.startswith(\"http\"):\n            image1 = download_file(image1)\n\n        if not os.path.exists(image1):\n            raise ValueError(f\"Input path {image1} does not exist.\")\n\n    if isinstance(image2, str):\n        if image2.startswith(\"http\"):\n            image2 = download_file(image2)\n\n        if not os.path.exists(image2):\n            raise ValueError(f\"Input path {image2} does not exist.\")\n\n    # Load the two images\n    x = plt.imread(image1)\n    y = plt.imread(image2)\n\n    # Create the plot\n    fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": height_ratios})\n    img0 = ax0.imshow(x, **show_args1)\n    img1 = ax0.imshow(y, alpha=alpha, **show_args2)\n\n    # Define the update function\n    def update(value):\n        img1.set_alpha(value)\n        fig.canvas.draw_idle()\n\n    # Create the slider\n    slider0 = mpwidgets.Slider(ax=ax1, label=\"alpha\", valmin=0, valmax=1, valinit=alpha)\n    slider0.on_changed(update)\n\n    # Display the plot\n    plt.show()\n</code></pre>"},{"location":"common/#samgeo.common.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>samgeo/common.py</code> <pre><code>def print_raster_info(\n    raster_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from rasterio.plot import show\n\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n        return info\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n        return None\n</code></pre>"},{"location":"common/#samgeo.common.random_string","title":"<code>random_string(string_length=6)</code>","text":"<p>Generates a random string of fixed length.</p> <p>Parameters:</p> Name Type Description Default <code>string_length</code> <code>int</code> <p>Fixed length. Defaults to 3.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A random string</p> Source code in <code>samgeo/common.py</code> <pre><code>def random_string(string_length=6):\n    \"\"\"Generates a random string of fixed length.\n\n    Args:\n        string_length (int, optional): Fixed length. Defaults to 3.\n\n    Returns:\n        str: A random string\n    \"\"\"\n    import random\n    import string\n\n    # random.seed(1001)\n    letters = string.ascii_lowercase\n    return \"\".join(random.choice(letters) for i in range(string_length))\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_geojson","title":"<code>raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".geojson\"):\n        output += \".geojson\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_gpkg","title":"<code>raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".gpkg\"):\n        output += \".gpkg\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_shp","title":"<code>raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".shp\"):\n        output += \".shp\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_vector","title":"<code>raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs)</code>","text":"<p>Vectorize a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs):\n    \"\"\"Vectorize a raster dataset.\n\n    Args:\n        source (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n    from rasterio import features\n\n    with rasterio.open(source) as src:\n        band = src.read()\n\n        mask = band != 0\n        shapes = features.shapes(band, mask=mask, transform=src.transform)\n\n    fc = [\n        {\"geometry\": shapely.geometry.shape(shape), \"properties\": {\"value\": value}}\n        for shape, value in shapes\n    ]\n    if simplify_tolerance is not None:\n        for i in fc:\n            i[\"geometry\"] = i[\"geometry\"].simplify(tolerance=simplify_tolerance)\n\n    gdf = gpd.GeoDataFrame.from_features(fc)\n    if src.crs is not None:\n        gdf.set_crs(crs=src.crs, inplace=True)\n\n    if dst_crs is not None:\n        gdf = gdf.to_crs(dst_crs)\n\n    gdf.to_file(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import pandas as pd\n    import rioxarray as rxr\n    import scipy.ndimage as ndi\n    import xarray as xr\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"equivalent_diameter_area\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n            if out_vector is not None:\n                tmp_vector = temp_file_path(\".gpkg\")\n                raster_to_vector(out_image, tmp_vector)\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n        return da, df\n</code></pre>"},{"location":"common/#samgeo.common.regularize","title":"<code>regularize(data, output_path=None, parallel_threshold=1.0, target_crs=None, simplify=True, simplify_tolerance=0.5, allow_45_degree=True, diagonal_threshold_reduction=15, allow_circles=True, circle_threshold=0.9, num_cores=1, include_metadata=False, **kwargs)</code>","text":"<p>Regularizes polygon geometries in a GeoDataFrame by aligning edges.</p> <p>Aligns edges to be parallel or perpendicular (optionally also 45 degrees) to their main direction. Handles reprojection, initial simplification, regularization, geometry cleanup, and parallel processing.</p> <p>This function is a wrapper around the <code>regularize_geodataframe</code> function from the <code>buildingregulariser</code> package. Credits to the original author Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[GeoDataFrame, str]</code> <p>Input GeoDataFrame with polygon or multipolygon geometries, or a file path to the GeoDataFrame.</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoDataFrame. If None, the output is not saved. Defaults to None.</p> <code>None</code> <code>parallel_threshold</code> <code>float</code> <p>Distance threshold for merging nearly parallel adjacent edges during regularization. Defaults to 1.0.</p> <code>1.0</code> <code>target_crs</code> <code>Optional[Union[str, CRS]]</code> <p>Target Coordinate Reference System for processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a projected CRS. Defaults to None.</p> <code>None</code> <code>simplify</code> <code>bool</code> <p>If True, applies initial simplification to the geometry before regularization. Defaults to True.</p> <code>True</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for the initial simplification step (if <code>simplify</code> is True). Also used for geometry cleanup steps. Defaults to 0.5.</p> <code>0.5</code> <code>allow_45_degree</code> <code>bool</code> <p>If True, allows edges to be oriented at 45-degree angles relative to the main direction during regularization. Defaults to True.</p> <code>True</code> <code>diagonal_threshold_reduction</code> <code>float</code> <p>Reduction factor in degrees to reduce the likelihood of diagonal edges being created. Larger values reduce the likelihood of diagonal edges. Defaults to 15.</p> <code>15</code> <code>allow_circles</code> <code>bool</code> <p>If True, attempts to detect polygons that are nearly circular and replaces them with perfect circles. Defaults to True.</p> <code>True</code> <code>circle_threshold</code> <code>float</code> <p>Intersection over Union (IoU) threshold used for circle detection (if <code>allow_circles</code> is True). Value between 0 and 1. Defaults to 0.9.</p> <code>0.9</code> <code>num_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. If 1, processing is done sequentially. Defaults to 1.</p> <code>1</code> <code>include_metadata</code> <code>bool</code> <p>If True, includes metadata about the regularization process in the output GeoDataFrame. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>to_file</code> method when saving the output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are</p> <code>Any</code> <p>preserved. Geometries that failed processing might be dropped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.</p> Source code in <code>samgeo/common.py</code> <pre><code>def regularize(\n    data: Union[gpd.GeoDataFrame, str],\n    output_path: Optional[str] = None,\n    parallel_threshold: float = 1.0,\n    target_crs: Optional[Union[str, \"pyproj.CRS\"]] = None,\n    simplify: bool = True,\n    simplify_tolerance: float = 0.5,\n    allow_45_degree: bool = True,\n    diagonal_threshold_reduction: float = 15,\n    allow_circles: bool = True,\n    circle_threshold: float = 0.9,\n    num_cores: int = 1,\n    include_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Regularizes polygon geometries in a GeoDataFrame by aligning edges.\n\n    Aligns edges to be parallel or perpendicular (optionally also 45 degrees)\n    to their main direction. Handles reprojection, initial simplification,\n    regularization, geometry cleanup, and parallel processing.\n\n    This function is a wrapper around the `regularize_geodataframe` function\n    from the `buildingregulariser` package. Credits to the original author\n    Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.\n\n    Args:\n        data (Union[gpd.GeoDataFrame, str]): Input GeoDataFrame with polygon or multipolygon geometries,\n            or a file path to the GeoDataFrame.\n        output_path (Optional[str], optional): Path to save the output GeoDataFrame. If None, the output is\n            not saved. Defaults to None.\n        parallel_threshold (float, optional): Distance threshold for merging nearly parallel adjacent edges\n            during regularization. Defaults to 1.0.\n        target_crs (Optional[Union[str, \"pyproj.CRS\"]], optional): Target Coordinate Reference System for\n            processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a\n            projected CRS. Defaults to None.\n        simplify (bool, optional): If True, applies initial simplification to the geometry before\n            regularization. Defaults to True.\n        simplify_tolerance (float, optional): Tolerance for the initial simplification step (if `simplify`\n            is True). Also used for geometry cleanup steps. Defaults to 0.5.\n        allow_45_degree (bool, optional): If True, allows edges to be oriented at 45-degree angles relative\n            to the main direction during regularization. Defaults to True.\n        diagonal_threshold_reduction (float, optional): Reduction factor in degrees to reduce the likelihood\n            of diagonal edges being created. Larger values reduce the likelihood of diagonal edges.\n            Defaults to 15.\n        allow_circles (bool, optional): If True, attempts to detect polygons that are nearly circular and\n            replaces them with perfect circles. Defaults to True.\n        circle_threshold (float, optional): Intersection over Union (IoU) threshold used for circle detection\n            (if `allow_circles` is True). Value between 0 and 1. Defaults to 0.9.\n        num_cores (int, optional): Number of CPU cores to use for parallel processing. If 1, processing is\n            done sequentially. Defaults to 1.\n        include_metadata (bool, optional): If True, includes metadata about the regularization process in the\n            output GeoDataFrame. Defaults to False.\n\n        **kwargs: Additional keyword arguments to pass to the `to_file` method when saving the output.\n\n    Returns:\n        gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are\n        preserved. Geometries that failed processing might be dropped.\n\n    Raises:\n        ValueError: If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.\n    \"\"\"\n    try:\n        from buildingregulariser import regularize_geodataframe\n    except ImportError:\n        install_package(\"buildingregulariser\")\n        from buildingregulariser import regularize_geodataframe\n\n    if isinstance(data, str):\n        data = gpd.read_file(data)\n    elif not isinstance(data, gpd.GeoDataFrame):\n        raise ValueError(\"Input data must be a GeoDataFrame or a file path.\")\n\n    # Check if the input data is empty\n    if data.empty:\n        raise ValueError(\"Input GeoDataFrame is empty.\")\n\n    gdf = regularize_geodataframe(\n        data,\n        parallel_threshold=parallel_threshold,\n        target_crs=target_crs,\n        simplify=simplify,\n        simplify_tolerance=simplify_tolerance,\n        allow_45_degree=allow_45_degree,\n        diagonal_threshold_reduction=diagonal_threshold_reduction,\n        allow_circles=allow_circles,\n        circle_threshold=circle_threshold,\n        num_cores=num_cores,\n        include_metadata=include_metadata,\n    )\n\n    if output_path:\n        gdf.to_file(output_path, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"common/#samgeo.common.regularize_legacy","title":"<code>regularize_legacy(source, output=None, crs='EPSG:4326', **kwargs)</code>","text":"<p>Regularize a polygon GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | GeoDataFrame</code> <p>The input file path or a GeoDataFrame.</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>gpd.GeoDataFrame: The output GeoDataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def regularize_legacy(source, output=None, crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Regularize a polygon GeoDataFrame.\n\n    Args:\n        source (str | gpd.GeoDataFrame): The input file path or a GeoDataFrame.\n        output (str, optional): The output file path. Defaults to None.\n\n\n    Returns:\n        gpd.GeoDataFrame: The output GeoDataFrame.\n    \"\"\"\n    if isinstance(source, str):\n        gdf = gpd.read_file(source)\n    elif isinstance(source, gpd.GeoDataFrame):\n        gdf = source\n    else:\n        raise ValueError(\"The input source must be a GeoDataFrame or a file path.\")\n\n    polygons = gdf.geometry.apply(lambda geom: geom.minimum_rotated_rectangle)\n    result = gpd.GeoDataFrame(geometry=polygons, data=gdf.drop(\"geometry\", axis=1))\n\n    if crs is not None:\n        result.to_crs(crs, inplace=True)\n    if output is not None:\n        result.to_file(output, **kwargs)\n    else:\n        return result\n</code></pre>"},{"location":"common/#samgeo.common.reproject","title":"<code>reproject(image, output, dst_crs='EPSG:4326', resampling='nearest', to_cog=True, **kwargs)</code>","text":"<p>Reprojects an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The input image filepath.</p> required <code>output</code> <code>str</code> <p>The output image filepath.</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>resampling</code> <code>Resampling</code> <p>The resampling method. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>to_cog</code> <code>bool</code> <p>Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.open.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def reproject(\n    image, output, dst_crs=\"EPSG:4326\", resampling=\"nearest\", to_cog=True, **kwargs\n):\n    \"\"\"Reprojects an image.\n\n    Args:\n        image (str): The input image filepath.\n        output (str): The output image filepath.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        resampling (Resampling, optional): The resampling method. Defaults to \"nearest\".\n        to_cog (bool, optional): Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rasterio.open.\n\n    \"\"\"\n    import rasterio as rio\n    import shutil\n    from rasterio.warp import Resampling, calculate_default_transform, reproject\n\n    if isinstance(resampling, str):\n        resampling = getattr(Resampling, resampling)\n\n    image = os.path.abspath(image)\n    output = os.path.abspath(output)\n\n    if not os.path.exists(os.path.dirname(output)):\n        os.makedirs(os.path.dirname(output))\n\n    # Check if input and output are the same file (in-place reprojection)\n    in_place = os.path.normpath(image) == os.path.normpath(output)\n\n    # If in-place, use a temporary file to avoid file locking issues on Windows\n    if in_place:\n        temp_output = temp_file_path(\".tif\")\n    else:\n        temp_output = output\n\n    with rio.open(image, **kwargs) as src:\n        transform, width, height = calculate_default_transform(\n            src.crs, dst_crs, src.width, src.height, *src.bounds\n        )\n        kwargs = src.meta.copy()\n        kwargs.update(\n            {\n                \"crs\": dst_crs,\n                \"transform\": transform,\n                \"width\": width,\n                \"height\": height,\n            }\n        )\n\n        with rio.open(temp_output, \"w\", **kwargs) as dst:\n            for i in range(1, src.count + 1):\n                reproject(\n                    source=rio.band(src, i),\n                    destination=rio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=dst_crs,\n                    resampling=resampling,\n                    **kwargs,\n                )\n\n    # If in-place, replace the original file with the temporary file\n    if in_place:\n        shutil.move(temp_output, output)\n\n    if to_cog:\n        image_to_cog(output, output)\n</code></pre>"},{"location":"common/#samgeo.common.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A list of (x, y) coordinates.</p> Source code in <code>samgeo/common.py</code> <pre><code>def rowcol_to_xy(\n    src_fp,\n    rows=None,\n    cols=None,\n    boxes=None,\n    zs=None,\n    offset=\"center\",\n    output=None,\n    dst_crs=\"EPSG:4326\",\n    **kwargs,\n):\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"common/#samgeo.common.sam_map_gui","title":"<code>sam_map_gui(sam, basemap='SATELLITE', repeat_mode=True, out_dir=None, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> <p>The SamGeo, SamGeo2, or SamGeo3 object.</p> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for the draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def sam_map_gui(\n    sam,\n    basemap=\"SATELLITE\",\n    repeat_mode=True,\n    out_dir=None,\n    min_size=0,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo): The SamGeo, SamGeo2, or SamGeo3 object.\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        repeat_mode (bool, optional): Whether to use the repeat mode for the draw control. Defaults to True.\n        out_dir (str, optional): The output directory. Defaults to None.\n        min_size (int, optional): Minimum mask size in pixels. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n\n        import ipyevents\n        import ipyleaflet\n        import ipywidgets as widgets\n        import leafmap\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first or install the full samgeo package with: pip install segment-geospatial[all].\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(repeat_mode=repeat_mode, **kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    if basemap is not None:\n        m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except Exception:\n        pass\n\n    m.fg_markers = []\n    m.bg_markers = []\n\n    fg_layer = ipyleaflet.LayerGroup(layers=m.fg_markers, name=\"Foreground\")\n    bg_layer = ipyleaflet.LayerGroup(layers=m.bg_markers, name=\"Background\")\n    m.add(fg_layer)\n    m.add(bg_layer)\n    m.fg_layer = fg_layer\n    m.bg_layer = bg_layer\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 0px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=padding),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    plus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load foreground points\",\n        icon=\"plus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    minus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load background points\",\n        icon=\"minus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    radio_buttons = widgets.RadioButtons(\n        options=[\"Foreground\", \"Background\"],\n        description=\"Class Type:\",\n        disabled=False,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    def on_radio_button_click(change):\n        # Set a flag to indicate UI interaction is happening\n        m._ui_interaction = True\n        # Use a small delay to reset the flag after the event is processed\n        import threading\n\n        threading.Timer(0.1, lambda: setattr(m, \"_ui_interaction\", False)).start()\n\n    radio_buttons.observe(on_radio_button_click, \"value\")\n\n    fg_count = widgets.IntText(\n        value=0,\n        description=\"Foreground #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n    bg_count = widgets.IntText(\n        value=0,\n        description=\"Background #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Mask opacity:\",\n        min=0,\n        max=1,\n        value=0.7,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    buttons = widgets.VBox(\n        [\n            radio_buttons,\n            widgets.HBox([fg_count, bg_count]),\n            opacity_slider,\n            widgets.HBox([rectangular, colorpicker]),\n            widgets.HBox(\n                [segment_button, save_button, reset_button],\n                layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n            ),\n        ]\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            mask_layer = m.find_layer(\"Masks\")\n            if mask_layer is not None:\n                mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, plus_button, minus_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        buttons,\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def marker_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    gdf = gpd.read_file(chooser.selected)\n                    centroids = gdf.centroid\n                    coords = [[point.x, point.y] for point in centroids]\n                    for coord in coords:\n                        if plus_button.value:\n                            if is_colab():  # Colab does not support AwesomeIcon\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"green\",\n                                    fill_color=\"green\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"plus-circle\",\n                                        marker_color=\"green\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.fg_layer.add(marker)\n                            m.fg_markers.append(marker)\n                            fg_count.value = len(m.fg_markers)\n                        elif minus_button.value:\n                            if is_colab():\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"red\",\n                                    fill_color=\"red\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"minus-circle\",\n                                        marker_color=\"red\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.bg_layer.add(marker)\n                            m.bg_markers.append(marker)\n                            bg_count.value = len(m.bg_markers)\n\n                except Exception as e:\n                    print(e)\n\n            if m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                delattr(m, \"marker_control\")\n\n            plus_button.value = False\n            minus_button.value = False\n\n    def marker_button_click(change):\n        if change[\"new\"]:\n            sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n            filechooser = FileChooser(\n                path=os.getcwd(),\n                sandbox_path=sandbox_path,\n                layout=widgets.Layout(width=\"454px\"),\n            )\n            filechooser.use_dir_icons = True\n            filechooser.filter_pattern = [\"*.shp\", \"*.geojson\", \"*.gpkg\"]\n            filechooser.register_callback(marker_callback)\n            marker_control = ipyleaflet.WidgetControl(\n                widget=filechooser, position=\"topright\"\n            )\n            m.add_control(marker_control)\n            m.marker_control = marker_control\n        else:\n            if hasattr(m, \"marker_control\") and m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                m.marker_control.close()\n\n    plus_button.observe(marker_button_click, \"value\")\n    minus_button.observe(marker_button_click, \"value\")\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def handle_map_interaction(**kwargs):\n        try:\n            if kwargs.get(\"type\") == \"click\":\n                # Skip if we're interacting with UI\n                if hasattr(m, \"_ui_interaction\") and m._ui_interaction:\n                    return\n                latlon = kwargs.get(\"coordinates\")\n                if radio_buttons.value == \"Foreground\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"green\",\n                            fill_color=\"green\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"plus-circle\",\n                                marker_color=\"green\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    fg_layer.add(marker)\n                    m.fg_markers.append(marker)\n                    fg_count.value = len(m.fg_markers)\n                elif radio_buttons.value == \"Background\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"red\",\n                            fill_color=\"red\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"minus-circle\",\n                                marker_color=\"red\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    bg_layer.add(marker)\n                    m.bg_markers.append(marker)\n                    bg_count.value = len(m.bg_markers)\n\n        except (TypeError, KeyError) as e:\n            print(f\"Error handling map interaction: {e}\")\n\n    m.on_interaction(handle_map_interaction)\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(m.fg_markers) == 0:\n                    print(\"Please add some foreground markers.\")\n                    segment_button.value = False\n                    return\n\n                else:\n                    try:\n                        fg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.fg_markers\n                        ]\n                        bg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.bg_markers\n                        ]\n                        point_coords = fg_points + bg_points\n                        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n\n                        filename = f\"masks_{random_string()}.tif\"\n                        filename = os.path.join(out_dir, filename)\n                        if sam.model_version == \"sam\":\n                            sam.predict(\n                                point_coords=point_coords,\n                                point_labels=point_labels,\n                                point_crs=\"EPSG:4326\",\n                                output=filename,\n                            )\n                        elif sam.model_version == \"sam2\":\n                            sam.predict_by_points(\n                                point_coords_batch=point_coords,\n                                point_labels_batch=point_labels,\n                                point_crs=\"EPSG:4326\",\n                                output=filename,\n                            )\n                        elif sam.model_version == \"sam3\":\n                            sam.generate_masks_by_points_patch(\n                                point_coords_batch=point_coords,\n                                point_labels_batch=point_labels,\n                                point_crs=\"EPSG:4326\",\n                                min_size=min_size,\n                                max_size=max_size,\n                            )\n                            sam.save_masks(\n                                output=filename, min_size=min_size, max_size=max_size\n                            )\n                        if m.find_layer(\"Masks\") is not None:\n                            m.remove_layer(m.find_layer(\"Masks\"))\n                        if m.find_layer(\"Regularized\") is not None:\n                            m.remove_layer(m.find_layer(\"Regularized\"))\n\n                        if hasattr(sam, \"prediction_fp\") and os.path.exists(\n                            sam.prediction_fp\n                        ):\n                            try:\n                                os.remove(sam.prediction_fp)\n                            except Exception:\n                                pass\n                        # Skip the image layer if localtileserver is not available\n                        try:\n                            m.add_raster(\n                                filename,\n                                nodata=0,\n                                cmap=\"tab20\",\n                                opacity=opacity_slider.value,\n                                layer_name=\"Masks\",\n                                zoom_to_layer=False,\n                            )\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=\"Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                        except Exception:\n                            pass\n                        output.clear_output()\n                        segment_button.value = False\n                        sam.prediction_fp = filename\n                    except Exception as e:\n                        segment_button.value = False\n                        print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.prediction_fp, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n\n                    fg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.fg_markers\n                    ]\n                    bg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.bg_markers\n                    ]\n\n                    coords_to_geojson(\n                        fg_points, filename.replace(\".tif\", \"_fg_markers.geojson\")\n                    )\n                    coords_to_geojson(\n                        bg_points, filename.replace(\".tif\", \"_bg_markers.geojson\")\n                    )\n\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                filechooser = FileChooser(\n                    path=os.getcwd(),\n                    filename=\"masks.tif\",\n                    sandbox_path=sandbox_path,\n                    layout=widgets.Layout(width=\"454px\"),\n                )\n                filechooser.use_dir_icons = True\n                filechooser.filter_pattern = [\"*.tif\"]\n                filechooser.register_callback(filechooser_callback)\n                save_control = ipyleaflet.WidgetControl(\n                    widget=filechooser, position=\"topright\"\n                )\n                m.add_control(save_control)\n                m.save_control = save_control\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.7\n            rectangular.value = False\n            colorpicker.value = \"#ffff00\"\n            output.clear_output()\n            try:\n                m.remove_layer(m.find_layer(\"Masks\"))\n                if m.find_layer(\"Regularized\") is not None:\n                    m.remove_layer(m.find_layer(\"Regularized\"))\n                m.clear_drawings()\n                if hasattr(m, \"fg_markers\"):\n                    m.user_rois = None\n                    m.fg_markers = []\n                    m.bg_markers = []\n                    m.fg_layer.clear_layers()\n                    m.bg_layer.clear_layers()\n                    fg_count.value = 0\n                    bg_count.value = 0\n                try:\n                    os.remove(sam.prediction_fp)\n                except Exception:\n                    pass\n            except Exception:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.show_canvas","title":"<code>show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/common.py</code> <pre><code>def show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        image = cv2.imread(image)\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be a URL or a NumPy array.\")\n\n    # Create an empty list to store the mouse click coordinates\n    left_clicks = []\n    right_clicks = []\n\n    # Create a mouse callback function\n    def get_mouse_coordinates(event, x, y):\n        if event == cv2.EVENT_LBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            left_clicks.append((x, y))\n\n            # Draw a green circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, fg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n        elif event == cv2.EVENT_RBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            right_clicks.append((x, y))\n\n            # Draw a red circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, bg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n    # Create a window to display the image\n    cv2.namedWindow(\"Image\")\n\n    # Set the mouse callback function for the window\n    cv2.setMouseCallback(\"Image\", get_mouse_coordinates)\n\n    # Display the image in the window\n    cv2.imshow(\"Image\", image)\n\n    # Wait for a key press to exit\n    cv2.waitKey(0)\n\n    # Destroy the window\n    cv2.destroyAllWindows()\n\n    return left_clicks, right_clicks\n</code></pre>"},{"location":"common/#samgeo.common.show_image_gui","title":"<code>show_image_gui(path)</code>","text":"<p>Show an interactive GUI to explore images. Args:     path (str): The path to the image file or directory containing images.</p> Source code in <code>samgeo/common.py</code> <pre><code>def show_image_gui(path: str) -&gt; None:\n    \"\"\"Show an interactive GUI to explore images.\n    Args:\n        path (str): The path to the image file or directory containing images.\n    \"\"\"\n\n    from PIL import Image\n\n    try:\n        import matplotlib\n        from ipywidgets import IntSlider, interact\n    except ImportError as e:\n        print(\n            f\"There was an error importing {e.name}, which is a dependency of leafmap. Install it with pip install leafmap or pip install segment-geospatial[all]\"\n        )\n\n    def setup_interactive_matplotlib():\n        \"\"\"Sets up ipympl backend for interactive plotting in Jupyter.\"\"\"\n        # Use the ipympl backend for interactive plotting\n        try:\n            import ipympl  # noqa: F401\n\n            matplotlib.use(\"module://ipympl.backend_nbagg\")\n        except ImportError:\n            print(\"ipympl is not installed. Falling back to default backend.\")\n\n    def load_images_from_folder(folder):\n        \"\"\"Load all images from the specified folder.\"\"\"\n        images = []\n        filenames = []\n        for filename in sorted(os.listdir(folder)):\n            if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n                img = Image.open(os.path.join(folder, filename))\n                img_array = np.array(img)\n                images.append(img_array)\n                filenames.append(filename)\n        return images, filenames\n\n    def load_single_image(image_path):\n        \"\"\"Load a single image from the specified image file path.\"\"\"\n        img = Image.open(image_path)\n        img_array = np.array(img)\n        return [img_array], [\n            os.path.basename(image_path)\n        ]  # Return as lists for consistency\n\n    # Check if the input path is a file or a directory\n    if os.path.isfile(path):\n        images, filenames = load_single_image(path)\n    elif os.path.isdir(path):\n        images, filenames = load_images_from_folder(path)\n    else:\n        print(\"Invalid path. Please provide a valid image file or directory.\")\n        return\n\n    total_images = len(images)\n\n    if total_images == 0:\n        print(\"No images found.\")\n        return\n\n    # Set up interactive plotting\n    import matplotlib.pyplot as plt\n\n    setup_interactive_matplotlib()\n\n    fig, ax = plt.subplots()\n    fig.canvas.toolbar_visible = True\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = True\n\n    # Display the first image initially\n    im_display = ax.imshow(images[0])\n    ax.set_title(f\"Image: {filenames[0]}\")\n    plt.tight_layout()\n\n    # Function to update the image when the slider changes (for multiple images)\n    def update_image(image_index):\n        im_display.set_data(images[image_index])\n        ax.set_title(f\"Image: {filenames[image_index]}\")\n        fig.canvas.draw()\n\n    # Function to show pixel information on click\n    def onclick(event):\n        if event.xdata is not None and event.ydata is not None:\n            col = int(event.xdata)\n            row = int(event.ydata)\n            pixel_value = images[current_image_index][\n                row, col\n            ]  # Use current image index\n            ax.set_title(\n                f\"Image: {filenames[current_image_index]} - X: {col}, Y: {row}, Pixel Value: {pixel_value}\"\n            )\n            fig.canvas.draw()\n\n    # Track the current image index (whether from slider or for single image)\n    current_image_index = 0\n\n    # Slider widget to choose between images (only if there is more than one image)\n    if total_images &gt; 1:\n        slider = IntSlider(min=0, max=total_images - 1, step=1, description=\"Image\")\n\n        def on_slider_change(change):\n            nonlocal current_image_index\n            current_image_index = change[\"new\"]  # Update current image index\n            update_image(current_image_index)\n\n        slider.observe(on_slider_change, names=\"value\")\n        fig.canvas.mpl_connect(\"button_press_event\", onclick)\n        interact(update_image, image_index=slider)\n    else:\n        # If there's only one image, no need for a slider, just show pixel info on click\n        fig.canvas.mpl_connect(\"button_press_event\", onclick)\n\n    # Show the plot\n    plt.show()\n</code></pre>"},{"location":"common/#samgeo.common.simplify","title":"<code>simplify(source, tolerance=0.01, output=None, **kwargs)</code>","text":"<p>Simplify a polygon GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | GeoDataFrame</code> <p>The input file path or a GeoDataFrame.</p> required <code>tolerance</code> <code>float</code> <p>The tolerance value for simplification. Defaults to 0.01.</p> <code>0.01</code> <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>gpd.GeoDataFrame: The output GeoDataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def simplify(source, tolerance=0.01, output=None, **kwargs):\n    \"\"\"Simplify a polygon GeoDataFrame.\n\n    Args:\n        source (str | gpd.GeoDataFrame): The input file path or a GeoDataFrame.\n        tolerance (float, optional): The tolerance value for simplification. Defaults to 0.01.\n        output (str, optional): The output file path. Defaults to None.\n\n    Returns:\n        gpd.GeoDataFrame: The output GeoDataFrame.\n    \"\"\"\n    if isinstance(source, str):\n        gdf = gpd.read_file(source)\n    elif isinstance(source, gpd.GeoDataFrame):\n        gdf = source\n    else:\n        raise ValueError(\"The input source must be a GeoDataFrame or a file path.\")\n\n    polygons = gdf.geometry.apply(\n        lambda geom: geom.simplify(tolerance, preserve_topology=True, **kwargs)\n    )\n    result = gpd.GeoDataFrame(geometry=polygons, data=gdf.drop(\"geometry\", axis=1))\n\n    if output is not None:\n        result.to_file(output)\n    else:\n        return result\n</code></pre>"},{"location":"common/#samgeo.common.smooth_vector","title":"<code>smooth_vector(vector_data, output_path=None, segment_length=None, smooth_iterations=3, num_cores=0, merge_collection=True, merge_field=None, merge_multipolygons=True, preserve_area=True, area_tolerance=0.01, **kwargs)</code>","text":"<p>Smooth a vector data using the smoothify library. See https://github.com/DPIRD-DMA/Smoothify for more details.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>Union[str, GeoDataFrame]</code> <p>The vector data to smooth.</p> required <code>output_path</code> <code>str</code> <p>The path to save the smoothed vector data. If None, returns the smoothed vector data.</p> <code>None</code> <code>segment_length</code> <code>float</code> <p>Resolution of the original raster data in map units. If None (default), automatically detects by finding the minimum segment length (from a data sample). Recommended to specify explicitly when known.</p> <code>None</code> <code>smooth_iterations</code> <code>int</code> <p>The number of iterations to smooth the vector data.</p> <code>3</code> <code>num_cores</code> <code>int</code> <p>Number of cores to use for parallel processing. If 0 (default), uses all available cores.</p> <code>0</code> <code>merge_collection</code> <code>bool</code> <p>Whether to merge/dissolve adjacent geometries in collections before smoothing.</p> <code>True</code> <code>merge_field</code> <code>str</code> <p>Column name to use for dissolving geometries. Only valid when merge_collection=True. If None, dissolves all geometries together. If specified, dissolves geometries grouped by the column values.</p> <code>None</code> <code>merge_multipolygons</code> <code>bool</code> <p>Whether to merge adjacent polygons within MultiPolygons before smoothing</p> <code>True</code> <code>preserve_area</code> <code>bool</code> <p>Whether to restore original area after smoothing via buffering (applies to Polygons only)</p> <code>True</code> <code>area_tolerance</code> <code>float</code> <p>Percentage of original area allowed as error (e.g., 0.01 = 0.01% error = 99.99% preservation). Only affects Polygons when preserve_area=True</p> <code>0.01</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The smoothed vector data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from samgeo import common\n&gt;&gt;&gt; gdf = common.read_vector(\"path/to/vector.geojson\")\n&gt;&gt;&gt; smoothed_gdf = common.smooth_vector(gdf, smooth_iterations=3, output_path=\"path/to/smoothed_vector.geojson\")\n&gt;&gt;&gt; smoothed_gdf.head()\n&gt;&gt;&gt; smoothed_gdf.explore()\n</code></pre> Source code in <code>samgeo/common.py</code> <pre><code>def smooth_vector(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    output_path: str = None,\n    segment_length: float = None,\n    smooth_iterations: int = 3,\n    num_cores: int = 0,\n    merge_collection: bool = True,\n    merge_field: str = None,\n    merge_multipolygons: bool = True,\n    preserve_area: bool = True,\n    area_tolerance: float = 0.01,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Smooth a vector data using the smoothify library.\n    See https://github.com/DPIRD-DMA/Smoothify for more details.\n\n    Args:\n        vector_data: The vector data to smooth.\n        output_path: The path to save the smoothed vector data. If None, returns the smoothed vector data.\n        segment_length: Resolution of the original raster data in map units. If None (default), automatically\n            detects by finding the minimum segment length (from a data sample). Recommended to specify explicitly when known.\n        smooth_iterations: The number of iterations to smooth the vector data.\n        num_cores: Number of cores to use for parallel processing. If 0 (default), uses all available cores.\n        merge_collection: Whether to merge/dissolve adjacent geometries in collections before smoothing.\n        merge_field: Column name to use for dissolving geometries. Only valid when merge_collection=True.\n            If None, dissolves all geometries together. If specified, dissolves geometries grouped by the column values.\n        merge_multipolygons: Whether to merge adjacent polygons within MultiPolygons before smoothing\n        preserve_area: Whether to restore original area after smoothing via buffering (applies to Polygons only)\n        area_tolerance: Percentage of original area allowed as error (e.g., 0.01 = 0.01% error = 99.99% preservation).\n            Only affects Polygons when preserve_area=True\n\n    Returns:\n        gpd.GeoDataFrame: The smoothed vector data.\n\n    Examples:\n        &gt;&gt;&gt; from samgeo import common\n        &gt;&gt;&gt; gdf = common.read_vector(\"path/to/vector.geojson\")\n        &gt;&gt;&gt; smoothed_gdf = common.smooth_vector(gdf, smooth_iterations=3, output_path=\"path/to/smoothed_vector.geojson\")\n        &gt;&gt;&gt; smoothed_gdf.head()\n        &gt;&gt;&gt; smoothed_gdf.explore()\n    \"\"\"\n    import leafmap\n\n    try:\n        from smoothify import smoothify\n    except ImportError:\n        install_package(\"smoothify\")\n        from smoothify import smoothify\n\n    if isinstance(vector_data, str):\n        vector_data = leafmap.read_vector(vector_data)\n\n    smoothed_vector_data = smoothify(\n        geom=vector_data,\n        segment_length=segment_length,\n        smooth_iterations=smooth_iterations,\n        num_cores=num_cores,\n        merge_collection=merge_collection,\n        merge_field=merge_field,\n        merge_multipolygons=merge_multipolygons,\n        preserve_area=preserve_area,\n        area_tolerance=area_tolerance,\n        **kwargs,\n    )\n    if output_path is not None:\n        smoothed_vector_data.to_file(output_path)\n    return smoothed_vector_data\n</code></pre>"},{"location":"common/#samgeo.common.split_raster","title":"<code>split_raster(filename, out_dir, tile_size=256, overlap=0)</code>","text":"<p>Split a raster into tiles.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path or http URL to the raster file.</p> required <code>out_dir</code> <code>str</code> <p>The path to the output directory.</p> required <code>tile_size</code> <code>int | tuple</code> <p>The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>The number of pixels to overlap between tiles. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def split_raster(filename, out_dir, tile_size=256, overlap=0):\n    \"\"\"Split a raster into tiles.\n\n    Args:\n        filename (str): The path or http URL to the raster file.\n        out_dir (str): The path to the output directory.\n        tile_size (int | tuple, optional): The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.\n        overlap (int, optional): The number of pixels to overlap between tiles. Defaults to 0.\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n\n    if isinstance(filename, str):\n        if filename.startswith(\"http\"):\n            output = filename.split(\"/\")[-1]\n            download_file(filename, output)\n            filename = output\n\n    # Open the input GeoTIFF file\n    ds = gdal.Open(filename)\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(tile_size, int):\n        tile_width = tile_size\n        tile_height = tile_size\n    elif isinstance(tile_size, tuple):\n        tile_width = tile_size[0]\n        tile_height = tile_size[1]\n\n    # Get the size of the input raster\n    width = ds.RasterXSize\n    height = ds.RasterYSize\n\n    # Calculate the number of tiles needed in both directions, taking into account the overlap\n    num_tiles_x = (width - overlap) // (tile_width - overlap) + int(\n        (width - overlap) % (tile_width - overlap) &gt; 0\n    )\n    num_tiles_y = (height - overlap) // (tile_height - overlap) + int(\n        (height - overlap) % (tile_height - overlap) &gt; 0\n    )\n\n    # Get the georeferencing information of the input raster\n    geotransform = ds.GetGeoTransform()\n\n    # Loop over all the tiles\n    for i in range(num_tiles_x):\n        for j in range(num_tiles_y):\n            # Calculate the pixel coordinates of the tile, taking into account the overlap and clamping to the edge of the raster\n            x_min = i * (tile_width - overlap)\n            y_min = j * (tile_height - overlap)\n            x_max = min(x_min + tile_width, width)\n            y_max = min(y_min + tile_height, height)\n\n            # Adjust the size of the last tile in each row and column to include any remaining pixels\n            if i == num_tiles_x - 1:\n                x_min = max(x_max - tile_width, 0)\n            if j == num_tiles_y - 1:\n                y_min = max(y_max - tile_height, 0)\n\n            # Calculate the size of the tile, taking into account the overlap\n            tile_width = x_max - x_min\n            tile_height = y_max - y_min\n\n            # Set the output file name\n            output_file = f\"{out_dir}/tile_{i}_{j}.tif\"\n\n            # Create a new dataset for the tile\n            driver = gdal.GetDriverByName(\"GTiff\")\n            tile_ds = driver.Create(\n                output_file,\n                tile_width,\n                tile_height,\n                ds.RasterCount,\n                ds.GetRasterBand(1).DataType,\n            )\n\n            # Calculate the georeferencing information for the output tile\n            tile_geotransform = (\n                geotransform[0] + x_min * geotransform[1],\n                geotransform[1],\n                0,\n                geotransform[3] + y_min * geotransform[5],\n                0,\n                geotransform[5],\n            )\n\n            # Set the geotransform and projection of the tile\n            tile_ds.SetGeoTransform(tile_geotransform)\n            tile_ds.SetProjection(ds.GetProjection())\n\n            # Read the data from the input raster band(s) and write it to the tile band(s)\n            for k in range(ds.RasterCount):\n                band = ds.GetRasterBand(k + 1)\n                tile_band = tile_ds.GetRasterBand(k + 1)\n                tile_data = band.ReadAsArray(x_min, y_min, tile_width, tile_height)\n                tile_band.WriteArray(tile_data)\n\n            # Close the tile dataset\n            tile_ds = None\n\n    # Close the input dataset\n    ds = None\n</code></pre>"},{"location":"common/#samgeo.common.temp_file_path","title":"<code>temp_file_path(extension)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The temporary file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def temp_file_path(extension):\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        extension (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not extension.startswith(\".\"):\n        extension = \".\" + extension\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{extension}\")\n\n    return file_path\n</code></pre>"},{"location":"common/#samgeo.common.text_sam_gui","title":"<code>text_sam_gui(sam, basemap='SATELLITE', out_dir=None, box_threshold=0.25, text_threshold=0.25, cmap='viridis', opacity=0.7, min_size=10, max_size=None, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>box_threshold</code> <code>float</code> <p>The threshold for the box. Defaults to 0.25.</p> <code>0.25</code> <code>text_threshold</code> <code>float</code> <p>The threshold for the text. Defaults to 0.25.</p> <code>0.25</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>opacity</code> <code>float</code> <p>The opacity of the mask. Defaults to 0.7.</p> <code>0.7</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def text_sam_gui(\n    sam,\n    basemap=\"SATELLITE\",\n    out_dir=None,\n    box_threshold=0.25,\n    text_threshold=0.25,\n    cmap=\"viridis\",\n    opacity=0.7,\n    min_size=10,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo):\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        out_dir (str, optional): The output directory. Defaults to None.\n        box_threshold (float, optional): The threshold for the box. Defaults to 0.25.\n        text_threshold (float, optional): The threshold for the text. Defaults to 0.25.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n        opacity (float, optional): The opacity of the mask. Defaults to 0.7.\n        min_size (int, optional): The minimum size of the object. Defaults to 10.\n        max_size (int, optional): The maximum size of the object. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n\n        import ipyevents\n        import ipyleaflet\n        import ipywidgets as widgets\n        import leafmap\n        import leafmap.colormaps as cm\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first.\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(**kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    if basemap is not None:\n        m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except Exception:\n        pass\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 4px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    text_prompt = widgets.Text(\n        description=\"Text prompt:\",\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    box_slider = widgets.FloatSlider(\n        description=\"Box threshold:\",\n        min=0,\n        max=1,\n        value=box_threshold,\n        step=0.01,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    if sam.model_version == \"sam3\":\n        box_slider.description = \"Conf. threshold:\"\n\n    text_slider = widgets.FloatSlider(\n        description=\"Text threshold:\",\n        min=0,\n        max=1,\n        step=0.01,\n        value=text_threshold,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    if sam.model_version == \"sam3\":\n        text_slider.description = \"Mask threshold:\"\n\n    cmap_dropdown = widgets.Dropdown(\n        description=\"Palette:\",\n        options=cm.list_colormaps(),\n        value=cmap,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Opacity:\",\n        min=0,\n        max=1,\n        value=opacity,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            if hasattr(m, \"layer_name\"):\n                mask_layer = m.find_layer(m.layer_name)\n                if mask_layer is not None:\n                    mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        text_prompt,\n        box_slider,\n        text_slider,\n        cmap_dropdown,\n        opacity_slider,\n        widgets.HBox([rectangular, colorpicker]),\n        widgets.HBox(\n            [segment_button, save_button, reset_button],\n            layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n        ),\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(text_prompt.value) == 0 and m.user_roi_bounds() is None:\n                    print(\n                        \"Please enter a text prompt or draw a region of interest first.\"\n                    )\n                elif sam.source is None:\n                    print(\"Please run sam.set_image() first.\")\n                else:\n                    print(\"Segmenting...\")\n                    if len(text_prompt.value) &gt; 0:\n                        layer_name = text_prompt.value.replace(\" \", \"_\")\n                    elif m.user_roi_bounds() is not None:\n                        layer_name = \"masks\"\n\n                    filename = os.path.join(\n                        out_dir, f\"{layer_name}_{random_string()}.tif\"\n                    )\n                    try:\n                        if sam.model_version == \"sam2\":\n                            sam.predict(\n                                sam.source,\n                                text_prompt.value,\n                                box_slider.value,\n                                text_slider.value,\n                                output=filename,\n                            )\n                        elif sam.model_version == \"sam3\":\n                            sam.confidence_threshold = box_slider.value\n                            sam.mask_threshold = text_slider.value\n                            if len(text_prompt.value) &gt; 0:\n                                sam.generate_masks(\n                                    prompt=text_prompt.value,\n                                    min_size=min_size,\n                                    max_size=max_size,\n                                )\n                            elif m.user_roi_bounds() is not None:\n                                sam.generate_masks_by_boxes(\n                                    boxes=[m.user_roi_bounds()],\n                                    box_crs=\"EPSG:4326\",\n                                    min_size=min_size,\n                                    max_size=max_size,\n                                )\n                            else:\n                                print(\n                                    \"Please enter a text prompt or draw a region of interest first.\"\n                                )\n                                return\n                            sam.save_masks(output=filename)\n                        else:\n                            print(\n                                f\"Unknown or unsupported model_version: {getattr(sam, 'model_version', None)}. Please set sam.model_version to 'sam2' or 'sam3'.\"\n                            )\n                            return\n                        sam.output = filename\n                        if m.find_layer(layer_name) is not None:\n                            m.remove_layer(m.find_layer(layer_name))\n                        if m.find_layer(f\"{layer_name}_rect\") is not None:\n                            m.remove_layer(m.find_layer(f\"{layer_name} Regularized\"))\n                    except Exception as e:\n                        output.clear_output()\n                        print(e)\n                    if os.path.exists(filename):\n                        try:\n                            m.add_raster(\n                                filename,\n                                layer_name=layer_name,\n                                palette=cmap_dropdown.value,\n                                opacity=opacity_slider.value,\n                                nodata=0,\n                                zoom_to_layer=False,\n                            )\n                            m.layer_name = layer_name\n\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=f\"{layer_name} Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                            output.clear_output()\n\n                            if sam.model_version == \"sam3\":\n                                print(f\"Found {len(sam.masks)} objects.\")\n                        except Exception as e:\n                            print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.output, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                output.clear_output()\n                if not hasattr(m, \"layer_name\"):\n                    print(\"Please click the Segment button first.\")\n                else:\n                    sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                    filechooser = FileChooser(\n                        path=os.getcwd(),\n                        filename=f\"{m.layer_name}.tif\",\n                        sandbox_path=sandbox_path,\n                        layout=widgets.Layout(width=\"454px\"),\n                    )\n                    filechooser.use_dir_icons = True\n                    filechooser.filter_pattern = [\"*.tif\"]\n                    filechooser.register_callback(filechooser_callback)\n                    save_control = ipyleaflet.WidgetControl(\n                        widget=filechooser, position=\"topright\"\n                    )\n                    m.add_control(save_control)\n                    m.save_control = save_control\n\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            save_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.7\n            model_version = getattr(sam, \"model_version\", \"sam2\")\n            if model_version == \"sam2\":\n                box_slider.value = 0.25\n                text_slider.value = 0.25\n            elif model_version == \"sam3\":\n                box_slider.value = 0.5\n                text_slider.value = 0.5\n            cmap_dropdown.value = \"viridis\"\n            text_prompt.value = \"\"\n            output.clear_output()\n            try:\n                if hasattr(m, \"layer_name\") and m.find_layer(m.layer_name) is not None:\n                    m.remove_layer(m.find_layer(m.layer_name))\n                m.clear_drawings()\n            except Exception:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.tms_to_geotiff","title":"<code>tms_to_geotiff(output, bbox, zoom=None, resolution=None, source='OpenStreetMap', crs='EPSG:3857', to_cog=False, return_image=False, overwrite=False, quiet=False, **kwargs)</code>","text":"<p>Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.     Credits to the GitHub user @gumblex.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output GeoTIFF file.</p> required <code>bbox</code> <code>list</code> <p>The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]</p> required <code>zoom</code> <code>int</code> <p>The map zoom level. Defaults to None.</p> <code>None</code> <code>resolution</code> <code>float</code> <p>The resolution in meters. Defaults to None.</p> <code>None</code> <code>source</code> <code>str</code> <p>The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>crs</code> <code>str</code> <p>The output CRS. Defaults to \"EPSG:3857\".</p> <code>'EPSG:3857'</code> <code>to_cog</code> <code>bool</code> <p>Convert to Cloud Optimized GeoTIFF. Defaults to False.</p> <code>False</code> <code>return_image</code> <code>bool</code> <p>Return the image as PIL.Image. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the output file if it already exists. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>Suppress output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def tms_to_geotiff(\n    output,\n    bbox,\n    zoom=None,\n    resolution=None,\n    source=\"OpenStreetMap\",\n    crs=\"EPSG:3857\",\n    to_cog=False,\n    return_image=False,\n    overwrite=False,\n    quiet=False,\n    **kwargs,\n):\n    \"\"\"Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.\n        Credits to the GitHub user @gumblex.\n\n    Args:\n        output (str): The output GeoTIFF file.\n        bbox (list): The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]\n        zoom (int, optional): The map zoom level. Defaults to None.\n        resolution (float, optional): The resolution in meters. Defaults to None.\n        source (str, optional): The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\",\n            \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".\n        crs (str, optional): The output CRS. Defaults to \"EPSG:3857\".\n        to_cog (bool, optional): Convert to Cloud Optimized GeoTIFF. Defaults to False.\n        return_image (bool, optional): Return the image as PIL.Image. Defaults to False.\n        overwrite (bool, optional): Overwrite the output file if it already exists. Defaults to False.\n        quiet (bool, optional): Suppress output. Defaults to False.\n        **kwargs: Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().\n\n    \"\"\"\n\n    import concurrent.futures\n    import io\n    import itertools\n    import math\n    import re\n\n    from PIL import Image\n\n    try:\n        from osgeo import gdal, osr\n    except ImportError:\n        raise ImportError(\"GDAL is not installed. Install it with pip install GDAL\")\n\n    try:\n        import httpx\n\n        SESSION = httpx.Client()\n    except ImportError:\n        import requests\n\n        SESSION = requests.Session()\n\n    if not overwrite and os.path.exists(output):\n        print(\n            f\"The output file {output} already exists. Use `overwrite=True` to overwrite it.\"\n        )\n        return\n\n    xyz_tiles = {\n        \"OPENSTREETMAP\": \"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n        \"ROADMAP\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"SATELLITE\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"TERRAIN\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"HYBRID\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n    }\n\n    basemaps = get_basemaps()\n\n    if isinstance(source, str):\n        if source.upper() in xyz_tiles:\n            source = xyz_tiles[source.upper()]\n        elif source in basemaps:\n            source = basemaps[source]\n        elif source.startswith(\"http\"):\n            pass\n    else:\n        raise ValueError(\n            'source must be one of \"OpenStreetMap\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or a URL'\n        )\n\n    def resolution_to_zoom_level(resolution):\n        \"\"\"\n        Convert map resolution in meters to zoom level for Web Mercator (EPSG:3857) tiles.\n        \"\"\"\n        # Web Mercator tile size in meters at zoom level 0\n        initial_resolution = 156543.03392804097\n\n        # Calculate the zoom level\n        zoom_level = math.log2(initial_resolution / resolution)\n\n        return int(zoom_level)\n\n    if isinstance(bbox, list) and len(bbox) == 4:\n        west, south, east, north = bbox\n    else:\n        raise ValueError(\n            \"bbox must be a list of 4 coordinates in the format of [xmin, ymin, xmax, ymax]\"\n        )\n\n    if zoom is None and resolution is None:\n        raise ValueError(\"Either zoom or resolution must be provided\")\n    elif zoom is not None and resolution is not None:\n        raise ValueError(\"Only one of zoom or resolution can be provided\")\n\n    if resolution is not None:\n        zoom = resolution_to_zoom_level(resolution)\n\n    EARTH_EQUATORIAL_RADIUS = 6378137.0\n\n    Image.MAX_IMAGE_PIXELS = None\n\n    gdal.UseExceptions()\n    web_mercator = osr.SpatialReference()\n    try:\n        web_mercator.ImportFromEPSG(3857)\n    except RuntimeError as e:\n        # https://github.com/PDAL/PDAL/issues/2544#issuecomment-637995923\n        if \"PROJ\" in str(e):\n            pattern = r\"/[\\w/]+\"\n            match = re.search(pattern, str(e))\n            if match:\n                file_path = match.group(0)\n                os.environ[\"PROJ_LIB\"] = file_path\n                os.environ[\"GDAL_DATA\"] = file_path.replace(\"proj\", \"gdal\")\n                web_mercator.ImportFromEPSG(3857)\n\n    WKT_3857 = web_mercator.ExportToWkt()\n\n    def from4326_to3857(lat, lon):\n        xtile = math.radians(lon) * EARTH_EQUATORIAL_RADIUS\n        ytile = (\n            math.log(math.tan(math.radians(45 + lat / 2.0))) * EARTH_EQUATORIAL_RADIUS\n        )\n        return (xtile, ytile)\n\n    def deg2num(lat, lon, zoom):\n        lat_r = math.radians(lat)\n        n = 2**zoom\n        xtile = (lon + 180) / 360 * n\n        ytile = (1 - math.log(math.tan(lat_r) + 1 / math.cos(lat_r)) / math.pi) / 2 * n\n        return (xtile, ytile)\n\n    def is_empty(im):\n        extrema = im.getextrema()\n        if len(extrema) &gt;= 3:\n            if len(extrema) &gt; 3 and extrema[-1] == (0, 0):\n                return True\n            for ext in extrema[:3]:\n                if ext != (0, 0):\n                    return False\n            return True\n        else:\n            return extrema[0] == (0, 0)\n\n    def paste_tile(bigim, base_size, tile, corner_xy, bbox):\n        if tile is None:\n            return bigim\n        im = Image.open(io.BytesIO(tile))\n        mode = \"RGB\" if im.mode == \"RGB\" else \"RGBA\"\n        size = im.size\n        if bigim is None:\n            base_size[0] = size[0]\n            base_size[1] = size[1]\n            newim = Image.new(\n                mode, (size[0] * (bbox[2] - bbox[0]), size[1] * (bbox[3] - bbox[1]))\n            )\n        else:\n            newim = bigim\n\n        dx = abs(corner_xy[0] - bbox[0])\n        dy = abs(corner_xy[1] - bbox[1])\n        xy0 = (size[0] * dx, size[1] * dy)\n        if mode == \"RGB\":\n            newim.paste(im, xy0)\n        else:\n            if im.mode != mode:\n                im = im.convert(mode)\n            if not is_empty(im):\n                newim.paste(im, xy0)\n        im.close()\n        return newim\n\n    def finish_picture(bigim, base_size, bbox, x0, y0, x1, y1):\n        xfrac = x0 - bbox[0]\n        yfrac = y0 - bbox[1]\n        x2 = round(base_size[0] * xfrac)\n        y2 = round(base_size[1] * yfrac)\n        imgw = round(base_size[0] * (x1 - x0))\n        imgh = round(base_size[1] * (y1 - y0))\n        retim = bigim.crop((x2, y2, x2 + imgw, y2 + imgh))\n        if retim.mode == \"RGBA\" and retim.getextrema()[3] == (255, 255):\n            retim = retim.convert(\"RGB\")\n        bigim.close()\n        return retim\n\n    def get_tile(url):\n        retry = 3\n        while 1:\n            try:\n                r = SESSION.get(url, timeout=60)\n                break\n            except Exception:\n                retry -= 1\n                if not retry:\n                    raise\n        if r.status_code == 404:\n            return None\n        elif not r.content:\n            return None\n        r.raise_for_status()\n        return r.content\n\n    def draw_tile(\n        source, lat0, lon0, lat1, lon1, zoom, filename, quiet=False, **kwargs\n    ):\n        x0, y0 = deg2num(lat0, lon0, zoom)\n        x1, y1 = deg2num(lat1, lon1, zoom)\n        x0, x1 = sorted([x0, x1])\n        y0, y1 = sorted([y0, y1])\n        corners = tuple(\n            itertools.product(\n                range(math.floor(x0), math.ceil(x1)),\n                range(math.floor(y0), math.ceil(y1)),\n            )\n        )\n        totalnum = len(corners)\n        futures = []\n        with concurrent.futures.ThreadPoolExecutor(5) as executor:\n            for x, y in corners:\n                futures.append(\n                    executor.submit(get_tile, source.format(z=zoom, x=x, y=y))\n                )\n            bbox = (math.floor(x0), math.floor(y0), math.ceil(x1), math.ceil(y1))\n            bigim = None\n            base_size = [256, 256]\n            for k, (fut, corner_xy) in enumerate(zip(futures, corners), 1):\n                bigim = paste_tile(bigim, base_size, fut.result(), corner_xy, bbox)\n                if not quiet:\n                    print(\n                        f\"Downloaded image {str(k).zfill(len(str(totalnum)))}/{totalnum}\"\n                    )\n\n        if not quiet:\n            print(\"Saving GeoTIFF. Please wait...\")\n        img = finish_picture(bigim, base_size, bbox, x0, y0, x1, y1)\n        imgbands = len(img.getbands())\n        driver = gdal.GetDriverByName(\"GTiff\")\n\n        if \"options\" not in kwargs:\n            kwargs[\"options\"] = [\n                \"COMPRESS=DEFLATE\",\n                \"PREDICTOR=2\",\n                \"ZLEVEL=9\",\n                \"TILED=YES\",\n            ]\n\n        gtiff = driver.Create(\n            filename,\n            img.size[0],\n            img.size[1],\n            imgbands,\n            gdal.GDT_Byte,\n            **kwargs,\n        )\n        xp0, yp0 = from4326_to3857(lat0, lon0)\n        xp1, yp1 = from4326_to3857(lat1, lon1)\n        pwidth = abs(xp1 - xp0) / img.size[0]\n        pheight = abs(yp1 - yp0) / img.size[1]\n        gtiff.SetGeoTransform((min(xp0, xp1), pwidth, 0, max(yp0, yp1), 0, -pheight))\n        gtiff.SetProjection(WKT_3857)\n        for band in range(imgbands):\n            array = np.array(img.getdata(band), dtype=\"u8\")\n            array = array.reshape((img.size[1], img.size[0]))\n            band = gtiff.GetRasterBand(band + 1)\n            band.WriteArray(array)\n        gtiff.FlushCache()\n\n        if not quiet:\n            print(f\"Image saved to {filename}\")\n        return img\n\n    try:\n        image = draw_tile(\n            source, south, west, north, east, zoom, output, quiet, **kwargs\n        )\n        if return_image:\n            return image\n        if crs.upper() != \"EPSG:3857\":\n            reproject(output, output, crs, to_cog=to_cog)\n        elif to_cog:\n            image_to_cog(output, output)\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.transform_coords","title":"<code>transform_coords(x, y, src_crs, dst_crs, **kwargs)</code>","text":"<p>Transform coordinates from one CRS to another.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The x coordinate.</p> required <code>y</code> <code>float</code> <p>The y coordinate.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS, e.g., \"EPSG:4326\".</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS, e.g., \"EPSG:3857\".</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The transformed coordinates in the format of (x, y)</p> Source code in <code>samgeo/common.py</code> <pre><code>def transform_coords(x, y, src_crs, dst_crs, **kwargs):\n    \"\"\"Transform coordinates from one CRS to another.\n\n    Args:\n        x (float): The x coordinate.\n        y (float): The y coordinate.\n        src_crs (str): The source CRS, e.g., \"EPSG:4326\".\n        dst_crs (str): The destination CRS, e.g., \"EPSG:3857\".\n\n    Returns:\n        dict: The transformed coordinates in the format of (x, y)\n    \"\"\"\n    transformer = pyproj.Transformer.from_crs(\n        src_crs, dst_crs, always_xy=True, **kwargs\n    )\n    return transformer.transform(x, y)\n</code></pre>"},{"location":"common/#samgeo.common.update_package","title":"<code>update_package(out_dir=None, keep=False, **kwargs)</code>","text":"<p>Updates the package from the GitHub repository without the need to use pip or conda.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the downloaded package. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the download_file() function.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def update_package(out_dir=None, keep=False, **kwargs):\n    \"\"\"Updates the package from the GitHub repository without the need to use pip or conda.\n\n    Args:\n        out_dir (str, optional): The output directory. Defaults to None.\n        keep (bool, optional): Whether to keep the downloaded package. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the download_file() function.\n    \"\"\"\n\n    import shutil\n\n    try:\n        if out_dir is None:\n            out_dir = os.getcwd()\n        url = (\n            \"https://github.com/opengeos/segment-geospatial/archive/refs/heads/main.zip\"\n        )\n        filename = \"segment-geospatial-main.zip\"\n        download_file(url, filename, **kwargs)\n\n        pkg_dir = os.path.join(out_dir, \"segment-geospatial-main\")\n        work_dir = os.getcwd()\n        os.chdir(pkg_dir)\n\n        if shutil.which(\"pip\") is None:\n            cmd = \"pip3 install .\"\n        else:\n            cmd = \"pip install .\"\n\n        os.system(cmd)\n        os.chdir(work_dir)\n\n        if not keep:\n            shutil.rmtree(pkg_dir)\n            try:\n                os.remove(filename)\n            except Exception:\n                pass\n\n        print(\"Package updated successfully.\")\n\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The geojson dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def vector_to_geojson(filename, output=None, **kwargs):\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"common/#samgeo.common.video_to_images","title":"<code>video_to_images(video_path, output_dir, frame_rate=None, prefix='')</code>","text":"<p>Converts a video into a series of images. Each frame of the video is saved as an image.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the images will be saved.</p> required <code>frame_rate</code> <code>Optional[int]</code> <p>The number of frames to save per second of video. If None, all frames will be saved. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix for the output image filenames. Defaults to 'frame_'.</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the video file cannot be read or if the output directory is invalid.</p> Example usage <p>video_to_images('input_video.mp4', 'output_images', frame_rate=1, prefix='image_')</p> Source code in <code>samgeo/common.py</code> <pre><code>def video_to_images(\n    video_path: str,\n    output_dir: str,\n    frame_rate: Optional[int] = None,\n    prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Converts a video into a series of images. Each frame of the video is saved as an image.\n\n    Args:\n        video_path (str): The path to the video file.\n        output_dir (str): The directory where the images will be saved.\n        frame_rate (Optional[int], optional): The number of frames to save per second of video.\n            If None, all frames will be saved. Defaults to None.\n        prefix (str, optional): The prefix for the output image filenames. Defaults to 'frame_'.\n\n    Raises:\n        ValueError: If the video file cannot be read or if the output directory is invalid.\n\n    Example usage:\n        video_to_images('input_video.mp4', 'output_images', frame_rate=1, prefix='image_')\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Error opening video file {video_path}\")\n\n    # Get video properties\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_rate = (\n        frame_rate if frame_rate else video_fps\n    )  # Default to original FPS if not provided\n\n    # Calculate the number of digits based on the total frames (e.g., if total frames are 1000, width = 4)\n    num_digits = len(str(total_frames))\n\n    print(f\"Video FPS: {video_fps}\")\n    print(f\"Total Frames: {total_frames}\")\n    print(f\"Saving every {video_fps // frame_rate} frame(s)\")\n\n    frame_count = 0\n    saved_frame_count = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Save frames based on frame_rate\n        if frame_count % (video_fps // frame_rate) == 0:\n            img_path = os.path.join(\n                output_dir, f\"{prefix}{saved_frame_count:0{num_digits}d}.jpg\"\n            )\n            cv2.imwrite(img_path, frame)\n            saved_frame_count += 1\n            # print(f\"Saved {img_path}\")\n\n        frame_count += 1\n\n    # Release the video capture object\n    cap.release()\n    print(f\"Finished saving {saved_frame_count} images to {output_dir}\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>segment-geospatial could always use more documentation, whether as part of the official segment-geospatial docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up segment-geospatial for local development.</p> <ol> <li> <p>Fork the segment-geospatial repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/segment-geospatial.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv segment-geospatial\n$ cd segment-geospatial/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 segment-geospatial tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests (see the section below - Unit Testing).</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.md.</li> <li>The pull request should work for Python 3.8, 3.9, 3.10, and 3.11. Check https://github.com/giswqs/segment-geospatial/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"contributing/#unit-testing","title":"Unit Testing","text":"<p>Unit tests are in the <code>tests</code> folder. If you add new functionality to the package, please add a unit test for it. You can either add the test to an existing test file or create a new one. For example, if you add a new function to <code>samgeo/samgeo.py</code>, you can add the unit test to <code>tests/test_samgeo.py</code>. If you add a new module to <code>samgeo/&lt;MODULE-NAME&gt;</code>, you can create a new test file in <code>tests/test_&lt;MODULE-NAME&gt;</code>. Please refer to <code>tests/test_samgeo.py</code> for examples. For more information about unit testing, please refer to this tutorial - Getting Started With Testing in Python.</p> <p>To run the unit tests, navigate to the root directory of the package and run the following command:</p> <pre><code>python -m unittest discover tests/\n</code></pre>"},{"location":"contributing/#add-new-dependencies","title":"Add new dependencies","text":"<p>If you PR involves adding new dependencies, please make sure that the new dependencies are available on both PyPI and conda-forge. Search here to see if the package is available on conda-forge. If the package is not available on conda-forge, it can't be added as a required dependency in <code>requirements.txt</code>. Instead, it should be added as an optional dependency in <code>requirements_dev.txt</code>.</p> <p>If the package is available on PyPI and conda-forge, but if it is challenging to install the package on some operating systems, we would recommend adding the package as an optional dependency in <code>requirements_dev.txt</code> rather than a required dependency in <code>requirements.txt</code>.</p> <p>The dependencies required for building the documentation should be added to <code>requirements_docs.txt</code>. In most cases, contributors do not need to add new dependencies to <code>requirements_docs.txt</code> unless the documentation fails to build due to missing dependencies.</p>"},{"location":"detectree2/","title":"detectree2 module","text":"<p>Tree crown delineation using detectree2.</p> <p>This module provides a high-level interface for automatic tree crown delineation in aerial RGB imagery using the detectree2 library, which is based on Mask R-CNN (Detectron2 implementation).</p> Reference <p>Ball, J.G.C., et al. (2023). Accurate delineation of individual tree crowns in tropical forests from aerial RGB imagery using Mask R-CNN. Remote Sens Ecol Conserv. 9(5):641-655. https://doi.org/10.1002/rse2.332</p> <p>Repository: https://github.com/PatBall1/detectree2</p>"},{"location":"detectree2/#samgeo.detectree2.TreeCrownDelineator","title":"<code>TreeCrownDelineator</code>","text":"<p>Class for automatic tree crown delineation using detectree2.</p> <p>This class provides methods for detecting and delineating individual tree crowns in aerial RGB imagery using pre-trained or custom Mask R-CNN models.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to the trained model weights.</p> <code>device</code> <code>str</code> <p>Device to run inference on ('cuda' or 'cpu').</p> <code>cfg</code> <code>str</code> <p>Detectron2 configuration object.</p> <code>predictor</code> <code>str</code> <p>Detectron2 DefaultPredictor instance.</p> Example <p>from samgeo.detectree2 import TreeCrownDelineator delineator = TreeCrownDelineator() delineator.predict(\"orthomosaic.tif\", \"crowns.gpkg\")</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>class TreeCrownDelineator:\n    \"\"\"Class for automatic tree crown delineation using detectree2.\n\n    This class provides methods for detecting and delineating individual tree\n    crowns in aerial RGB imagery using pre-trained or custom Mask R-CNN models.\n\n    Attributes:\n        model_path (str): Path to the trained model weights.\n        device (str): Device to run inference on ('cuda' or 'cpu').\n        cfg: Detectron2 configuration object.\n        predictor: Detectron2 DefaultPredictor instance.\n\n    Example:\n        &gt;&gt;&gt; from samgeo.detectree2 import TreeCrownDelineator\n        &gt;&gt;&gt; delineator = TreeCrownDelineator()\n        &gt;&gt;&gt; delineator.predict(\"orthomosaic.tif\", \"crowns.gpkg\")\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        model_name: str = \"default\",\n        device: Optional[str] = None,\n        confidence_threshold: float = 0.5,\n        nms_threshold: float = 0.3,\n    ) -&gt; None:\n        \"\"\"Initialize the TreeCrownDelineator.\n\n        Args:\n            model_path: Path to a trained model file (.pth). If None, downloads\n                a pre-trained model based on model_name.\n            model_name: Name of pre-trained model to use if model_path is None.\n                Options: 'paracou', 'sepilok', 'danum', 'default'.\n            device: Device for inference ('cuda' or 'cpu'). If None, auto-detects.\n            confidence_threshold: Minimum confidence score for predictions (0-1).\n            nms_threshold: IoU threshold for non-maximum suppression (0-1).\n        \"\"\"\n        _check_detectree2()\n        _check_detectron2()\n\n        import torch\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.confidence_threshold = confidence_threshold\n        self.nms_threshold = nms_threshold\n        self.model_path = model_path\n        self.model_name = model_name\n        self._predictor = None\n        self._cfg = None\n\n        # Download model if not provided\n        if self.model_path is None:\n            self.model_path = self._download_model(model_name)\n\n        logger.info(f\"TreeCrownDelineator initialized with model: {self.model_path}\")\n        logger.info(f\"Using device: {self.device}\")\n\n    def _download_model(self, model_name: str) -&gt; str:\n        \"\"\"Download a pre-trained model from the model garden.\n\n        Args:\n            model_name: Name of the model to download.\n\n        Returns:\n            Path to the downloaded model file.\n        \"\"\"\n        from samgeo.common import download_file\n\n        if model_name not in PRETRAINED_MODELS:\n            available = \", \".join(PRETRAINED_MODELS.keys())\n            raise ValueError(\n                f\"Unknown model '{model_name}'. Available models: {available}\"\n            )\n\n        url = PRETRAINED_MODELS[model_name]\n        filename = os.path.basename(url)\n\n        # Download to cache directory\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"detectree2\")\n        os.makedirs(cache_dir, exist_ok=True)\n        model_path = os.path.join(cache_dir, filename)\n\n        if not os.path.exists(model_path):\n            logger.info(f\"Downloading pre-trained model '{model_name}' from {url}\")\n            download_file(url, model_path)\n            logger.info(f\"Model downloaded to {model_path}\")\n        else:\n            logger.info(f\"Using cached model: {model_path}\")\n\n        return model_path\n\n    def _setup_predictor(self) -&gt; None:\n        \"\"\"Set up the Detectron2 predictor with the model configuration.\"\"\"\n        if self._predictor is not None:\n            return\n\n        from detectree2.models.train import setup_cfg\n        from detectron2.engine import DefaultPredictor\n\n        self._cfg = setup_cfg(update_model=self.model_path)\n        self._cfg.MODEL.DEVICE = self.device\n        self._cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.confidence_threshold\n        self._cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = self.nms_threshold\n\n        self._predictor = DefaultPredictor(self._cfg)\n        logger.info(\"Predictor initialized successfully\")\n\n    def predict(\n        self,\n        image_path: str,\n        output_path: str,\n        tile_width: int = 40,\n        tile_height: int = 40,\n        buffer: int = 30,\n        simplify_tolerance: float = 0.3,\n        min_confidence: float = 0.5,\n        iou_threshold: float = 0.6,\n        output_format: str = \"gpkg\",\n        cleanup: bool = True,\n        **kwargs: Any,\n    ) -&gt; \"gpd.GeoDataFrame\":\n        \"\"\"Detect and delineate tree crowns in an orthomosaic.\n\n        Args:\n            image_path: Path to the input orthomosaic (GeoTIFF).\n            output_path: Path for the output crown polygons.\n            tile_width: Width of prediction tiles in meters.\n            tile_height: Height of prediction tiles in meters.\n            buffer: Buffer size around tiles in meters (for edge handling).\n            simplify_tolerance: Tolerance for simplifying crown geometries.\n            min_confidence: Minimum confidence score to keep predictions.\n            iou_threshold: IoU threshold for removing overlapping crowns.\n            output_format: Output format ('gpkg', 'shp', 'geojson').\n            cleanup: Whether to remove temporary files after prediction.\n            **kwargs: Additional arguments passed to tile_data.\n\n        Returns:\n            GeoDataFrame containing the detected tree crown polygons.\n        \"\"\"\n        import geopandas as gpd\n\n        from detectree2.models.outputs import (\n            clean_crowns,\n            project_to_geojson,\n            stitch_crowns,\n        )\n        from detectree2.models.predict import predict_on_data\n        from detectree2.preprocessing.tiling import tile_data\n\n        # Initialize predictor\n        self._setup_predictor()\n\n        # Create temporary directory for tiles\n        temp_dir = tempfile.mkdtemp(prefix=\"detectree2_\")\n        tiles_dir = os.path.join(temp_dir, \"tiles\")\n        pred_dir = os.path.join(temp_dir, \"predictions\")\n        geo_dir = os.path.join(temp_dir, \"predictions_geo\")\n\n        try:\n            logger.info(f\"Tiling orthomosaic: {image_path}\")\n            tile_data(\n                image_path,\n                tiles_dir,\n                buffer=buffer,\n                tile_width=tile_width,\n                tile_height=tile_height,\n                **kwargs,\n            )\n\n            logger.info(\"Running predictions on tiles...\")\n            predict_on_data(tiles_dir, pred_dir, predictor=self._predictor)\n\n            logger.info(\"Projecting predictions to geographic coordinates...\")\n            os.makedirs(geo_dir, exist_ok=True)\n            project_to_geojson(tiles_dir, pred_dir, geo_dir)\n\n            logger.info(\"Stitching and cleaning crown predictions...\")\n            crowns = stitch_crowns(geo_dir)\n            crowns = clean_crowns(crowns, iou_threshold, confidence=min_confidence)\n\n            # Simplify geometries\n            if simplify_tolerance &gt; 0:\n                crowns = crowns.set_geometry(crowns.simplify(simplify_tolerance))\n\n            # Save to file\n            driver_map = {\n                \"gpkg\": \"GPKG\",\n                \"shp\": \"ESRI Shapefile\",\n                \"geojson\": \"GeoJSON\",\n            }\n            driver = driver_map.get(output_format.lower(), \"GPKG\")\n\n            crowns.to_file(output_path, driver=driver)\n            logger.info(f\"Crown polygons saved to: {output_path}\")\n\n            return crowns\n\n        finally:\n            if cleanup and os.path.exists(temp_dir):\n                shutil.rmtree(temp_dir)\n                logger.debug(f\"Cleaned up temporary directory: {temp_dir}\")\n\n    def predict_tiles(\n        self,\n        tiles_dir: str,\n        output_dir: Optional[str] = None,\n    ) -&gt; List[str]:\n        \"\"\"Run predictions on pre-tiled images.\n\n        Args:\n            tiles_dir: Directory containing tiled images.\n            output_dir: Directory to save predictions. If None, saves in tiles_dir.\n\n        Returns:\n            List of paths to prediction files.\n        \"\"\"\n        from detectree2.models.predict import predict_on_data\n\n        self._setup_predictor()\n\n        if output_dir is None:\n            output_dir = tiles_dir\n\n        pred_dir = os.path.join(output_dir, \"predictions\")\n        logger.info(f\"Running predictions on tiles in: {tiles_dir}\")\n        predict_on_data(tiles_dir, pred_dir, predictor=self._predictor)\n\n        # Find prediction files\n        if os.path.exists(pred_dir):\n            pred_files = list(Path(pred_dir).glob(\"*.json\"))\n            logger.info(f\"Generated {len(pred_files)} prediction files\")\n            return [str(f) for f in pred_files]\n\n        return []\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.TreeCrownDelineator.__init__","title":"<code>__init__(model_path=None, model_name='default', device=None, confidence_threshold=0.5, nms_threshold=0.3)</code>","text":"<p>Initialize the TreeCrownDelineator.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to a trained model file (.pth). If None, downloads a pre-trained model based on model_name.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name of pre-trained model to use if model_path is None. Options: 'paracou', 'sepilok', 'danum', 'default'.</p> <code>'default'</code> <code>device</code> <code>Optional[str]</code> <p>Device for inference ('cuda' or 'cpu'). If None, auto-detects.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score for predictions (0-1).</p> <code>0.5</code> <code>nms_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression (0-1).</p> <code>0.3</code> Source code in <code>samgeo/detectree2.py</code> <pre><code>def __init__(\n    self,\n    model_path: Optional[str] = None,\n    model_name: str = \"default\",\n    device: Optional[str] = None,\n    confidence_threshold: float = 0.5,\n    nms_threshold: float = 0.3,\n) -&gt; None:\n    \"\"\"Initialize the TreeCrownDelineator.\n\n    Args:\n        model_path: Path to a trained model file (.pth). If None, downloads\n            a pre-trained model based on model_name.\n        model_name: Name of pre-trained model to use if model_path is None.\n            Options: 'paracou', 'sepilok', 'danum', 'default'.\n        device: Device for inference ('cuda' or 'cpu'). If None, auto-detects.\n        confidence_threshold: Minimum confidence score for predictions (0-1).\n        nms_threshold: IoU threshold for non-maximum suppression (0-1).\n    \"\"\"\n    _check_detectree2()\n    _check_detectron2()\n\n    import torch\n\n    self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.confidence_threshold = confidence_threshold\n    self.nms_threshold = nms_threshold\n    self.model_path = model_path\n    self.model_name = model_name\n    self._predictor = None\n    self._cfg = None\n\n    # Download model if not provided\n    if self.model_path is None:\n        self.model_path = self._download_model(model_name)\n\n    logger.info(f\"TreeCrownDelineator initialized with model: {self.model_path}\")\n    logger.info(f\"Using device: {self.device}\")\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.TreeCrownDelineator.predict","title":"<code>predict(image_path, output_path, tile_width=40, tile_height=40, buffer=30, simplify_tolerance=0.3, min_confidence=0.5, iou_threshold=0.6, output_format='gpkg', cleanup=True, **kwargs)</code>","text":"<p>Detect and delineate tree crowns in an orthomosaic.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input orthomosaic (GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path for the output crown polygons.</p> required <code>tile_width</code> <code>int</code> <p>Width of prediction tiles in meters.</p> <code>40</code> <code>tile_height</code> <code>int</code> <p>Height of prediction tiles in meters.</p> <code>40</code> <code>buffer</code> <code>int</code> <p>Buffer size around tiles in meters (for edge handling).</p> <code>30</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for simplifying crown geometries.</p> <code>0.3</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence score to keep predictions.</p> <code>0.5</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for removing overlapping crowns.</p> <code>0.6</code> <code>output_format</code> <code>str</code> <p>Output format ('gpkg', 'shp', 'geojson').</p> <code>'gpkg'</code> <code>cleanup</code> <code>bool</code> <p>Whether to remove temporary files after prediction.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to tile_data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing the detected tree crown polygons.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def predict(\n    self,\n    image_path: str,\n    output_path: str,\n    tile_width: int = 40,\n    tile_height: int = 40,\n    buffer: int = 30,\n    simplify_tolerance: float = 0.3,\n    min_confidence: float = 0.5,\n    iou_threshold: float = 0.6,\n    output_format: str = \"gpkg\",\n    cleanup: bool = True,\n    **kwargs: Any,\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"Detect and delineate tree crowns in an orthomosaic.\n\n    Args:\n        image_path: Path to the input orthomosaic (GeoTIFF).\n        output_path: Path for the output crown polygons.\n        tile_width: Width of prediction tiles in meters.\n        tile_height: Height of prediction tiles in meters.\n        buffer: Buffer size around tiles in meters (for edge handling).\n        simplify_tolerance: Tolerance for simplifying crown geometries.\n        min_confidence: Minimum confidence score to keep predictions.\n        iou_threshold: IoU threshold for removing overlapping crowns.\n        output_format: Output format ('gpkg', 'shp', 'geojson').\n        cleanup: Whether to remove temporary files after prediction.\n        **kwargs: Additional arguments passed to tile_data.\n\n    Returns:\n        GeoDataFrame containing the detected tree crown polygons.\n    \"\"\"\n    import geopandas as gpd\n\n    from detectree2.models.outputs import (\n        clean_crowns,\n        project_to_geojson,\n        stitch_crowns,\n    )\n    from detectree2.models.predict import predict_on_data\n    from detectree2.preprocessing.tiling import tile_data\n\n    # Initialize predictor\n    self._setup_predictor()\n\n    # Create temporary directory for tiles\n    temp_dir = tempfile.mkdtemp(prefix=\"detectree2_\")\n    tiles_dir = os.path.join(temp_dir, \"tiles\")\n    pred_dir = os.path.join(temp_dir, \"predictions\")\n    geo_dir = os.path.join(temp_dir, \"predictions_geo\")\n\n    try:\n        logger.info(f\"Tiling orthomosaic: {image_path}\")\n        tile_data(\n            image_path,\n            tiles_dir,\n            buffer=buffer,\n            tile_width=tile_width,\n            tile_height=tile_height,\n            **kwargs,\n        )\n\n        logger.info(\"Running predictions on tiles...\")\n        predict_on_data(tiles_dir, pred_dir, predictor=self._predictor)\n\n        logger.info(\"Projecting predictions to geographic coordinates...\")\n        os.makedirs(geo_dir, exist_ok=True)\n        project_to_geojson(tiles_dir, pred_dir, geo_dir)\n\n        logger.info(\"Stitching and cleaning crown predictions...\")\n        crowns = stitch_crowns(geo_dir)\n        crowns = clean_crowns(crowns, iou_threshold, confidence=min_confidence)\n\n        # Simplify geometries\n        if simplify_tolerance &gt; 0:\n            crowns = crowns.set_geometry(crowns.simplify(simplify_tolerance))\n\n        # Save to file\n        driver_map = {\n            \"gpkg\": \"GPKG\",\n            \"shp\": \"ESRI Shapefile\",\n            \"geojson\": \"GeoJSON\",\n        }\n        driver = driver_map.get(output_format.lower(), \"GPKG\")\n\n        crowns.to_file(output_path, driver=driver)\n        logger.info(f\"Crown polygons saved to: {output_path}\")\n\n        return crowns\n\n    finally:\n        if cleanup and os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n            logger.debug(f\"Cleaned up temporary directory: {temp_dir}\")\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.TreeCrownDelineator.predict_tiles","title":"<code>predict_tiles(tiles_dir, output_dir=None)</code>","text":"<p>Run predictions on pre-tiled images.</p> <p>Parameters:</p> Name Type Description Default <code>tiles_dir</code> <code>str</code> <p>Directory containing tiled images.</p> required <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save predictions. If None, saves in tiles_dir.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to prediction files.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def predict_tiles(\n    self,\n    tiles_dir: str,\n    output_dir: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Run predictions on pre-tiled images.\n\n    Args:\n        tiles_dir: Directory containing tiled images.\n        output_dir: Directory to save predictions. If None, saves in tiles_dir.\n\n    Returns:\n        List of paths to prediction files.\n    \"\"\"\n    from detectree2.models.predict import predict_on_data\n\n    self._setup_predictor()\n\n    if output_dir is None:\n        output_dir = tiles_dir\n\n    pred_dir = os.path.join(output_dir, \"predictions\")\n    logger.info(f\"Running predictions on tiles in: {tiles_dir}\")\n    predict_on_data(tiles_dir, pred_dir, predictor=self._predictor)\n\n    # Find prediction files\n    if os.path.exists(pred_dir):\n        pred_files = list(Path(pred_dir).glob(\"*.json\"))\n        logger.info(f\"Generated {len(pred_files)} prediction files\")\n        return [str(f) for f in pred_files]\n\n    return []\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.download_sample_data","title":"<code>download_sample_data(output_dir='./detectree2_sample')</code>","text":"<p>Download sample data for testing detectree2.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save the sample data.</p> <code>'./detectree2_sample'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output directory.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def download_sample_data(output_dir: str = \"./detectree2_sample\") -&gt; str:\n    \"\"\"Download sample data for testing detectree2.\n\n    Args:\n        output_dir: Directory to save the sample data.\n\n    Returns:\n        Path to the output directory.\n    \"\"\"\n    from samgeo.common import download_file\n\n    sample_url = \"https://zenodo.org/records/8136161/files/Paracou_sample.zip\"\n\n    os.makedirs(output_dir, exist_ok=True)\n    zip_path = os.path.join(output_dir, \"sample.zip\")\n\n    logger.info(f\"Downloading sample data from {sample_url}\")\n    download_file(sample_url, zip_path)\n\n    # Extract\n    import zipfile\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(output_dir)\n\n    os.remove(zip_path)\n    logger.info(f\"Sample data extracted to: {output_dir}\")\n\n    return output_dir\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.list_pretrained_models","title":"<code>list_pretrained_models()</code>","text":"<p>List available pre-trained models.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping model names to their download URLs.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def list_pretrained_models() -&gt; Dict[str, str]:\n    \"\"\"List available pre-trained models.\n\n    Returns:\n        Dictionary mapping model names to their download URLs.\n    \"\"\"\n    return PRETRAINED_MODELS.copy()\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.prepare_training_data","title":"<code>prepare_training_data(image_path, crowns_path, output_dir, tile_width=40, tile_height=40, buffer=30, threshold=0.6, test_fraction=0.15, mode='rgb')</code>","text":"<p>Prepare training and test data for detectree2.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input orthomosaic (GeoTIFF).</p> required <code>crowns_path</code> <code>str</code> <p>Path to manually delineated crown polygons.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the training data.</p> required <code>tile_width</code> <code>int</code> <p>Width of tiles in meters.</p> <code>40</code> <code>tile_height</code> <code>int</code> <p>Height of tiles in meters.</p> <code>40</code> <code>buffer</code> <code>int</code> <p>Buffer size around tiles in meters.</p> <code>30</code> <code>threshold</code> <code>float</code> <p>Minimum crown coverage to keep a tile.</p> <code>0.6</code> <code>test_fraction</code> <code>float</code> <p>Fraction of data to use for testing (0-1).</p> <code>0.15</code> <code>mode</code> <code>str</code> <p>Image mode ('rgb' or 'ms' for multispectral).</p> <code>'rgb'</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple of (train_dir, test_dir) paths.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def prepare_training_data(\n    image_path: str,\n    crowns_path: str,\n    output_dir: str,\n    tile_width: int = 40,\n    tile_height: int = 40,\n    buffer: int = 30,\n    threshold: float = 0.6,\n    test_fraction: float = 0.15,\n    mode: str = \"rgb\",\n) -&gt; Tuple[str, str]:\n    \"\"\"Prepare training and test data for detectree2.\n\n    Args:\n        image_path: Path to the input orthomosaic (GeoTIFF).\n        crowns_path: Path to manually delineated crown polygons.\n        output_dir: Directory to save the training data.\n        tile_width: Width of tiles in meters.\n        tile_height: Height of tiles in meters.\n        buffer: Buffer size around tiles in meters.\n        threshold: Minimum crown coverage to keep a tile.\n        test_fraction: Fraction of data to use for testing (0-1).\n        mode: Image mode ('rgb' or 'ms' for multispectral).\n\n    Returns:\n        Tuple of (train_dir, test_dir) paths.\n    \"\"\"\n    _check_detectree2()\n\n    from detectree2.preprocessing.tiling import tile_data, to_traintest_folders\n\n    # First tile the data\n    tile_orthomosaic(\n        image_path,\n        output_dir,\n        tile_width=tile_width,\n        tile_height=tile_height,\n        buffer=buffer,\n        crowns_path=crowns_path,\n        threshold=threshold,\n        mode=mode,\n    )\n\n    # Split into train/test\n    logger.info(f\"Splitting data into train/test (test fraction: {test_fraction})\")\n    to_traintest_folders(output_dir, output_dir, test_frac=test_fraction)\n\n    train_dir = os.path.join(output_dir, \"train\")\n    test_dir = os.path.join(output_dir, \"test\")\n\n    logger.info(f\"Training data: {train_dir}\")\n    logger.info(f\"Test data: {test_dir}\")\n\n    return train_dir, test_dir\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.stitch_predictions","title":"<code>stitch_predictions(geo_predictions_dir, output_path, iou_threshold=0.6, min_confidence=0.5, simplify_tolerance=0.3, output_format='gpkg')</code>","text":"<p>Stitch and clean tile predictions into a single crown map.</p> <p>Parameters:</p> Name Type Description Default <code>geo_predictions_dir</code> <code>str</code> <p>Directory containing geo-referenced predictions.</p> required <code>output_path</code> <code>str</code> <p>Path for the output crown polygons.</p> required <code>iou_threshold</code> <code>float</code> <p>IoU threshold for removing overlapping crowns.</p> <code>0.6</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence score to keep predictions.</p> <code>0.5</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for simplifying crown geometries.</p> <code>0.3</code> <code>output_format</code> <code>str</code> <p>Output format ('gpkg', 'shp', 'geojson').</p> <code>'gpkg'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing the stitched and cleaned crown polygons.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def stitch_predictions(\n    geo_predictions_dir: str,\n    output_path: str,\n    iou_threshold: float = 0.6,\n    min_confidence: float = 0.5,\n    simplify_tolerance: float = 0.3,\n    output_format: str = \"gpkg\",\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"Stitch and clean tile predictions into a single crown map.\n\n    Args:\n        geo_predictions_dir: Directory containing geo-referenced predictions.\n        output_path: Path for the output crown polygons.\n        iou_threshold: IoU threshold for removing overlapping crowns.\n        min_confidence: Minimum confidence score to keep predictions.\n        simplify_tolerance: Tolerance for simplifying crown geometries.\n        output_format: Output format ('gpkg', 'shp', 'geojson').\n\n    Returns:\n        GeoDataFrame containing the stitched and cleaned crown polygons.\n    \"\"\"\n    _check_detectree2()\n\n    import geopandas as gpd\n\n    from detectree2.models.outputs import clean_crowns, stitch_crowns\n\n    logger.info(f\"Stitching predictions from: {geo_predictions_dir}\")\n    crowns = stitch_crowns(geo_predictions_dir)\n\n    logger.info(\"Cleaning overlapping crowns...\")\n    crowns = clean_crowns(crowns, iou_threshold, confidence=min_confidence)\n\n    if simplify_tolerance &gt; 0:\n        crowns = crowns.set_geometry(crowns.simplify(simplify_tolerance))\n\n    # Save to file\n    driver_map = {\n        \"gpkg\": \"GPKG\",\n        \"shp\": \"ESRI Shapefile\",\n        \"geojson\": \"GeoJSON\",\n    }\n    driver = driver_map.get(output_format.lower(), \"GPKG\")\n\n    crowns.to_file(output_path, driver=driver)\n    logger.info(f\"Crown polygons saved to: {output_path}\")\n\n    return crowns\n</code></pre>"},{"location":"detectree2/#samgeo.detectree2.tile_orthomosaic","title":"<code>tile_orthomosaic(image_path, output_dir, tile_width=40, tile_height=40, buffer=30, crowns_path=None, threshold=0.6, mode='rgb', **kwargs)</code>","text":"<p>Tile an orthomosaic for training or prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input orthomosaic (GeoTIFF).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the tiles.</p> required <code>tile_width</code> <code>int</code> <p>Width of tiles in meters.</p> <code>40</code> <code>tile_height</code> <code>int</code> <p>Height of tiles in meters.</p> <code>40</code> <code>buffer</code> <code>int</code> <p>Buffer size around tiles in meters.</p> <code>30</code> <code>crowns_path</code> <code>Optional[str]</code> <p>Path to crown polygons (for training data preparation).</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum crown coverage to keep a tile (when crowns provided).</p> <code>0.6</code> <code>mode</code> <code>str</code> <p>Image mode ('rgb' or 'ms' for multispectral).</p> <code>'rgb'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to tile_data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output directory containing tiles.</p> Source code in <code>samgeo/detectree2.py</code> <pre><code>def tile_orthomosaic(\n    image_path: str,\n    output_dir: str,\n    tile_width: int = 40,\n    tile_height: int = 40,\n    buffer: int = 30,\n    crowns_path: Optional[str] = None,\n    threshold: float = 0.6,\n    mode: str = \"rgb\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Tile an orthomosaic for training or prediction.\n\n    Args:\n        image_path: Path to the input orthomosaic (GeoTIFF).\n        output_dir: Directory to save the tiles.\n        tile_width: Width of tiles in meters.\n        tile_height: Height of tiles in meters.\n        buffer: Buffer size around tiles in meters.\n        crowns_path: Path to crown polygons (for training data preparation).\n        threshold: Minimum crown coverage to keep a tile (when crowns provided).\n        mode: Image mode ('rgb' or 'ms' for multispectral).\n        **kwargs: Additional arguments passed to tile_data.\n\n    Returns:\n        Path to the output directory containing tiles.\n    \"\"\"\n    _check_detectree2()\n\n    import geopandas as gpd\n    import rasterio\n\n    from detectree2.preprocessing.tiling import tile_data\n\n    crowns = None\n    if crowns_path is not None:\n        # Read crowns and match CRS to image\n        with rasterio.open(image_path) as src:\n            img_crs = src.crs\n        crowns = gpd.read_file(crowns_path)\n        crowns = crowns.to_crs(img_crs)\n\n    logger.info(f\"Tiling orthomosaic: {image_path}\")\n    tile_data(\n        image_path,\n        output_dir,\n        buffer=buffer,\n        tile_width=tile_width,\n        tile_height=tile_height,\n        crowns=crowns,\n        threshold=threshold,\n        mode=mode,\n        **kwargs,\n    )\n\n    logger.info(f\"Tiles saved to: {output_dir}\")\n    return output_dir\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#coordinate-reference-systems-crs","title":"Coordinate Reference Systems (CRS)","text":""},{"location":"faq/#does-samgeo-automatically-convert-my-imagery-to-epsg4326","title":"Does samgeo automatically convert my imagery to EPSG:4326?","text":"<p>No. samgeo does NOT automatically reproject or convert your GeoTIFF files to EPSG:4326. Your imagery stays in its native coordinate system throughout the entire segmentation process.</p> <p>Key points: - When you load a GeoTIFF with <code>set_image()</code> or use <code>generate_masks_tiled()</code>, the image data is read directly without any CRS transformation - The output masks inherit the same CRS as your input imagery - The <code>reproject()</code> function in <code>samgeo.common</code> has EPSG:4326 as a default parameter, but it's only used when you explicitly call that function - it is NOT called automatically during segmentation</p> <p>Why might you see distortion? 1. Your visualization tool is displaying the results in EPSG:4326 (like some web map libraries) 2. You're manually calling the <code>reproject()</code> function somewhere in your workflow 3. Your original imagery already has CRS-related issues</p> <p>Recommendation: - Verify your input TIF has the correct CRS metadata using <code>gdalinfo</code> or rasterio (<code>src.crs</code>) - The output masks will preserve the same CRS as your input - If you need to reproject for visualization, do it as a separate step AFTER segmentation:   <pre><code>from samgeo import common\ncommon.reproject(\"input_masks.tif\", \"output_epsg4326.tif\", dst_crs=\"EPSG:4326\")\n</code></pre></p>"},{"location":"faq/#how-can-i-check-the-crs-of-my-geotiff","title":"How can I check the CRS of my GeoTIFF?","text":"<p>Using GDAL: <pre><code>gdalinfo your_file.tif\n</code></pre></p> <p>Using Python with rasterio: <pre><code>import rasterio\nwith rasterio.open(\"your_file.tif\") as src:\n    print(f\"CRS: {src.crs}\")\n</code></pre></p>"},{"location":"faq/#my-imagery-looks-distorted-during-segmentation","title":"My imagery looks distorted during segmentation","text":"<p>The segmentation algorithm works on pixel values, not geographic coordinates, so CRS should not affect segmentation quality. If you're seeing distortion:</p> <ol> <li>Check if it's a visualization issue: The distortion might only appear when viewing results in a different CRS</li> <li>Verify your input data: Use <code>gdalinfo</code> to check if the CRS metadata is correct</li> <li>Try reprojecting to a more suitable CRS: Some projections preserve shape better than others for specific regions (e.g., UTM for local areas)</li> </ol>"},{"location":"fast_sam/","title":"fast_sam module","text":"<p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM. https://github.com/opengeos/FastSAM</p>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo","title":"<code>SamGeo</code>","text":"<p>               Bases: <code>FastSAM</code></p> <p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>class SamGeo(FastSAM):\n    \"\"\"Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).\"\"\"\n\n    def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n        \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n        if \"checkpoint_dir\" in kwargs:\n            checkpoint_dir = kwargs[\"checkpoint_dir\"]\n            kwargs.pop(\"checkpoint_dir\")\n        else:\n            checkpoint_dir = os.environ.get(\n                \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n            )\n\n        models = {\n            \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n            \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n        }\n\n        if model not in models:\n            raise ValueError(\n                f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n            )\n\n        model_path = os.path.join(checkpoint_dir, model)\n\n        if not os.path.exists(model_path):\n            print(f\"Downloading {model} to {model_path}...\")\n            download_file(models[model], model_path)\n\n        super().__init__(model, **kwargs)\n\n    def set_image(self, image, device=None, **kwargs):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n            device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n            kwargs: Additional keyword arguments to pass to the FastSAM model.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        everything_results = self(image, device=device, **kwargs)\n\n        self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n\n    def everything_prompt(self, output=None, **kwargs):\n        \"\"\"Segment the image with the everything prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n        Args:\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.everything_prompt()\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def point_prompt(self, points, pointlabel, output=None, **kwargs):\n        \"\"\"Segment the image with the point prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n        Args:\n            points (list): A list of points.\n            pointlabel (list): A list of labels for each point.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.point_prompt(points, pointlabel)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n        \"\"\"Segment the image with the box prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n        Args:\n            bbox (list, optional): The bounding box. Defaults to None.\n            bboxes (list, optional): A list of bounding boxes. Defaults to None.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.box_prompt(bbox, bboxes)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def text_prompt(self, text, output=None, **kwargs):\n        \"\"\"Segment the image with the text prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n        Args:\n            text (str): The text to segment.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.text_prompt(text)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def save_masks(\n        self,\n        output=None,\n        better_quality=True,\n        dtype=None,\n        mask_multiplier=255,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Save the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n        annotations = self.annotations\n        if isinstance(annotations[0], dict):\n            annotations = [annotation[\"segmentation\"] for annotation in annotations]\n        image = self.prompt_process.img\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        height = image.shape[0]\n        width = image.shape[1]\n\n        if better_quality:\n            if isinstance(annotations[0], torch.Tensor):\n                annotations = np.array(annotations.cpu())\n            for i, mask in enumerate(annotations):\n                mask = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n                )\n                annotations[i] = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n                )\n        if self.device == \"cpu\":\n            annotations = np.array(annotations)\n\n        else:\n            if isinstance(annotations[0], np.ndarray):\n                annotations = torch.from_numpy(annotations)\n\n        if isinstance(annotations, torch.Tensor):\n            annotations = annotations.cpu().numpy()\n\n        if dtype is None:\n            # Set output image data type based on the number of objects\n            if len(annotations) &lt; 255:\n                dtype = np.uint8\n            elif len(annotations) &lt; 65535:\n                dtype = np.uint16\n            else:\n                dtype = np.uint32\n\n        masks = np.sum(annotations, axis=0)\n\n        masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n        masks[masks &gt; 0] = 1\n        masks = masks.astype(dtype) * mask_multiplier\n        self.objects = masks\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n        else:\n            return masks\n\n    def fast_show_mask(\n        self,\n        random_color=False,\n    ):\n        \"\"\"Show the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Args:\n            random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n        image = self.prompt_process.img\n        target_height = image.shape[0]\n        target_width = image.shape[1]\n        annotations = self.annotations\n        annotation = np.array(annotations.cpu())\n\n        mask_sum = annotation.shape[0]\n        height = annotation.shape[1]\n        weight = annotation.shape[2]\n        # Sort annotations based on area.\n        areas = np.sum(annotation, axis=(1, 2))\n        sorted_indices = np.argsort(areas)\n        annotation = annotation[sorted_indices]\n\n        index = (annotation != 0).argmax(axis=0)\n        if random_color:\n            color = np.random.random((mask_sum, 1, 1, 3))\n        else:\n            color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n                [30 / 255, 144 / 255, 255 / 255]\n            )\n        transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n        visual = np.concatenate([color, transparency], axis=-1)\n        mask_image = np.expand_dims(annotation, -1) * visual\n\n        show = np.zeros((height, weight, 4))\n        h_indices, w_indices = np.meshgrid(\n            np.arange(height), np.arange(weight), indexing=\"ij\"\n        )\n        indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n        # Use vectorized indexing to update the values of 'show'.\n        show[h_indices, w_indices, :] = mask_image[indices]\n\n        show = cv2.resize(\n            show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n        )\n\n        return show\n\n    def raster_to_vector(\n        self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n    ):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            image,\n            output,\n            simplify_tolerance=simplify_tolerance,\n            dst_crs=dst_crs,\n            **kwargs,\n        )\n\n    def show_anns(\n        self,\n        output=None,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        annotations = self.annotations\n        prompt_process = self.prompt_process\n\n        if output is None:\n            output = temp_file_path(\".png\")\n\n        prompt_process.plot(annotations, output, **kwargs)\n\n        show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.__init__","title":"<code>__init__(model='FastSAM-x.pt', **kwargs)</code>","text":"<p>Initialize the FastSAM algorithm.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n    \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n    if \"checkpoint_dir\" in kwargs:\n        checkpoint_dir = kwargs[\"checkpoint_dir\"]\n        kwargs.pop(\"checkpoint_dir\")\n    else:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    models = {\n        \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n        \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n    }\n\n    if model not in models:\n        raise ValueError(\n            f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n        )\n\n    model_path = os.path.join(checkpoint_dir, model)\n\n    if not os.path.exists(model_path):\n        print(f\"Downloading {model} to {model_path}...\")\n        download_file(models[model], model_path)\n\n    super().__init__(model, **kwargs)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.box_prompt","title":"<code>box_prompt(bbox=None, bboxes=None, output=None, **kwargs)</code>","text":"<p>Segment the image with the box prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list</code> <p>The bounding box. Defaults to None.</p> <code>None</code> <code>bboxes</code> <code>list</code> <p>A list of bounding boxes. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n    \"\"\"Segment the image with the box prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n    Args:\n        bbox (list, optional): The bounding box. Defaults to None.\n        bboxes (list, optional): A list of bounding boxes. Defaults to None.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.box_prompt(bbox, bboxes)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.everything_prompt","title":"<code>everything_prompt(output=None, **kwargs)</code>","text":"<p>Segment the image with the everything prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def everything_prompt(self, output=None, **kwargs):\n    \"\"\"Segment the image with the everything prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n    Args:\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.everything_prompt()\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.fast_show_mask","title":"<code>fast_show_mask(random_color=False)</code>","text":"<p>Show the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Parameters:</p> Name Type Description Default <code>random_color</code> <code>bool</code> <p>Whether to use random colors for each object. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>np.ndarray: The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def fast_show_mask(\n    self,\n    random_color=False,\n):\n    \"\"\"Show the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Args:\n        random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n    image = self.prompt_process.img\n    target_height = image.shape[0]\n    target_width = image.shape[1]\n    annotations = self.annotations\n    annotation = np.array(annotations.cpu())\n\n    mask_sum = annotation.shape[0]\n    height = annotation.shape[1]\n    weight = annotation.shape[2]\n    # Sort annotations based on area.\n    areas = np.sum(annotation, axis=(1, 2))\n    sorted_indices = np.argsort(areas)\n    annotation = annotation[sorted_indices]\n\n    index = (annotation != 0).argmax(axis=0)\n    if random_color:\n        color = np.random.random((mask_sum, 1, 1, 3))\n    else:\n        color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n            [30 / 255, 144 / 255, 255 / 255]\n        )\n    transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n    visual = np.concatenate([color, transparency], axis=-1)\n    mask_image = np.expand_dims(annotation, -1) * visual\n\n    show = np.zeros((height, weight, 4))\n    h_indices, w_indices = np.meshgrid(\n        np.arange(height), np.arange(weight), indexing=\"ij\"\n    )\n    indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n    # Use vectorized indexing to update the values of 'show'.\n    show[h_indices, w_indices, :] = mask_image[indices]\n\n    show = cv2.resize(\n        show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n    )\n\n    return show\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.point_prompt","title":"<code>point_prompt(points, pointlabel, output=None, **kwargs)</code>","text":"<p>Segment the image with the point prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list</code> <p>A list of points.</p> required <code>pointlabel</code> <code>list</code> <p>A list of labels for each point.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def point_prompt(self, points, pointlabel, output=None, **kwargs):\n    \"\"\"Segment the image with the point prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n    Args:\n        points (list): A list of points.\n        pointlabel (list): A list of labels for each point.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.point_prompt(points, pointlabel)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def raster_to_vector(\n    self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        image,\n        output,\n        simplify_tolerance=simplify_tolerance,\n        dst_crs=dst_crs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.save_masks","title":"<code>save_masks(output=None, better_quality=True, dtype=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    better_quality=True,\n    dtype=None,\n    mask_multiplier=255,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Save the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n    annotations = self.annotations\n    if isinstance(annotations[0], dict):\n        annotations = [annotation[\"segmentation\"] for annotation in annotations]\n    image = self.prompt_process.img\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    height = image.shape[0]\n    width = image.shape[1]\n\n    if better_quality:\n        if isinstance(annotations[0], torch.Tensor):\n            annotations = np.array(annotations.cpu())\n        for i, mask in enumerate(annotations):\n            mask = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n            )\n            annotations[i] = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n            )\n    if self.device == \"cpu\":\n        annotations = np.array(annotations)\n\n    else:\n        if isinstance(annotations[0], np.ndarray):\n            annotations = torch.from_numpy(annotations)\n\n    if isinstance(annotations, torch.Tensor):\n        annotations = annotations.cpu().numpy()\n\n    if dtype is None:\n        # Set output image data type based on the number of objects\n        if len(annotations) &lt; 255:\n            dtype = np.uint8\n        elif len(annotations) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n    masks = np.sum(annotations, axis=0)\n\n    masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n    masks[masks &gt; 0] = 1\n    masks = masks.astype(dtype) * mask_multiplier\n    self.objects = masks\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n    else:\n        return masks\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.set_image","title":"<code>set_image(image, device=None, **kwargs)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the FastSAM model.</p> <code>{}</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def set_image(self, image, device=None, **kwargs):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n        device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n        kwargs: Additional keyword arguments to pass to the FastSAM model.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    everything_results = self(image, device=device, **kwargs)\n\n    self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.show_anns","title":"<code>show_anns(output=None, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> required <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> required Source code in <code>samgeo/fast_sam.py</code> <pre><code>def show_anns(\n    self,\n    output=None,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    annotations = self.annotations\n    prompt_process = self.prompt_process\n\n    if output is None:\n        output = temp_file_path(\".png\")\n\n    prompt_process.plot(annotations, output, **kwargs)\n\n    show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.text_prompt","title":"<code>text_prompt(text, output=None, **kwargs)</code>","text":"<p>Segment the image with the text prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to segment.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def text_prompt(self, text, output=None, **kwargs):\n    \"\"\"Segment the image with the text prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n    Args:\n        text (str): The text to segment.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.text_prompt(text)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"hq_sam/","title":"hq_sam module","text":"<p>Segment remote sensing imagery with HQ-SAM (High Quality Segment Anything Model). See https://github.com/SysCV/sam-hq</p>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo","title":"<code>SamGeo</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        hq=False,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n\n        hq = True  # Using HQ-SAM\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n            )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in ascending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                objects[m] = index + 1\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__call__","title":"<code>__call__(image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__init__","title":"<code>__init__(model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, hq=False, sam_kwargs=None, **kwargs)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use the HQ-SAM model. Defaults to False.</p> <code>False</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    hq=False,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n\n    hq = True  # Using HQ-SAM\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache()</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.generate","title":"<code>generate(source, output=None, foreground=True, batch=False, erosion_kernel=None, mask_multiplier=255, unique=True, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in ascending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            objects[m] = index + 1\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype=np.float32, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>float32</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.set_image","title":"<code>set_image(image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: The predicted mask as a numpy array.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-with-pixi-recommended","title":"Install with pixi (Recommended)","text":"<p>Installing segment-geospatial with <code>uv</code> or <code>pip</code> can be challenging on some platforms (especially Windows) due to complicated pytorch/cuda dependencies and numpy version conflicts. Pixi is recommended to avoid these issues, as it provides faster and more reliable dependency resolution than conda or mamba.</p>"},{"location":"installation/#1-install-pixi","title":"1) Install Pixi","text":""},{"location":"installation/#linuxmacos-bashzsh","title":"Linux/macOS (bash/zsh)","text":"<pre><code>curl -fsSL https://pixi.sh/install.sh | sh\n</code></pre> <p>Close and re-open your terminal (or reload your shell) so <code>pixi</code> is on your <code>PATH</code>. Then confirm:</p> <pre><code>pixi --version\n</code></pre>"},{"location":"installation/#windows-powershell","title":"Windows (PowerShell)","text":"<p>Open PowerShell (preferably as a normal user, Admin not required), then run:</p> <pre><code>powershell -ExecutionPolicy Bypass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\n</code></pre> <p>Close and re-open PowerShell, then confirm:</p> <pre><code>pixi --version\n</code></pre>"},{"location":"installation/#2-create-a-pixi-project","title":"2) Create a Pixi project","text":"<p>Navigate to a directory where you want to create the project and run:</p> <pre><code>pixi init geo\ncd geo\n</code></pre>"},{"location":"installation/#3-configure-pixitoml","title":"3) Configure <code>pixi.toml</code>","text":"<p>Open <code>pixi.toml</code> in the <code>geo</code> directory and replace its contents with the following depending on your system.</p> <p>If you have an NVIDIA GPU with CUDA, run <code>nvidia-smi</code> to check the CUDA version.</p>"},{"location":"installation/#for-gpu-with-cuda-12x","title":"For GPU with CUDA 12.x:","text":"<pre><code>[workspace]\nchannels = [\"https://prefix.dev/conda-forge\"]\nname = \"geo\"\nplatforms = [\"linux-64\", \"win-64\"]\n\n[system-requirements]\ncuda = \"12.0\"\n\n[dependencies]\npython = \"3.12.*\"\npytorch-gpu = \"&gt;=2.7.1,&lt;3\"\nsegment-geospatial = \"&gt;=1.2.0\"\nsam3 = \"&gt;=0.1.0.20251211\"\njupyterlab = \"*\"\nipykernel = \"*\"\nlibopenblas = \"&gt;=0.3.30\"\n</code></pre>"},{"location":"installation/#for-gpu-with-cuda-13x","title":"For GPU with CUDA 13.x:","text":"<pre><code>[workspace]\nchannels = [\"https://prefix.dev/conda-forge\"]\nname = \"geo\"\nplatforms = [\"linux-64\", \"win-64\"]\n\n[system-requirements]\ncuda = \"13.0\"\n\n[dependencies]\npython = \"3.12.*\"\npytorch-gpu = \"&gt;=2.7.1,&lt;3\"\nsegment-geospatial = \"&gt;=1.2.0\"\nsam3 = \"&gt;=0.1.0.20251211\"\njupyterlab = \"*\"\nipykernel = \"*\"\n</code></pre>"},{"location":"installation/#for-cpu","title":"For CPU:","text":"<pre><code>[workspace]\nchannels = [\"https://prefix.dev/conda-forge\"]\nname = \"geo\"\nplatforms = [\"linux-64\", \"win-64\"]\n\n[dependencies]\npython = \"3.12.*\"\npytorch-cpu = \"&gt;=2.7.1,&lt;3\"\nsegment-geospatial = \"&gt;=1.2.0\"\nsam3 = \"&gt;=0.1.0.20251211\"\njupyterlab = \"*\"\nipykernel = \"*\"\nlibopenblas = \"&gt;=0.3.30\"\n</code></pre>"},{"location":"installation/#4-install-the-environment","title":"4) Install the environment","text":"<p>From the <code>geo</code> folder:</p> <pre><code>pixi install\n</code></pre> <p>This step may take several minutes on first install depending on your internet connection and system.</p>"},{"location":"installation/#5-verify-pytorch-cuda","title":"5) Verify PyTorch + CUDA","text":"<p>If you have an NVIDIA GPU with CUDA, run the following command to verify the PyTorch + CUDA installation:</p> <pre><code>pixi run python -c \"import torch; print('PyTorch:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('GPU:', (torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'))\"\n</code></pre> <p>Expected output should be like this:</p> <ul> <li><code>PyTorch: 2.7.1</code> (or higher)</li> <li><code>CUDA available: True</code></li> <li><code>GPU: NVIDIA RTX 4090</code> (your GPU name)</li> </ul> <p>If CUDA is <code>False</code>, check:</p> <ul> <li><code>nvidia-smi</code> works in your terminal</li> <li>NVIDIA driver is up to date</li> </ul>"},{"location":"installation/#6-request-access-to-sam-3-optional","title":"6) Request access to SAM 3 (Optional)","text":"<p>To use SAM 3, you will need to request access by filling out this form on Hugging Face at https://huggingface.co/facebook/sam3. Once your request has been approved, run the following command in the terminal to authenticate:</p> <pre><code>pixi run hf auth login\n</code></pre> <p>After authentication, you can download the SAM 3 model from Hugging Face:</p> <pre><code>pixi run hf download facebook/sam3\n</code></pre> <p>Important Note: SAM 3 currently requires an NVIDIA GPU with CUDA support. You won't be able to use SAM 3 if you have a CPU only system (source). You will get an error message like this: <code>Failed to load model: Torch not compiled with CUDA enabled</code>.</p>"},{"location":"installation/#7-start-jupyter-lab","title":"7) Start Jupyter Lab","text":"<p>To start using segment-geospatial in Jupyter Lab:</p> <pre><code>pixi run jupyter lab\n</code></pre> <p>This will open Jupyter Lab in your default browser. You can now create a new notebook and start using segment-geospatial!</p>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>segment-geospatial is available on PyPI. To install segment-geospatial, run this command in your terminal:</p> <pre><code>pip install segment-geospatial\n</code></pre>"},{"location":"installation/#install-from-conda-forge","title":"Install from conda-forge","text":"<p>segment-geospatial is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install segment-geospatial using the following commands. It is recommended to create a fresh conda environment for segment-geospatial. The following commands will create a new conda environment named <code>geo</code> and install segment-geospatial and its dependencies:</p> <pre><code>conda create -n geo python\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge segment-geospatial\n</code></pre> <p>If your system has a GPU, but the above commands do not install the GPU version of pytorch, you can force the installation of the GPU version of pytorch using the following command:</p> <pre><code>mamba install -c conda-forge segment-geospatial \"pytorch=*=cuda*\"\n</code></pre> <p>Samgeo-geospatial has some optional dependencies that are not included in the default conda environment. To install these dependencies, run the following command:</p> <pre><code>mamba install -c conda-forge groundingdino-py segment-anything-fast\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/segment-geospatial\n</code></pre>"},{"location":"installation/#use-docker","title":"Use docker","text":"<p>You can also use docker to run segment-geospatial:</p> <pre><code>docker run -it -p 8888:8888 giswqs/segment-geospatial:latest\n</code></pre> <p>To enable GPU for segment-geospatial, run the following command to run a short benchmark on your GPU:</p> <pre><code>docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance.\n        -fullscreen       (run n-body simulation in fullscreen mode)\n        -fp64             (use double precision floating point values for simulation)\n        -hostmem          (stores simulation data in host memory)\n        -benchmark        (run benchmark to measure performance)\n        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)\n        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)\n        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)\n        -compare          (compares simulation results running once on the default GPU and once on the CPU)\n        -cpu              (run n-body simulation on the CPU)\n        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n&gt; Windowed mode\n&gt; Simulation data stored in video memory\n&gt; Single precision floating point simulation\n&gt; 1 Devices used for simulation\nGPU Device 0: \"Turing\" with compute capability 7.5\n\n&gt; Compute 7.5 CUDA device: [Quadro RTX 5000]\n49152 bodies, total time for 10 iterations: 69.386 ms\n= 348.185 billion interactions per second\n= 6963.703 single-precision GFLOP/s at 20 flops per interaction\n</code></pre> <p>If you encounter the following error:</p> <pre><code>nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.\n</code></pre> <p>Try adding <code>sudo</code> to the command:</p> <pre><code>sudo docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>Once everything is working, you can run the following command to start a Jupyter Notebook server:</p> <pre><code>docker run -it -p 8888:8888 --gpus=all giswqs/segment-geospatial:latest\n</code></pre>"},{"location":"samgeo/","title":"samgeo module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"samgeo/#samgeo.samgeo.SamGeo","title":"<code>SamGeo</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n        hq = False  # Not using HQ-SAM\n\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.model_version = \"sam\"\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        batch_sample_size=(512, 512),\n        batch_nodata_threshold=1.0,\n        nodata_value=None,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        min_size=0,\n        max_size=None,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            batch_sample_size (tuple, optional): When batch=True, the size of the sample window when iterating over rasters.\n            batch_nodata_threshold (float,optional): Batch samples with a fraction of nodata pixels above this threshold will\n                not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful\n                when rasters have large areas of nodata values which can be skipped.\n            nodata_value (int, optional): Nodata value to use in checking batch_nodata_threshold. The default, None,\n                will use the nodata value in the raster metadata if present.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (int, optional): The maximum size of the objects. Defaults to None.\n            **kwargs: Other arguments for save_masks().\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    sample_size=batch_sample_size,\n                    sample_nodata_threshold=batch_nodata_threshold,\n                    nodata_value=nodata_value,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n        self._min_size = min_size\n        self._max_size = max_size\n\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **kwargs,\n        )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        min_size=0,\n        max_size=None,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (int, optional): The maximum size of the objects. Defaults to None.\n            **kwargs: Other arguments for array_to_image().\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in descending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            count = len(sorted_masks)\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and ann[\"area\"] &gt; max_size:\n                    continue\n                objects[m] = count - index\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and m[\"area\"] &gt; max_size:\n                    continue\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n                continue\n            if (\n                hasattr(self, \"_max_size\")\n                and isinstance(self._max_size, int)\n                and ann[\"area\"] &gt; self._max_size\n            ):\n                continue\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        # if \"dpi\" not in kwargs:\n        #     kwargs[\"dpi\"] = 100\n\n        # if \"bbox_inches\" not in kwargs:\n        #     kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source, **kwargs)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__call__","title":"<code>__call__(image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__init__","title":"<code>__init__(model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, sam_kwargs=None, **kwargs)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n    hq = False  # Not using HQ-SAM\n\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.model_version = \"sam\"\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache()</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.generate","title":"<code>generate(source, output=None, foreground=True, batch=False, batch_sample_size=(512, 512), batch_nodata_threshold=1.0, nodata_value=None, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>batch_sample_size</code> <code>tuple</code> <p>When batch=True, the size of the sample window when iterating over rasters.</p> <code>(512, 512)</code> <code>batch_nodata_threshold</code> <code>(float, optional)</code> <p>Batch samples with a fraction of nodata pixels above this threshold will not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful when rasters have large areas of nodata values which can be skipped.</p> <code>1.0</code> <code>nodata_value</code> <code>int</code> <p>Nodata value to use in checking batch_nodata_threshold. The default, None, will use the nodata value in the raster metadata if present.</p> <code>None</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    batch_sample_size=(512, 512),\n    batch_nodata_threshold=1.0,\n    nodata_value=None,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    min_size=0,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        batch_sample_size (tuple, optional): When batch=True, the size of the sample window when iterating over rasters.\n        batch_nodata_threshold (float,optional): Batch samples with a fraction of nodata pixels above this threshold will\n            not be used to generate a mask. The default, 1.0, will skip samples with 100% nodata values. This is useful\n            when rasters have large areas of nodata values which can be skipped.\n        nodata_value (int, optional): Nodata value to use in checking batch_nodata_threshold. The default, None,\n            will use the nodata value in the raster metadata if present.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (int, optional): The maximum size of the objects. Defaults to None.\n        **kwargs: Other arguments for save_masks().\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                sample_size=batch_sample_size,\n                sample_nodata_threshold=batch_nodata_threshold,\n                nodata_value=nodata_value,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n    self._min_size = min_size\n    self._max_size = max_size\n\n    # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n    self.save_masks(\n        output,\n        foreground,\n        unique,\n        erosion_kernel,\n        mask_multiplier,\n        min_size,\n        max_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Other arguments for array_to_image().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    min_size=0,\n    max_size=None,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (int, optional): The maximum size of the objects. Defaults to None.\n        **kwargs: Other arguments for array_to_image().\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in descending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        count = len(sorted_masks)\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and ann[\"area\"] &gt; max_size:\n                continue\n            objects[m] = count - index\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and m[\"area\"] &gt; max_size:\n                continue\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype=np.float32, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>float32</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.set_image","title":"<code>set_image(image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n            continue\n        if (\n            hasattr(self, \"_max_size\")\n            and isinstance(self._max_size, int)\n            and ann[\"area\"] &gt; self._max_size\n        ):\n            continue\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    # if \"dpi\" not in kwargs:\n    #     kwargs[\"dpi\"] = 100\n\n    # if \"bbox_inches\" not in kwargs:\n    #     kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: The predicted mask as a numpy array.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/","title":"samgeo2 module","text":""},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2","title":"<code>SamGeo2</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model 2 (SAM2). See https://github.com/facebookresearch/segment-anything-2 for details.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>class SamGeo2:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model 2 (SAM2). See\n    https://github.com/facebookresearch/segment-anything-2 for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"sam2-hiera-large\",\n        device: Optional[str] = None,\n        empty_cache: bool = True,\n        automatic: bool = True,\n        video: bool = False,\n        mode: str = \"eval\",\n        hydra_overrides_extra: Optional[List[str]] = None,\n        apply_postprocessing: bool = False,\n        points_per_side: Optional[int] = 32,\n        points_per_batch: int = 64,\n        pred_iou_thresh: float = 0.8,\n        stability_score_thresh: float = 0.95,\n        stability_score_offset: float = 1.0,\n        mask_threshold: float = 0.0,\n        box_nms_thresh: float = 0.7,\n        crop_n_layers: int = 0,\n        crop_nms_thresh: float = 0.7,\n        crop_overlap_ratio: float = 512 / 1500,\n        crop_n_points_downscale_factor: int = 1,\n        point_grids: Optional[List[np.ndarray]] = None,\n        min_mask_region_area: int = 0,\n        output_mode: str = \"binary_mask\",\n        use_m2m: bool = False,\n        multimask_output: bool = False,\n        max_hole_area: float = 0.0,\n        max_sprinkle_area: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SamGeo2 class.\n\n        Args:\n            model_id (str): The model ID to use. Can be one of the following: \"sam2-hiera-tiny\",\n                \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\".\n                Defaults to \"sam2-hiera-large\".\n            device (Optional[str]): The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.\n            empty_cache (bool): Whether to empty the cache. Defaults to True.\n            automatic (bool): Whether to use automatic mask generation. Defaults to True.\n            video (bool): Whether to use video prediction. Defaults to False.\n            mode (str): The mode to use. Defaults to \"eval\".\n            hydra_overrides_extra (Optional[List[str]]): Additional Hydra overrides. Defaults to None.\n            apply_postprocessing (bool): Whether to apply postprocessing. Defaults to False.\n            points_per_side (int or None): The number of points to be sampled\n                along one side of the image. The total number of points is\n                points_per_side**2. If None, 'point_grids' must provide explicit\n                point sampling.\n            points_per_batch (int): Sets the number of points run simultaneously\n                by the model. Higher numbers may be faster but use more GPU memory.\n            pred_iou_thresh (float): A filtering threshold in [0,1], using the\n                model's predicted mask quality.\n            stability_score_thresh (float): A filtering threshold in [0,1], using\n                the stability of the mask under changes to the cutoff used to binarize\n                the model's mask predictions.\n            stability_score_offset (float): The amount to shift the cutoff when\n                calculated the stability score.\n            mask_threshold (float): Threshold for binarizing the mask logits\n            box_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks.\n            crop_n_layers (int): If &gt;0, mask prediction will be run again on\n                crops of the image. Sets the number of layers to run, where each\n                layer has 2**i_layer number of image crops.\n            crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks between different crops.\n            crop_overlap_ratio (float): Sets the degree to which crops overlap.\n                In the first crop layer, crops will overlap by this fraction of\n                the image length. Later layers with more crops scale down this overlap.\n            crop_n_points_downscale_factor (int): The number of points-per-side\n                sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n            point_grids (list(np.ndarray) or None): A list over explicit grids\n                of points used for sampling, normalized to [0,1]. The nth grid in the\n                list is used in the nth crop layer. Exclusive with points_per_side.\n            min_mask_region_area (int): If &gt;0, postprocessing will be applied\n                to remove disconnected regions and holes in masks with area smaller\n                than min_mask_region_area. Requires opencv.\n            output_mode (str): The form masks are returned in. Can be 'binary_mask',\n                'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools.\n                For large resolutions, 'binary_mask' may consume large amounts of\n                memory.\n            use_m2m (bool): Whether to add a one step refinement using previous mask predictions.\n            multimask_output (bool): Whether to output multimask at each point of the grid.\n                Defaults to False.\n            max_hole_area (int): If max_hole_area &gt; 0, we fill small holes in up to\n                the maximum area of max_hole_area in low_res_masks.\n            max_sprinkle_area (int): If max_sprinkle_area &gt; 0, we remove small sprinkles up to\n                the maximum area of max_sprinkle_area in low_res_masks.\n            **kwargs (Any): Additional keyword arguments to pass to\n                SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().\n        \"\"\"\n\n        try:\n            from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n            from sam2.sam2_image_predictor import SAM2ImagePredictor\n            from sam2.sam2_video_predictor import SAM2VideoPredictor\n        except ImportError as e:\n            raise ImportError(\n                f\"To use SamGeo 2, install it as:\\n\\tpip install segment-geospatial[samgeo2]\"\n            )\n\n        if isinstance(model_id, str):\n            if not model_id.startswith(\"facebook/\"):\n                model_id = f\"facebook/{model_id}\"\n        else:\n            raise ValueError(\"model_id must be a string\")\n\n        allowed_models = [\n            \"facebook/sam2-hiera-tiny\",\n            \"facebook/sam2-hiera-small\",\n            \"facebook/sam2-hiera-base-plus\",\n            \"facebook/sam2-hiera-large\",\n        ]\n\n        if model_id not in allowed_models:\n            raise ValueError(\n                f\"model_id must be one of the following: {', '.join(allowed_models)}\"\n            )\n\n        if device is None:\n            device = common.choose_device(empty_cache=empty_cache)\n\n        if hydra_overrides_extra is None:\n            hydra_overrides_extra = []\n\n        self.model_id = model_id\n        self.model_version = \"sam2\"\n        self.device = device\n\n        if video:\n            automatic = False\n\n        if automatic:\n            self.mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                points_per_side=points_per_side,\n                points_per_batch=points_per_batch,\n                pred_iou_thresh=pred_iou_thresh,\n                stability_score_thresh=stability_score_thresh,\n                stability_score_offset=stability_score_offset,\n                mask_threshold=mask_threshold,\n                box_nms_thresh=box_nms_thresh,\n                crop_n_layers=crop_n_layers,\n                crop_nms_thresh=crop_nms_thresh,\n                crop_overlap_ratio=crop_overlap_ratio,\n                crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n                point_grids=point_grids,\n                min_mask_region_area=min_mask_region_area,\n                output_mode=output_mode,\n                use_m2m=use_m2m,\n                multimask_output=multimask_output,\n                **kwargs,\n            )\n        elif video:\n            self.predictor = SAM2VideoPredictor.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                **kwargs,\n            )\n        else:\n            self.predictor = SAM2ImagePredictor.from_pretrained(\n                model_id,\n                device=device,\n                mode=mode,\n                hydra_overrides_extra=hydra_overrides_extra,\n                apply_postprocessing=apply_postprocessing,\n                mask_threshold=mask_threshold,\n                max_hole_area=max_hole_area,\n                max_sprinkle_area=max_sprinkle_area,\n                **kwargs,\n            )\n\n    def generate(\n        self,\n        source: Union[str, np.ndarray],\n        output: Optional[str] = None,\n        foreground: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: int = None,\n        **kwargs: Any,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Generate masks for the input image.\n\n        Args:\n            source (Union[str, np.ndarray]): The path to the input image or the\n                input image as a numpy array.\n            output (Optional[str]): The path to the output image. Defaults to None.\n            foreground (bool): Whether to generate the foreground mask. Defaults\n                to True.\n            erosion_kernel (Optional[Tuple[int, int]]): The erosion kernel for\n                filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range,\n                for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool): Whether to assign a unique value to each object.\n                Defaults to True.\n                The unique value increases from 1 to the number of objects. The\n                larger the number, the larger the object area.\n            min_size (int): The minimum size of the object. Defaults to 0.\n            max_size (int): The maximum size of the object. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = common.download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self._min_size = min_size\n        self._max_size = max_size\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output,\n                foreground,\n                unique,\n                erosion_kernel,\n                mask_multiplier,\n                min_size,\n                max_size,\n                **kwargs,\n            )\n\n    def save_masks(\n        self,\n        output: Optional[str] = None,\n        foreground: bool = True,\n        unique: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        min_size: int = 0,\n        max_size: int = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the masks to the output path. The output is either a binary mask\n        or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to\n                None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask.\n                Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each\n                object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering\n                object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to\n                None.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1]. You can use this\n                parameter to scale the mask to a larger range, for example\n                [0, 255]. Defaults to 255.\n            min_size (int, optional): The minimum size of the object. Defaults to 0.\n            max_size (int, optional): The maximum size of the object. Defaults to None.\n            **kwargs: Additional keyword arguments for common.array_to_image().\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in descending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            count = len(sorted_masks)\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and ann[\"area\"] &gt; max_size:\n                    continue\n                objects[m] = count - index\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and m[\"area\"] &gt; max_size:\n                    continue\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            common.array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        cmap: str = \"binary_r\",\n        axis: str = \"off\",\n        foreground: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only.\n                Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        alpha: float = 0.35,\n        output: Optional[str] = None,\n        blend: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n                continue\n            if (\n                hasattr(self, \"_max_size\")\n                and isinstance(self._max_size, int)\n                and ann[\"area\"] &gt; self._max_size\n            ):\n                continue\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        # if \"dpi\" not in kwargs:\n        #     kwargs[\"dpi\"] = 100\n\n        # if \"bbox_inches\" not in kwargs:\n        #     kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = common.blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            common.array_to_image(array, output, self.source, **kwargs)\n\n    @torch.no_grad()\n    def set_image(\n        self,\n        image: Union[str, np.ndarray, Image],\n    ) -&gt; None:\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (Union[str, np.ndarray, Image]): The input image as a path,\n                a numpy array, or an Image.\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray) or isinstance(image, Image):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image)\n\n    @torch.no_grad()\n    def set_image_batch(\n        self,\n        image_list: List[Union[np.ndarray, str, Image]],\n    ) -&gt; None:\n        \"\"\"Set a batch of images for prediction.\n\n        Args:\n            image_list (List[Union[np.ndarray, str, Image]]): A list of images,\n            which can be numpy arrays, file paths, or PIL images.\n\n        Raises:\n            ValueError: If an input image path does not exist or if the input\n                image type is not supported.\n        \"\"\"\n        images = []\n        for image in image_list:\n            if isinstance(image, str):\n                if image.startswith(\"http\"):\n                    image = common.download_file(image)\n\n                if not os.path.exists(image):\n                    raise ValueError(f\"Input path {image} does not exist.\")\n\n                image = cv2.imread(image)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            elif isinstance(image, Image):\n                image = np.array(image)\n            elif isinstance(image, np.ndarray):\n                pass\n            else:\n                raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n            images.append(image)\n\n        self.predictor.set_image_batch(images)\n\n    def predict(\n        self,\n        point_coords: Optional[np.ndarray] = None,\n        point_labels: Optional[np.ndarray] = None,\n        boxes: Optional[np.ndarray] = None,\n        mask_input: Optional[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords: bool = True,\n        point_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"float32\",\n        return_results: bool = False,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predict the mask for the input image.\n\n        Args:\n            point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n            point_labels (np.ndarray, optional): The point labels. Defaults to None.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            multimask_output (bool, optional): Whether to output multimask at each\n                point of the grid. Defaults to False.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            normalize_coords (bool, optional): Whether to normalize the coordinates.\n                Defaults to True.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks,\n                scores, and logits. Defaults to False.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n                and the logits.\n        \"\"\"\n        import geopandas as gpd\n\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = common.vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = common.geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = common.coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = common.bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n\n        self.boxes = input_boxes\n\n        masks, scores, logits = predictor.predict(\n            point_coords=point_coords,\n            point_labels=point_labels,\n            box=input_boxes,\n            mask_input=mask_input,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def predict_by_points(\n        self,\n        point_coords_batch: List[np.ndarray] = None,\n        point_labels_batch: List[np.ndarray] = None,\n        box_batch: List[np.ndarray] = None,\n        mask_input_batch: List[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords=True,\n        point_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        index: Optional[int] = None,\n        unique: bool = True,\n        mask_multiplier: int = 255,\n        dtype: str = \"int32\",\n        return_results: bool = False,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predict the mask for the input image.\n\n        Args:\n            point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n            point_labels (np.ndarray, optional): The point labels. Defaults to None.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            multimask_output (bool, optional): Whether to output multimask at each\n                point of the grid. Defaults to True.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            normalize_coords (bool, optional): Whether to normalize the coordinates.\n                Defaults to True.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask,\n                which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.int32.\n            return_results (bool, optional): Whether to return the predicted masks,\n                scores, and logits. Defaults to False.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n                and the logits.\n        \"\"\"\n        import geopandas as gpd\n\n        if hasattr(self, \"image_batch\") and self.image_batch is not None:\n            pass\n        elif self.image is not None:\n            self.predictor.set_image_batch([self.image])\n            setattr(self, \"image_batch\", [self.image])\n        else:\n            raise ValueError(\"Please set the input image first using set_image().\")\n\n        if isinstance(point_coords_batch, dict):\n            point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n        if isinstance(point_coords_batch, str) or isinstance(\n            point_coords_batch, gpd.GeoDataFrame\n        ):\n            if isinstance(point_coords_batch, str):\n                gdf = gpd.read_file(point_coords_batch)\n            else:\n                gdf = point_coords_batch\n            if gdf.crs is None and (point_crs is not None):\n                gdf.crs = point_crs\n\n            points = gdf.geometry.apply(lambda geom: [geom.x, geom.y])\n            coordinates_array = np.array([[point] for point in points])\n            points = common.coords_to_xy(self.source, coordinates_array, point_crs)\n            num_points = points.shape[0]\n            if point_labels_batch is None:\n                labels = np.array([[1] for i in range(num_points)])\n            else:\n                labels = point_labels_batch\n\n        elif isinstance(point_coords_batch, list):\n            if point_crs is not None:\n                point_coords_batch_crs = common.coords_to_xy(\n                    self.source, point_coords_batch, point_crs\n                )\n            else:\n                point_coords_batch_crs = point_coords_batch\n            num_points = len(point_coords_batch)\n\n            points = []\n            points.append([[point] for point in point_coords_batch_crs])\n\n            if point_labels_batch is None:\n                labels = np.array([[1] for i in range(num_points)])\n            elif isinstance(point_labels_batch, list):\n                labels = []\n                labels.append([[label] for label in point_labels_batch])\n                labels = labels[0]\n            else:\n                labels = point_labels_batch\n\n            points = np.array(points[0])\n            labels = np.array(labels)\n\n        elif isinstance(point_coords_batch, np.ndarray):\n            points = point_coords_batch\n            labels = point_labels_batch\n        else:\n            raise ValueError(\"point_coords must be a list, a GeoDataFrame, or a path.\")\n\n        predictor = self.predictor\n\n        masks_batch, scores_batch, logits_batch = predictor.predict_batch(\n            point_coords_batch=[points],\n            point_labels_batch=[labels],\n            box_batch=box_batch,\n            mask_input_batch=mask_input_batch,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        masks = masks_batch[0]\n        scores = scores_batch[0]\n        logits = logits_batch[0]\n\n        if multimask_output and (index is not None):\n            masks = masks[:, index, :, :]\n\n        if masks.ndim &gt; 3:\n            masks = masks.squeeze()\n\n        output_masks = []\n        sums = np.sum(masks, axis=(1, 2))\n        for index, mask in enumerate(masks):\n            item = {\"segmentation\": mask.astype(\"bool\"), \"area\": sums[index]}\n            output_masks.append(item)\n\n        self.masks = output_masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            self.save_masks(\n                output,\n                foreground=True,\n                unique=unique,\n                mask_multiplier=mask_multiplier,\n                dtype=dtype,\n                **kwargs,\n            )\n\n        if return_results:\n            return output_masks, scores, logits\n\n    def predict_batch(\n        self,\n        point_coords_batch: List[np.ndarray] = None,\n        point_labels_batch: List[np.ndarray] = None,\n        box_batch: List[np.ndarray] = None,\n        mask_input_batch: List[np.ndarray] = None,\n        multimask_output: bool = False,\n        return_logits: bool = False,\n        normalize_coords=True,\n    ) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Predict masks for a batch of images.\n\n        Args:\n            point_coords_batch (Optional[List[np.ndarray]]): A batch of point\n                coordinates. Defaults to None.\n            point_labels_batch (Optional[List[np.ndarray]]): A batch of point\n                labels. Defaults to None.\n            box_batch (Optional[List[np.ndarray]]): A batch of bounding boxes.\n                Defaults to None.\n            mask_input_batch (Optional[List[np.ndarray]]): A batch of mask inputs.\n                Defaults to None.\n            multimask_output (bool): Whether to output multimask at each point\n                of the grid. Defaults to False.\n            return_logits (bool): Whether to return the logits. Defaults to False.\n            normalize_coords (bool): Whether to normalize the coordinates.\n                Defaults to True.\n\n        Returns:\n            Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists\n                of masks, multimasks, and logits.\n        \"\"\"\n\n        return self.predictor.predict_batch(\n            point_coords_batch=point_coords_batch,\n            point_labels_batch=point_labels_batch,\n            box_batch=box_batch,\n            mask_input_batch=mask_input_batch,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n    @torch.inference_mode()\n    def init_state(\n        self,\n        video_path: str,\n        offload_video_to_cpu: bool = False,\n        offload_state_to_cpu: bool = False,\n        async_loading_frames: bool = False,\n    ) -&gt; Any:\n        \"\"\"Initialize an inference state.\n\n        Args:\n            video_path (str): The path to the video file.\n            offload_video_to_cpu (bool): Whether to offload the video to CPU.\n                Defaults to False.\n            offload_state_to_cpu (bool): Whether to offload the state to CPU.\n                Defaults to False.\n            async_loading_frames (bool): Whether to load frames asynchronously.\n                Defaults to False.\n\n        Returns:\n            Any: The initialized inference state.\n        \"\"\"\n        return self.predictor.init_state(\n            video_path,\n            offload_video_to_cpu=offload_video_to_cpu,\n            offload_state_to_cpu=offload_state_to_cpu,\n            async_loading_frames=async_loading_frames,\n        )\n\n    @torch.inference_mode()\n    def reset_state(self, inference_state: Any) -&gt; None:\n        \"\"\"Remove all input points or masks in all frames throughout the video.\n\n        Args:\n            inference_state (Any): The current inference state.\n        \"\"\"\n        self.predictor.reset_state(inference_state)\n\n    @torch.inference_mode()\n    def add_new_points_or_box(\n        self,\n        inference_state: Any,\n        frame_idx: int,\n        obj_id: int,\n        points: Optional[np.ndarray] = None,\n        labels: Optional[np.ndarray] = None,\n        clear_old_points: bool = True,\n        normalize_coords: bool = True,\n        box: Optional[np.ndarray] = None,\n    ) -&gt; Any:\n        \"\"\"Add new points or a box to the inference state.\n\n        Args:\n            inference_state (Any): The current inference state.\n            frame_idx (int): The frame index.\n            obj_id (int): The object ID.\n            points (Optional[np.ndarray]): The points to add. Defaults to None.\n            labels (Optional[np.ndarray]): The labels for the points. Defaults to None.\n            clear_old_points (bool): Whether to clear old points. Defaults to True.\n            normalize_coords (bool): Whether to normalize the coordinates. Defaults to True.\n            box (Optional[np.ndarray]): The bounding box to add. Defaults to None.\n\n        Returns:\n            Any: The updated inference state.\n        \"\"\"\n        return self.predictor.add_new_points_or_box(\n            inference_state,\n            frame_idx,\n            obj_id,\n            points=points,\n            labels=labels,\n            clear_old_points=clear_old_points,\n            normalize_coords=normalize_coords,\n            box=box,\n        )\n\n    @torch.inference_mode()\n    def add_new_mask(\n        self,\n        inference_state: Any,\n        frame_idx: int,\n        obj_id: int,\n        mask: np.ndarray,\n    ) -&gt; Any:\n        \"\"\"Add a new mask to the inference state.\n\n        Args:\n            inference_state (Any): The current inference state.\n            frame_idx (int): The frame index.\n            obj_id (int): The object ID.\n            mask (np.ndarray): The mask to add.\n\n        Returns:\n            Any: The updated inference state.\n        \"\"\"\n        return self.predictor.add_new_mask(inference_state, frame_idx, obj_id, mask)\n\n    @torch.inference_mode()\n    def propagate_in_video_preflight(self, inference_state: Any) -&gt; Any:\n        \"\"\"Propagate the inference state in video preflight.\n\n        Args:\n            inference_state (Any): The current inference state.\n\n        Returns:\n            Any: The propagated inference state.\n        \"\"\"\n        return self.predictor.propagate_in_video_preflight(inference_state)\n\n    @torch.inference_mode()\n    def propagate_in_video(\n        self,\n        inference_state: Any,\n        start_frame_idx: Optional[int] = None,\n        max_frame_num_to_track: Optional[int] = None,\n        reverse: bool = False,\n    ) -&gt; Any:\n        \"\"\"Propagate the inference state in video.\n\n        Args:\n            inference_state (Any): The current inference state.\n            start_frame_idx (Optional[int]): The starting frame index. Defaults to None.\n            max_frame_num_to_track (Optional[int]): The maximum number of frames\n                to track. Defaults to None.\n            reverse (bool): Whether to propagate in reverse. Defaults to False.\n\n        Returns:\n            Any: The propagated inference state.\n        \"\"\"\n        return self.predictor.propagate_in_video(\n            inference_state,\n            start_frame_idx=start_frame_idx,\n            max_frame_num_to_track=max_frame_num_to_track,\n            reverse=reverse,\n        )\n\n    def tensor_to_numpy(\n        self,\n        index: Optional[int] = None,\n        output: Optional[str] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"uint8\",\n        save_args: Optional[Dict[str, Any]] = None,\n    ) -&gt; Optional[np.ndarray]:\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (Optional[int], optional): The index of the mask to save.\n                Defaults to None, which will save the mask with the highest score.\n            output (Optional[str], optional): The path to the output image.\n                Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1].\n            dtype (str, optional): The data type of the output image. Defaults\n                to \"uint8\".\n            save_args (Optional[Dict[str, Any]], optional): Optional arguments\n                for saving the output image. Defaults to None.\n\n        Returns:\n            Optional[np.ndarray]: The predicted mask as a numpy array, or None\n                if output is specified.\n        \"\"\"\n        if save_args is None:\n            save_args = {}\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 0\n\n        masks = masks[:, index, :, :]\n        if len(masks.shape) == 4 and masks.shape[1] == 1:\n            masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (_, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            common.array_to_image(\n                mask_overlay, output, self.source, dtype=dtype, **save_args\n            )\n        else:\n            return mask_overlay\n\n    def save_prediction(\n        self,\n        output: str,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"float32\",\n        vector: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (Optional[int], optional): The index of the mask to save.\n                Defaults to None, which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output\n                mask, which is usually a binary mask [0, 1].\n            dtype (str, optional): The data type of the output image. Defaults\n                to \"float32\".\n            vector (Optional[str], optional): The path to the output vector file.\n                Defaults to None.\n            simplify_tolerance (Optional[float], optional): The maximum allowed\n                geometry displacement. The higher this value, the smaller the\n                number of vertices in the resulting geometry.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            common.raster_to_vector(\n                output, vector, simplify_tolerance=simplify_tolerance\n            )\n\n    def show_map(\n        self,\n        basemap: str = \"SATELLITE\",\n        repeat_mode: bool = True,\n        out_dir: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following:\n                SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for\n                draw control. Defaults to True.\n            out_dir (Optional[str], optional): The path to the output directory.\n                Defaults to None.\n\n        Returns:\n            Any: The map object.\n        \"\"\"\n        return common.sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(\n        self,\n        fg_color: Tuple[int, int, int] = (0, 255, 0),\n        bg_color: Tuple[int, int, int] = (0, 0, 255),\n        radius: int = 5,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            fg_color (Tuple[int, int, int], optional): The color for the foreground points.\n                Defaults to (0, 255, 0).\n            bg_color (Tuple[int, int, int], optional): The color for the background points.\n                Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            Tuple[list, list]: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def _convert_prompts(self, prompts: Dict[int, Any]) -&gt; Dict[int, Any]:\n        \"\"\"Convert the points and labels in the prompts to numpy arrays with specific data types.\n\n        Args:\n            prompts (Dict[str, Any]): A dictionary containing the prompts with points and labels.\n\n        Returns:\n            Dict[str, Any]: The updated dictionary with points and labels converted to numpy arrays.\n        \"\"\"\n        for _, value in prompts.items():\n            # Convert points to np.float32 array\n            if \"points\" in value:\n                value[\"points\"] = np.array(value[\"points\"], dtype=np.float32)\n            # Convert labels to np.int32 array\n            if \"labels\" in value:\n                value[\"labels\"] = np.array(value[\"labels\"], dtype=np.int32)\n            # Convert box to np.float32 array\n            if \"box\" in value:\n                value[\"box\"] = np.array(value[\"box\"], dtype=np.float32)\n\n        return prompts\n\n    def set_video(\n        self,\n        video_path: str,\n        output_dir: str = None,\n        frame_rate: Optional[int] = None,\n        prefix: str = \"\",\n    ) -&gt; None:\n        \"\"\"Set the video path and parameters.\n\n        Args:\n            video_path (str): The path to the video file.\n            start_frame (int, optional): The starting frame index. Defaults to 0.\n            end_frame (Optional[int], optional): The ending frame index. Defaults to None.\n            step (int, optional): The step size. Defaults to 1.\n            frame_rate (Optional[int], optional): The frame rate. Defaults to None.\n        \"\"\"\n\n        if isinstance(video_path, str):\n            if video_path.startswith(\"http\"):\n                video_path = common.download_file(video_path)\n            if os.path.isfile(video_path):\n                if output_dir is None:\n                    output_dir = common.make_temp_dir()\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                print(f\"Output directory: {output_dir}\")\n                common.video_to_images(\n                    video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n                )\n\n            elif os.path.isdir(video_path):\n                files = sorted(os.listdir(video_path))\n                if len(files) == 0:\n                    raise ValueError(f\"No files found in {video_path}.\")\n                elif files[0].endswith(\".tif\"):\n                    self._tif_source = os.path.join(video_path, files[0])\n                    self._tif_dir = video_path\n                    self._tif_names = files\n                    video_path = common.geotiff_to_jpg_batch(video_path)\n                output_dir = video_path\n\n            if not os.path.exists(video_path):\n                raise ValueError(f\"Input path {video_path} does not exist.\")\n        else:\n            raise ValueError(\"Input video_path must be a string.\")\n\n        self.video_path = output_dir\n        self._num_images = len(os.listdir(output_dir))\n        self._frame_names = sorted(os.listdir(output_dir))\n        self.inference_state = self.predictor.init_state(video_path=output_dir)\n\n    def predict_video(\n        self,\n        prompts: Dict[int, Any] = None,\n        point_crs: Optional[str] = None,\n        output_dir: Optional[str] = None,\n        img_ext: str = \"png\",\n    ) -&gt; None:\n        \"\"\"Predict masks for the video.\n\n        Args:\n            prompts (Dict[int, Any]): A dictionary containing the prompts with points and labels.\n            point_crs (Optional[str]): The coordinate reference system (CRS) of the point prompts.\n            output_dir (Optional[str]): The directory to save the output images. Defaults to None.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n        \"\"\"\n\n        from PIL import Image\n\n        def save_image_from_dict(data, output_path=\"output_image.png\"):\n            # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n            array_shape = next(iter(data.values())).shape[1:]\n\n            # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n            output_array = np.zeros(array_shape, dtype=np.uint8)\n\n            # Iterate over each key and array in the dictionary\n            for key, array in data.items():\n                # Assign the key value wherever the boolean array is True\n                output_array[array[0]] = key\n\n            # Convert the output array to a PIL image\n            image = Image.fromarray(output_array)\n\n            # Save the image\n            image.save(output_path)\n\n        if prompts is None:\n            if hasattr(self, \"prompts\"):\n                prompts = self.prompts\n            else:\n                raise ValueError(\"Please provide prompts.\")\n\n        if point_crs is not None and self._tif_source is not None:\n            for prompt in prompts.values():\n                points = prompt.get(\"points\", None)\n                if points is not None:\n                    points = common.coords_to_xy(self._tif_source, points, point_crs)\n                    prompt[\"points\"] = points\n                box = prompt.get(\"box\", None)\n                if box is not None:\n                    box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                    prompt[\"box\"] = box\n\n        prompts = self._convert_prompts(prompts)\n        predictor = self.predictor\n        inference_state = self.inference_state\n        for obj_id, prompt in prompts.items():\n            points = prompt.get(\"points\", None)\n            labels = prompt.get(\"labels\", None)\n            box = prompt.get(\"box\", None)\n            frame_idx = prompt.get(\"frame_idx\", None)\n\n            _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n                inference_state=inference_state,\n                frame_idx=frame_idx,\n                obj_id=obj_id,\n                points=points,\n                labels=labels,\n                box=box,\n            )\n\n        video_segments = {}\n        num_frames = self._num_images\n        num_digits = len(str(num_frames))\n\n        if output_dir is not None:\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n\n        for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n            inference_state\n        ):\n            video_segments[out_frame_idx] = {\n                out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                for i, out_obj_id in enumerate(out_obj_ids)\n            }\n\n            if output_dir is not None:\n                output_path = os.path.join(\n                    output_dir, f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n                )\n                save_image_from_dict(video_segments[out_frame_idx], output_path)\n\n        self.video_segments = video_segments\n\n        # if output_dir is not None:\n        #     self.save_video_segments(output_dir, img_ext)\n\n    def save_video_segments(self, output_dir: str, img_ext: str = \"png\") -&gt; None:\n        \"\"\"Save the video segments to the output directory.\n\n        Args:\n            output_dir (str): The path to the output directory.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n        \"\"\"\n        from PIL import Image\n\n        def save_image_from_dict(\n            data, output_path=\"output_image.png\", crs_source=None, **kwargs\n        ):\n            # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n            array_shape = next(iter(data.values())).shape[1:]\n\n            # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n            output_array = np.zeros(array_shape, dtype=np.uint8)\n\n            # Iterate over each key and array in the dictionary\n            for key, array in data.items():\n                # Assign the key value wherever the boolean array is True\n                output_array[array[0]] = key\n\n            if crs_source is None:\n                # Convert the output array to a PIL image\n                image = Image.fromarray(output_array)\n\n                # Save the image\n                image.save(output_path)\n            else:\n                output_path = output_path.replace(\".png\", \".tif\")\n                common.array_to_image(output_array, output_path, crs_source, **kwargs)\n\n        num_frames = len(self.video_segments)\n        num_digits = len(str(num_frames))\n\n        if hasattr(self, \"_tif_source\") and self._tif_source.endswith(\".tif\"):\n            crs_source = self._tif_source\n            filenames = self._tif_names\n        else:\n            crs_source = None\n            filenames = None\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        # Initialize the tqdm progress bar\n        for frame_idx, video_segment in tqdm(\n            self.video_segments.items(), desc=\"Rendering frames\", total=num_frames\n        ):\n            if filenames is None:\n                output_path = os.path.join(\n                    output_dir, f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n                )\n            else:\n                output_path = os.path.join(output_dir, filenames[frame_idx])\n            save_image_from_dict(video_segment, output_path, crs_source)\n\n    def save_video_segments_blended(\n        self,\n        output_dir: str,\n        img_ext: str = \"png\",\n        alpha: float = 0.6,\n        dpi: int = 200,\n        frame_stride: int = 1,\n        output_video: Optional[str] = None,\n        fps: int = 30,\n    ) -&gt; None:\n        \"\"\"Save blended video segments to the output directory and optionally create a video.\n\n        Args:\n            output_dir (str): The directory to save the output images.\n            img_ext (str): The file extension for the output images. Defaults to \"png\".\n            alpha (float): The alpha value for the blended masks. Defaults to 0.6.\n\n            dpi (int): The DPI (dots per inch) for the output images. Defaults to 200.\n            frame_stride (int): The stride for selecting frames to save. Defaults to 1.\n            output_video (Optional[str]): The path to the output video file. Defaults to None.\n            fps (int): The frames per second for the output video. Defaults to 30.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        from PIL import Image\n\n        def show_mask(mask, ax, obj_id=None, random_color=False):\n            if random_color:\n                color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n            else:\n                cmap = plt.get_cmap(\"tab10\")\n                cmap_idx = 0 if obj_id is None else obj_id\n                color = np.array([*cmap(cmap_idx)[:3], alpha])\n            h, w = mask.shape[-2:]\n            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n            ax.imshow(mask_image)\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        plt.close(\"all\")\n\n        video_segments = self.video_segments\n        video_dir = self.video_path\n        frame_names = self._frame_names\n        num_frames = len(frame_names)\n        num_digits = len(str(num_frames))\n\n        # Initialize the tqdm progress bar\n        for out_frame_idx in tqdm(\n            range(0, len(frame_names), frame_stride), desc=\"Rendering frames\"\n        ):\n            image = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n\n            # Get original image dimensions\n            w, h = image.size\n\n            # Set DPI and calculate figure size based on the original image dimensions\n            figsize = (\n                w / dpi,\n                h / dpi,\n            )\n            figsize = (\n                figsize[0] * 1.3,\n                figsize[1] * 1.3,\n            )\n\n            # Create a figure with the exact size and DPI\n            fig = plt.figure(figsize=figsize, dpi=dpi)\n\n            # Disable axis to prevent whitespace\n            plt.axis(\"off\")\n\n            # Display the original image\n            plt.imshow(image)\n\n            # Overlay masks for each object ID\n            for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n                show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n\n            # Save the figure with no borders or extra padding\n            filename = f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n            filepath = os.path.join(output_dir, filename)\n            plt.savefig(filepath, dpi=dpi, pad_inches=0, bbox_inches=\"tight\")\n            plt.close(fig)\n\n        if output_video is not None:\n            common.images_to_video(output_dir, output_video, fps=fps)\n\n    def show_images(self, path: str = None) -&gt; None:\n        \"\"\"Show the images in the video.\n\n        Args:\n            path (str, optional): The path to the images. Defaults to None.\n        \"\"\"\n        if path is None:\n            path = self.video_path\n\n        if path is not None:\n            common.show_image_gui(path)\n\n    def show_prompts(\n        self,\n        prompts: Dict[int, Any],\n        frame_idx: int = 0,\n        mask: Any = None,\n        random_color: bool = False,\n        point_crs: Optional[str] = None,\n        figsize: Tuple[int, int] = (9, 6),\n    ) -&gt; None:\n        \"\"\"Show the prompts on the image.\n\n        Args:\n            prompts (Dict[int, Any]): A dictionary containing the prompts with\n                points and labels.\n            frame_idx (int, optional): The frame index. Defaults to 0.\n            mask (Any, optional): The mask. Defaults to None.\n            random_color (bool, optional): Whether to use random colors for the\n                masks. Defaults to False.\n            point_crs (Optional[str], optional): The coordinate reference system\n            figsize (Tuple[int, int], optional): The figure size. Defaults to (9, 6).\n\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n        from PIL import Image\n\n        def show_mask(mask, ax, obj_id=None, random_color=random_color):\n            if random_color:\n                color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n            else:\n                cmap = plt.get_cmap(\"tab10\")\n                cmap_idx = 0 if obj_id is None else obj_id\n                color = np.array([*cmap(cmap_idx)[:3], 0.6])\n            h, w = mask.shape[-2:]\n            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n            ax.imshow(mask_image)\n\n        def show_points(coords, labels, ax, marker_size=200):\n            pos_points = coords[labels == 1]\n            neg_points = coords[labels == 0]\n            ax.scatter(\n                pos_points[:, 0],\n                pos_points[:, 1],\n                color=\"green\",\n                marker=\"*\",\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n            ax.scatter(\n                neg_points[:, 0],\n                neg_points[:, 1],\n                color=\"red\",\n                marker=\"*\",\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n\n        def show_box(box, ax):\n            x0, y0 = box[0], box[1]\n            w, h = box[2] - box[0], box[3] - box[1]\n            ax.add_patch(\n                plt.Rectangle(\n                    (x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2\n                )\n            )\n\n        if point_crs is not None and self._tif_source is not None:\n            for prompt in prompts.values():\n                points = prompt.get(\"points\", None)\n                if points is not None:\n                    points = common.coords_to_xy(self._tif_source, points, point_crs)\n                    prompt[\"points\"] = points\n                box = prompt.get(\"box\", None)\n                if box is not None:\n                    box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                    prompt[\"box\"] = box\n\n        prompts = self._convert_prompts(prompts)\n        self.prompts = prompts\n        video_dir = self.video_path\n        frame_names = self._frame_names\n        fig = plt.figure(figsize=figsize)\n        fig.canvas.toolbar_visible = True\n        fig.canvas.header_visible = False\n        fig.canvas.footer_visible = True\n        plt.title(f\"frame {frame_idx}\")\n        plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n\n        for obj_id, prompt in prompts.items():\n            points = prompt.get(\"points\", None)\n            labels = prompt.get(\"labels\", None)\n            box = prompt.get(\"box\", None)\n            anno_frame_idx = prompt.get(\"frame_idx\", None)\n            if anno_frame_idx == frame_idx:\n                if points is not None:\n                    show_points(points, labels, plt.gca())\n                if box is not None:\n                    show_box(box, plt.gca())\n                if mask is not None:\n                    show_mask(mask, plt.gca(), obj_id=obj_id)\n\n        plt.show()\n\n    def raster_to_vector(self, raster, vector, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a raster image file to a vector dataset.\n\n        Args:\n            raster (str): The path to the raster image.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        common.raster_to_vector(\n            raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def region_groups(\n        self,\n        image: Union[str, \"xr.DataArray\", np.ndarray],\n        connectivity: int = 1,\n        min_size: int = 10,\n        max_size: Optional[int] = None,\n        threshold: Optional[int] = None,\n        properties: Optional[List[str]] = None,\n        intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n        out_csv: Optional[str] = None,\n        out_vector: Optional[str] = None,\n        out_image: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[\n        Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n    ]:\n        \"\"\"\n        Segment regions in an image and filter them based on size.\n\n        Args:\n            image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n                path, xarray DataArray, or numpy array.\n            connectivity (int, optional): Connectivity for labeling. Defaults to 1\n                for 4-connectivity. Use 2 for 8-connectivity.\n            min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n            max_size (Optional[int], optional): Maximum size of regions to keep.\n                Defaults to None.\n            threshold (Optional[int], optional): Threshold for filling holes.\n                Defaults to None, which is equal to min_size.\n            properties (Optional[List[str]], optional): List of properties to measure.\n                See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n                Defaults to None.\n            intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n                Intensity image to use for properties. Defaults to None.\n            out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n                Defaults to None.\n            out_vector (Optional[str], optional): Path to save the vector file.\n                Defaults to None.\n            out_image (Optional[str], optional): Path to save the output image.\n                Defaults to None.\n\n        Returns:\n            Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n        \"\"\"\n        return common.region_groups(\n            image,\n            connectivity=connectivity,\n            min_size=min_size,\n            max_size=max_size,\n            threshold=threshold,\n            properties=properties,\n            intensity_image=intensity_image,\n            out_csv=out_csv,\n            out_vector=out_vector,\n            out_image=out_image,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.__init__","title":"<code>__init__(model_id='sam2-hiera-large', device=None, empty_cache=True, automatic=True, video=False, mode='eval', hydra_overrides_extra=None, apply_postprocessing=False, points_per_side=32, points_per_batch=64, pred_iou_thresh=0.8, stability_score_thresh=0.95, stability_score_offset=1.0, mask_threshold=0.0, box_nms_thresh=0.7, crop_n_layers=0, crop_nms_thresh=0.7, crop_overlap_ratio=512 / 1500, crop_n_points_downscale_factor=1, point_grids=None, min_mask_region_area=0, output_mode='binary_mask', use_m2m=False, multimask_output=False, max_hole_area=0.0, max_sprinkle_area=0.0, **kwargs)</code>","text":"<p>Initializes the SamGeo2 class.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model ID to use. Can be one of the following: \"sam2-hiera-tiny\", \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\". Defaults to \"sam2-hiera-large\".</p> <code>'sam2-hiera-large'</code> <code>device</code> <code>Optional[str]</code> <p>The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.</p> <code>None</code> <code>empty_cache</code> <code>bool</code> <p>Whether to empty the cache. Defaults to True.</p> <code>True</code> <code>automatic</code> <code>bool</code> <p>Whether to use automatic mask generation. Defaults to True.</p> <code>True</code> <code>video</code> <code>bool</code> <p>Whether to use video prediction. Defaults to False.</p> <code>False</code> <code>mode</code> <code>str</code> <p>The mode to use. Defaults to \"eval\".</p> <code>'eval'</code> <code>hydra_overrides_extra</code> <code>Optional[List[str]]</code> <p>Additional Hydra overrides. Defaults to None.</p> <code>None</code> <code>apply_postprocessing</code> <code>bool</code> <p>Whether to apply postprocessing. Defaults to False.</p> <code>False</code> <code>points_per_side</code> <code>int or None</code> <p>The number of points to be sampled along one side of the image. The total number of points is points_per_side**2. If None, 'point_grids' must provide explicit point sampling.</p> <code>32</code> <code>points_per_batch</code> <code>int</code> <p>Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.</p> <code>64</code> <code>pred_iou_thresh</code> <code>float</code> <p>A filtering threshold in [0,1], using the model's predicted mask quality.</p> <code>0.8</code> <code>stability_score_thresh</code> <code>float</code> <p>A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.</p> <code>0.95</code> <code>stability_score_offset</code> <code>float</code> <p>The amount to shift the cutoff when calculated the stability score.</p> <code>1.0</code> <code>mask_threshold</code> <code>float</code> <p>Threshold for binarizing the mask logits</p> <code>0.0</code> <code>box_nms_thresh</code> <code>float</code> <p>The box IoU cutoff used by non-maximal suppression to filter duplicate masks.</p> <code>0.7</code> <code>crop_n_layers</code> <code>int</code> <p>If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.</p> <code>0</code> <code>crop_nms_thresh</code> <code>float</code> <p>The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.</p> <code>0.7</code> <code>crop_overlap_ratio</code> <code>float</code> <p>Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.</p> <code>512 / 1500</code> <code>crop_n_points_downscale_factor</code> <code>int</code> <p>The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.</p> <code>1</code> <code>point_grids</code> <code>list(ndarray) or None</code> <p>A list over explicit grids of points used for sampling, normalized to [0,1]. The nth grid in the list is used in the nth crop layer. Exclusive with points_per_side.</p> <code>None</code> <code>min_mask_region_area</code> <code>int</code> <p>If &gt;0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than min_mask_region_area. Requires opencv.</p> <code>0</code> <code>output_mode</code> <code>str</code> <p>The form masks are returned in. Can be 'binary_mask', 'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools. For large resolutions, 'binary_mask' may consume large amounts of memory.</p> <code>'binary_mask'</code> <code>use_m2m</code> <code>bool</code> <p>Whether to add a one step refinement using previous mask predictions.</p> <code>False</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>max_hole_area</code> <code>int</code> <p>If max_hole_area &gt; 0, we fill small holes in up to the maximum area of max_hole_area in low_res_masks.</p> <code>0.0</code> <code>max_sprinkle_area</code> <code>int</code> <p>If max_sprinkle_area &gt; 0, we remove small sprinkles up to the maximum area of max_sprinkle_area in low_res_masks.</p> <code>0.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"sam2-hiera-large\",\n    device: Optional[str] = None,\n    empty_cache: bool = True,\n    automatic: bool = True,\n    video: bool = False,\n    mode: str = \"eval\",\n    hydra_overrides_extra: Optional[List[str]] = None,\n    apply_postprocessing: bool = False,\n    points_per_side: Optional[int] = 32,\n    points_per_batch: int = 64,\n    pred_iou_thresh: float = 0.8,\n    stability_score_thresh: float = 0.95,\n    stability_score_offset: float = 1.0,\n    mask_threshold: float = 0.0,\n    box_nms_thresh: float = 0.7,\n    crop_n_layers: int = 0,\n    crop_nms_thresh: float = 0.7,\n    crop_overlap_ratio: float = 512 / 1500,\n    crop_n_points_downscale_factor: int = 1,\n    point_grids: Optional[List[np.ndarray]] = None,\n    min_mask_region_area: int = 0,\n    output_mode: str = \"binary_mask\",\n    use_m2m: bool = False,\n    multimask_output: bool = False,\n    max_hole_area: float = 0.0,\n    max_sprinkle_area: float = 0.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the SamGeo2 class.\n\n    Args:\n        model_id (str): The model ID to use. Can be one of the following: \"sam2-hiera-tiny\",\n            \"sam2-hiera-small\", \"sam2-hiera-base-plus\", \"sam2-hiera-large\".\n            Defaults to \"sam2-hiera-large\".\n        device (Optional[str]): The device to use (e.g., \"cpu\", \"cuda\", \"mps\"). Defaults to None.\n        empty_cache (bool): Whether to empty the cache. Defaults to True.\n        automatic (bool): Whether to use automatic mask generation. Defaults to True.\n        video (bool): Whether to use video prediction. Defaults to False.\n        mode (str): The mode to use. Defaults to \"eval\".\n        hydra_overrides_extra (Optional[List[str]]): Additional Hydra overrides. Defaults to None.\n        apply_postprocessing (bool): Whether to apply postprocessing. Defaults to False.\n        points_per_side (int or None): The number of points to be sampled\n            along one side of the image. The total number of points is\n            points_per_side**2. If None, 'point_grids' must provide explicit\n            point sampling.\n        points_per_batch (int): Sets the number of points run simultaneously\n            by the model. Higher numbers may be faster but use more GPU memory.\n        pred_iou_thresh (float): A filtering threshold in [0,1], using the\n            model's predicted mask quality.\n        stability_score_thresh (float): A filtering threshold in [0,1], using\n            the stability of the mask under changes to the cutoff used to binarize\n            the model's mask predictions.\n        stability_score_offset (float): The amount to shift the cutoff when\n            calculated the stability score.\n        mask_threshold (float): Threshold for binarizing the mask logits\n        box_nms_thresh (float): The box IoU cutoff used by non-maximal\n            suppression to filter duplicate masks.\n        crop_n_layers (int): If &gt;0, mask prediction will be run again on\n            crops of the image. Sets the number of layers to run, where each\n            layer has 2**i_layer number of image crops.\n        crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n            suppression to filter duplicate masks between different crops.\n        crop_overlap_ratio (float): Sets the degree to which crops overlap.\n            In the first crop layer, crops will overlap by this fraction of\n            the image length. Later layers with more crops scale down this overlap.\n        crop_n_points_downscale_factor (int): The number of points-per-side\n            sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n        point_grids (list(np.ndarray) or None): A list over explicit grids\n            of points used for sampling, normalized to [0,1]. The nth grid in the\n            list is used in the nth crop layer. Exclusive with points_per_side.\n        min_mask_region_area (int): If &gt;0, postprocessing will be applied\n            to remove disconnected regions and holes in masks with area smaller\n            than min_mask_region_area. Requires opencv.\n        output_mode (str): The form masks are returned in. Can be 'binary_mask',\n            'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools.\n            For large resolutions, 'binary_mask' may consume large amounts of\n            memory.\n        use_m2m (bool): Whether to add a one step refinement using previous mask predictions.\n        multimask_output (bool): Whether to output multimask at each point of the grid.\n            Defaults to False.\n        max_hole_area (int): If max_hole_area &gt; 0, we fill small holes in up to\n            the maximum area of max_hole_area in low_res_masks.\n        max_sprinkle_area (int): If max_sprinkle_area &gt; 0, we remove small sprinkles up to\n            the maximum area of max_sprinkle_area in low_res_masks.\n        **kwargs (Any): Additional keyword arguments to pass to\n            SAM2AutomaticMaskGenerator.from_pretrained() or SAM2ImagePredictor.from_pretrained().\n    \"\"\"\n\n    try:\n        from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n        from sam2.sam2_image_predictor import SAM2ImagePredictor\n        from sam2.sam2_video_predictor import SAM2VideoPredictor\n    except ImportError as e:\n        raise ImportError(\n            f\"To use SamGeo 2, install it as:\\n\\tpip install segment-geospatial[samgeo2]\"\n        )\n\n    if isinstance(model_id, str):\n        if not model_id.startswith(\"facebook/\"):\n            model_id = f\"facebook/{model_id}\"\n    else:\n        raise ValueError(\"model_id must be a string\")\n\n    allowed_models = [\n        \"facebook/sam2-hiera-tiny\",\n        \"facebook/sam2-hiera-small\",\n        \"facebook/sam2-hiera-base-plus\",\n        \"facebook/sam2-hiera-large\",\n    ]\n\n    if model_id not in allowed_models:\n        raise ValueError(\n            f\"model_id must be one of the following: {', '.join(allowed_models)}\"\n        )\n\n    if device is None:\n        device = common.choose_device(empty_cache=empty_cache)\n\n    if hydra_overrides_extra is None:\n        hydra_overrides_extra = []\n\n    self.model_id = model_id\n    self.model_version = \"sam2\"\n    self.device = device\n\n    if video:\n        automatic = False\n\n    if automatic:\n        self.mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            points_per_side=points_per_side,\n            points_per_batch=points_per_batch,\n            pred_iou_thresh=pred_iou_thresh,\n            stability_score_thresh=stability_score_thresh,\n            stability_score_offset=stability_score_offset,\n            mask_threshold=mask_threshold,\n            box_nms_thresh=box_nms_thresh,\n            crop_n_layers=crop_n_layers,\n            crop_nms_thresh=crop_nms_thresh,\n            crop_overlap_ratio=crop_overlap_ratio,\n            crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n            point_grids=point_grids,\n            min_mask_region_area=min_mask_region_area,\n            output_mode=output_mode,\n            use_m2m=use_m2m,\n            multimask_output=multimask_output,\n            **kwargs,\n        )\n    elif video:\n        self.predictor = SAM2VideoPredictor.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            **kwargs,\n        )\n    else:\n        self.predictor = SAM2ImagePredictor.from_pretrained(\n            model_id,\n            device=device,\n            mode=mode,\n            hydra_overrides_extra=hydra_overrides_extra,\n            apply_postprocessing=apply_postprocessing,\n            mask_threshold=mask_threshold,\n            max_hole_area=max_hole_area,\n            max_sprinkle_area=max_sprinkle_area,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.add_new_mask","title":"<code>add_new_mask(inference_state, frame_idx, obj_id, mask)</code>","text":"<p>Add a new mask to the inference state.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index.</p> required <code>obj_id</code> <code>int</code> <p>The object ID.</p> required <code>mask</code> <code>ndarray</code> <p>The mask to add.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The updated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef add_new_mask(\n    self,\n    inference_state: Any,\n    frame_idx: int,\n    obj_id: int,\n    mask: np.ndarray,\n) -&gt; Any:\n    \"\"\"Add a new mask to the inference state.\n\n    Args:\n        inference_state (Any): The current inference state.\n        frame_idx (int): The frame index.\n        obj_id (int): The object ID.\n        mask (np.ndarray): The mask to add.\n\n    Returns:\n        Any: The updated inference state.\n    \"\"\"\n    return self.predictor.add_new_mask(inference_state, frame_idx, obj_id, mask)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.add_new_points_or_box","title":"<code>add_new_points_or_box(inference_state, frame_idx, obj_id, points=None, labels=None, clear_old_points=True, normalize_coords=True, box=None)</code>","text":"<p>Add new points or a box to the inference state.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index.</p> required <code>obj_id</code> <code>int</code> <p>The object ID.</p> required <code>points</code> <code>Optional[ndarray]</code> <p>The points to add. Defaults to None.</p> <code>None</code> <code>labels</code> <code>Optional[ndarray]</code> <p>The labels for the points. Defaults to None.</p> <code>None</code> <code>clear_old_points</code> <code>bool</code> <p>Whether to clear old points. Defaults to True.</p> <code>True</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>box</code> <code>Optional[ndarray]</code> <p>The bounding box to add. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The updated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef add_new_points_or_box(\n    self,\n    inference_state: Any,\n    frame_idx: int,\n    obj_id: int,\n    points: Optional[np.ndarray] = None,\n    labels: Optional[np.ndarray] = None,\n    clear_old_points: bool = True,\n    normalize_coords: bool = True,\n    box: Optional[np.ndarray] = None,\n) -&gt; Any:\n    \"\"\"Add new points or a box to the inference state.\n\n    Args:\n        inference_state (Any): The current inference state.\n        frame_idx (int): The frame index.\n        obj_id (int): The object ID.\n        points (Optional[np.ndarray]): The points to add. Defaults to None.\n        labels (Optional[np.ndarray]): The labels for the points. Defaults to None.\n        clear_old_points (bool): Whether to clear old points. Defaults to True.\n        normalize_coords (bool): Whether to normalize the coordinates. Defaults to True.\n        box (Optional[np.ndarray]): The bounding box to add. Defaults to None.\n\n    Returns:\n        Any: The updated inference state.\n    \"\"\"\n    return self.predictor.add_new_points_or_box(\n        inference_state,\n        frame_idx,\n        obj_id,\n        points=points,\n        labels=labels,\n        clear_old_points=clear_old_points,\n        normalize_coords=normalize_coords,\n        box=box,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.generate","title":"<code>generate(source, output=None, foreground=True, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, ndarray]</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>Optional[Tuple[int, int]]</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing the generated masks.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def generate(\n    self,\n    source: Union[str, np.ndarray],\n    output: Optional[str] = None,\n    foreground: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: int = None,\n    **kwargs: Any,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate masks for the input image.\n\n    Args:\n        source (Union[str, np.ndarray]): The path to the input image or the\n            input image as a numpy array.\n        output (Optional[str]): The path to the output image. Defaults to None.\n        foreground (bool): Whether to generate the foreground mask. Defaults\n            to True.\n        erosion_kernel (Optional[Tuple[int, int]]): The erosion kernel for\n            filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range,\n            for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool): Whether to assign a unique value to each object.\n            Defaults to True.\n            The unique value increases from 1 to the number of objects. The\n            larger the number, the larger the object area.\n        min_size (int): The minimum size of the object. Defaults to 0.\n        max_size (int): The maximum size of the object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = common.download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self._min_size = min_size\n    self._max_size = max_size\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.init_state","title":"<code>init_state(video_path, offload_video_to_cpu=False, offload_state_to_cpu=False, async_loading_frames=False)</code>","text":"<p>Initialize an inference state.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>offload_video_to_cpu</code> <code>bool</code> <p>Whether to offload the video to CPU. Defaults to False.</p> <code>False</code> <code>offload_state_to_cpu</code> <code>bool</code> <p>Whether to offload the state to CPU. Defaults to False.</p> <code>False</code> <code>async_loading_frames</code> <code>bool</code> <p>Whether to load frames asynchronously. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef init_state(\n    self,\n    video_path: str,\n    offload_video_to_cpu: bool = False,\n    offload_state_to_cpu: bool = False,\n    async_loading_frames: bool = False,\n) -&gt; Any:\n    \"\"\"Initialize an inference state.\n\n    Args:\n        video_path (str): The path to the video file.\n        offload_video_to_cpu (bool): Whether to offload the video to CPU.\n            Defaults to False.\n        offload_state_to_cpu (bool): Whether to offload the state to CPU.\n            Defaults to False.\n        async_loading_frames (bool): Whether to load frames asynchronously.\n            Defaults to False.\n\n    Returns:\n        Any: The initialized inference state.\n    \"\"\"\n    return self.predictor.init_state(\n        video_path,\n        offload_video_to_cpu=offload_video_to_cpu,\n        offload_state_to_cpu=offload_state_to_cpu,\n        async_loading_frames=async_loading_frames,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, mask_input=None, multimask_output=False, return_logits=False, normalize_coords=True, point_crs=None, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict the mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>ndarray</code> <p>The point coordinates. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>ndarray</code> <p>The point labels. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask, and the logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict(\n    self,\n    point_coords: Optional[np.ndarray] = None,\n    point_labels: Optional[np.ndarray] = None,\n    boxes: Optional[np.ndarray] = None,\n    mask_input: Optional[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords: bool = True,\n    point_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"float32\",\n    return_results: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Predict the mask for the input image.\n\n    Args:\n        point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n        point_labels (np.ndarray, optional): The point labels. Defaults to None.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        multimask_output (bool, optional): Whether to output multimask at each\n            point of the grid. Defaults to False.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        normalize_coords (bool, optional): Whether to normalize the coordinates.\n            Defaults to True.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks,\n            scores, and logits. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n            and the logits.\n    \"\"\"\n    import geopandas as gpd\n\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = common.vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = common.geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = common.coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = common.bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n\n    self.boxes = input_boxes\n\n    masks, scores, logits = predictor.predict(\n        point_coords=point_coords,\n        point_labels=point_labels,\n        box=input_boxes,\n        mask_input=mask_input,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_batch","title":"<code>predict_batch(point_coords_batch=None, point_labels_batch=None, box_batch=None, mask_input_batch=None, multimask_output=False, return_logits=False, normalize_coords=True)</code>","text":"<p>Predict masks for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of point coordinates. Defaults to None.</p> <code>None</code> <code>point_labels_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of point labels. Defaults to None.</p> <code>None</code> <code>box_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of bounding boxes. Defaults to None.</p> <code>None</code> <code>mask_input_batch</code> <code>Optional[List[ndarray]]</code> <p>A batch of mask inputs. Defaults to None.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to False.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>Whether to return the logits. Defaults to False.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[List[ndarray], List[ndarray], List[ndarray]]</code> <p>Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists of masks, multimasks, and logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_batch(\n    self,\n    point_coords_batch: List[np.ndarray] = None,\n    point_labels_batch: List[np.ndarray] = None,\n    box_batch: List[np.ndarray] = None,\n    mask_input_batch: List[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords=True,\n) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Predict masks for a batch of images.\n\n    Args:\n        point_coords_batch (Optional[List[np.ndarray]]): A batch of point\n            coordinates. Defaults to None.\n        point_labels_batch (Optional[List[np.ndarray]]): A batch of point\n            labels. Defaults to None.\n        box_batch (Optional[List[np.ndarray]]): A batch of bounding boxes.\n            Defaults to None.\n        mask_input_batch (Optional[List[np.ndarray]]): A batch of mask inputs.\n            Defaults to None.\n        multimask_output (bool): Whether to output multimask at each point\n            of the grid. Defaults to False.\n        return_logits (bool): Whether to return the logits. Defaults to False.\n        normalize_coords (bool): Whether to normalize the coordinates.\n            Defaults to True.\n\n    Returns:\n        Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: Lists\n            of masks, multimasks, and logits.\n    \"\"\"\n\n    return self.predictor.predict_batch(\n        point_coords_batch=point_coords_batch,\n        point_labels_batch=point_labels_batch,\n        box_batch=box_batch,\n        mask_input_batch=mask_input_batch,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_by_points","title":"<code>predict_by_points(point_coords_batch=None, point_labels_batch=None, box_batch=None, mask_input_batch=None, multimask_output=False, return_logits=False, normalize_coords=True, point_crs=None, output=None, index=None, unique=True, mask_multiplier=255, dtype='int32', return_results=False, **kwargs)</code>","text":"<p>Predict the mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>ndarray</code> <p>The point coordinates. Defaults to None.</p> required <code>point_labels</code> <code>ndarray</code> <p>The point labels. Defaults to None.</p> required <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> required <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> required <code>multimask_output</code> <code>bool</code> <p>Whether to output multimask at each point of the grid. Defaults to True.</p> <code>False</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>Whether to normalize the coordinates. Defaults to True.</p> <code>True</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.int32.</p> <code>'int32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask, and the logits.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_by_points(\n    self,\n    point_coords_batch: List[np.ndarray] = None,\n    point_labels_batch: List[np.ndarray] = None,\n    box_batch: List[np.ndarray] = None,\n    mask_input_batch: List[np.ndarray] = None,\n    multimask_output: bool = False,\n    return_logits: bool = False,\n    normalize_coords=True,\n    point_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    index: Optional[int] = None,\n    unique: bool = True,\n    mask_multiplier: int = 255,\n    dtype: str = \"int32\",\n    return_results: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Predict the mask for the input image.\n\n    Args:\n        point_coords (np.ndarray, optional): The point coordinates. Defaults to None.\n        point_labels (np.ndarray, optional): The point labels. Defaults to None.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        multimask_output (bool, optional): Whether to output multimask at each\n            point of the grid. Defaults to True.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        normalize_coords (bool, optional): Whether to normalize the coordinates.\n            Defaults to True.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask,\n            which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.int32.\n        return_results (bool, optional): Whether to return the predicted masks,\n            scores, and logits. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The mask, the multimask,\n            and the logits.\n    \"\"\"\n    import geopandas as gpd\n\n    if hasattr(self, \"image_batch\") and self.image_batch is not None:\n        pass\n    elif self.image is not None:\n        self.predictor.set_image_batch([self.image])\n        setattr(self, \"image_batch\", [self.image])\n    else:\n        raise ValueError(\"Please set the input image first using set_image().\")\n\n    if isinstance(point_coords_batch, dict):\n        point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n    if isinstance(point_coords_batch, str) or isinstance(\n        point_coords_batch, gpd.GeoDataFrame\n    ):\n        if isinstance(point_coords_batch, str):\n            gdf = gpd.read_file(point_coords_batch)\n        else:\n            gdf = point_coords_batch\n        if gdf.crs is None and (point_crs is not None):\n            gdf.crs = point_crs\n\n        points = gdf.geometry.apply(lambda geom: [geom.x, geom.y])\n        coordinates_array = np.array([[point] for point in points])\n        points = common.coords_to_xy(self.source, coordinates_array, point_crs)\n        num_points = points.shape[0]\n        if point_labels_batch is None:\n            labels = np.array([[1] for i in range(num_points)])\n        else:\n            labels = point_labels_batch\n\n    elif isinstance(point_coords_batch, list):\n        if point_crs is not None:\n            point_coords_batch_crs = common.coords_to_xy(\n                self.source, point_coords_batch, point_crs\n            )\n        else:\n            point_coords_batch_crs = point_coords_batch\n        num_points = len(point_coords_batch)\n\n        points = []\n        points.append([[point] for point in point_coords_batch_crs])\n\n        if point_labels_batch is None:\n            labels = np.array([[1] for i in range(num_points)])\n        elif isinstance(point_labels_batch, list):\n            labels = []\n            labels.append([[label] for label in point_labels_batch])\n            labels = labels[0]\n        else:\n            labels = point_labels_batch\n\n        points = np.array(points[0])\n        labels = np.array(labels)\n\n    elif isinstance(point_coords_batch, np.ndarray):\n        points = point_coords_batch\n        labels = point_labels_batch\n    else:\n        raise ValueError(\"point_coords must be a list, a GeoDataFrame, or a path.\")\n\n    predictor = self.predictor\n\n    masks_batch, scores_batch, logits_batch = predictor.predict_batch(\n        point_coords_batch=[points],\n        point_labels_batch=[labels],\n        box_batch=box_batch,\n        mask_input_batch=mask_input_batch,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    masks = masks_batch[0]\n    scores = scores_batch[0]\n    logits = logits_batch[0]\n\n    if multimask_output and (index is not None):\n        masks = masks[:, index, :, :]\n\n    if masks.ndim &gt; 3:\n        masks = masks.squeeze()\n\n    output_masks = []\n    sums = np.sum(masks, axis=(1, 2))\n    for index, mask in enumerate(masks):\n        item = {\"segmentation\": mask.astype(\"bool\"), \"area\": sums[index]}\n        output_masks.append(item)\n\n    self.masks = output_masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        self.save_masks(\n            output,\n            foreground=True,\n            unique=unique,\n            mask_multiplier=mask_multiplier,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    if return_results:\n        return output_masks, scores, logits\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.predict_video","title":"<code>predict_video(prompts=None, point_crs=None, output_dir=None, img_ext='png')</code>","text":"<p>Predict masks for the video.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Dict[int, Any]</code> <p>A dictionary containing the prompts with points and labels.</p> <code>None</code> <code>point_crs</code> <code>Optional[str]</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>The directory to save the output images. Defaults to None.</p> <code>None</code> <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def predict_video(\n    self,\n    prompts: Dict[int, Any] = None,\n    point_crs: Optional[str] = None,\n    output_dir: Optional[str] = None,\n    img_ext: str = \"png\",\n) -&gt; None:\n    \"\"\"Predict masks for the video.\n\n    Args:\n        prompts (Dict[int, Any]): A dictionary containing the prompts with points and labels.\n        point_crs (Optional[str]): The coordinate reference system (CRS) of the point prompts.\n        output_dir (Optional[str]): The directory to save the output images. Defaults to None.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n    \"\"\"\n\n    from PIL import Image\n\n    def save_image_from_dict(data, output_path=\"output_image.png\"):\n        # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n        array_shape = next(iter(data.values())).shape[1:]\n\n        # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n        output_array = np.zeros(array_shape, dtype=np.uint8)\n\n        # Iterate over each key and array in the dictionary\n        for key, array in data.items():\n            # Assign the key value wherever the boolean array is True\n            output_array[array[0]] = key\n\n        # Convert the output array to a PIL image\n        image = Image.fromarray(output_array)\n\n        # Save the image\n        image.save(output_path)\n\n    if prompts is None:\n        if hasattr(self, \"prompts\"):\n            prompts = self.prompts\n        else:\n            raise ValueError(\"Please provide prompts.\")\n\n    if point_crs is not None and self._tif_source is not None:\n        for prompt in prompts.values():\n            points = prompt.get(\"points\", None)\n            if points is not None:\n                points = common.coords_to_xy(self._tif_source, points, point_crs)\n                prompt[\"points\"] = points\n            box = prompt.get(\"box\", None)\n            if box is not None:\n                box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                prompt[\"box\"] = box\n\n    prompts = self._convert_prompts(prompts)\n    predictor = self.predictor\n    inference_state = self.inference_state\n    for obj_id, prompt in prompts.items():\n        points = prompt.get(\"points\", None)\n        labels = prompt.get(\"labels\", None)\n        box = prompt.get(\"box\", None)\n        frame_idx = prompt.get(\"frame_idx\", None)\n\n        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n            inference_state=inference_state,\n            frame_idx=frame_idx,\n            obj_id=obj_id,\n            points=points,\n            labels=labels,\n            box=box,\n        )\n\n    video_segments = {}\n    num_frames = self._num_images\n    num_digits = len(str(num_frames))\n\n    if output_dir is not None:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n        inference_state\n    ):\n        video_segments[out_frame_idx] = {\n            out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n            for i, out_obj_id in enumerate(out_obj_ids)\n        }\n\n        if output_dir is not None:\n            output_path = os.path.join(\n                output_dir, f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n            )\n            save_image_from_dict(video_segments[out_frame_idx], output_path)\n\n    self.video_segments = video_segments\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.propagate_in_video","title":"<code>propagate_in_video(inference_state, start_frame_idx=None, max_frame_num_to_track=None, reverse=False)</code>","text":"<p>Propagate the inference state in video.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <code>start_frame_idx</code> <code>Optional[int]</code> <p>The starting frame index. Defaults to None.</p> <code>None</code> <code>max_frame_num_to_track</code> <code>Optional[int]</code> <p>The maximum number of frames to track. Defaults to None.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Whether to propagate in reverse. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The propagated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef propagate_in_video(\n    self,\n    inference_state: Any,\n    start_frame_idx: Optional[int] = None,\n    max_frame_num_to_track: Optional[int] = None,\n    reverse: bool = False,\n) -&gt; Any:\n    \"\"\"Propagate the inference state in video.\n\n    Args:\n        inference_state (Any): The current inference state.\n        start_frame_idx (Optional[int]): The starting frame index. Defaults to None.\n        max_frame_num_to_track (Optional[int]): The maximum number of frames\n            to track. Defaults to None.\n        reverse (bool): Whether to propagate in reverse. Defaults to False.\n\n    Returns:\n        Any: The propagated inference state.\n    \"\"\"\n    return self.predictor.propagate_in_video(\n        inference_state,\n        start_frame_idx=start_frame_idx,\n        max_frame_num_to_track=max_frame_num_to_track,\n        reverse=reverse,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.propagate_in_video_preflight","title":"<code>propagate_in_video_preflight(inference_state)</code>","text":"<p>Propagate the inference state in video preflight.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The propagated inference state.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef propagate_in_video_preflight(self, inference_state: Any) -&gt; Any:\n    \"\"\"Propagate the inference state in video preflight.\n\n    Args:\n        inference_state (Any): The current inference state.\n\n    Returns:\n        Any: The propagated inference state.\n    \"\"\"\n    return self.predictor.propagate_in_video_preflight(inference_state)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.raster_to_vector","title":"<code>raster_to_vector(raster, vector, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a raster image file to a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>The path to the raster image.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def raster_to_vector(self, raster, vector, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a raster image file to a vector dataset.\n\n    Args:\n        raster (str): The path to the raster image.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    common.raster_to_vector(\n        raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to use for properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def region_groups(\n    self,\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[\n    Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to use for properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    return common.region_groups(\n        image,\n        connectivity=connectivity,\n        min_size=min_size,\n        max_size=max_size,\n        threshold=threshold,\n        properties=properties,\n        intensity_image=intensity_image,\n        out_csv=out_csv,\n        out_vector=out_vector,\n        out_image=out_image,\n        **kwargs,\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.reset_state","title":"<code>reset_state(inference_state)</code>","text":"<p>Remove all input points or masks in all frames throughout the video.</p> <p>Parameters:</p> Name Type Description Default <code>inference_state</code> <code>Any</code> <p>The current inference state.</p> required Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.inference_mode()\ndef reset_state(self, inference_state: Any) -&gt; None:\n    \"\"\"Remove all input points or masks in all frames throughout the video.\n\n    Args:\n        inference_state (Any): The current inference state.\n    \"\"\"\n    self.predictor.reset_state(inference_state)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for common.array_to_image().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_masks(\n    self,\n    output: Optional[str] = None,\n    foreground: bool = True,\n    unique: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    min_size: int = 0,\n    max_size: int = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the masks to the output path. The output is either a binary mask\n    or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to\n            None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask.\n            Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each\n            object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering\n            object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to\n            None.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1]. You can use this\n            parameter to scale the mask to a larger range, for example\n            [0, 255]. Defaults to 255.\n        min_size (int, optional): The minimum size of the object. Defaults to 0.\n        max_size (int, optional): The maximum size of the object. Defaults to None.\n        **kwargs: Additional keyword arguments for common.array_to_image().\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in descending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        count = len(sorted_masks)\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and ann[\"area\"] &gt; max_size:\n                continue\n            objects[m] = count - index\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and m[\"area\"] &gt; max_size:\n                continue\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        common.array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype='float32', vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>str</code> <p>The data type of the output image. Defaults to \"float32\".</p> <code>'float32'</code> <code>vector</code> <code>Optional[str]</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_prediction(\n    self,\n    output: str,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"float32\",\n    vector: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (Optional[int], optional): The index of the mask to save.\n            Defaults to None, which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1].\n        dtype (str, optional): The data type of the output image. Defaults\n            to \"float32\".\n        vector (Optional[str], optional): The path to the output vector file.\n            Defaults to None.\n        simplify_tolerance (Optional[float], optional): The maximum allowed\n            geometry displacement. The higher this value, the smaller the\n            number of vertices in the resulting geometry.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        common.raster_to_vector(\n            output, vector, simplify_tolerance=simplify_tolerance\n        )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_video_segments","title":"<code>save_video_segments(output_dir, img_ext='png')</code>","text":"<p>Save the video segments to the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The path to the output directory.</p> required <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_video_segments(self, output_dir: str, img_ext: str = \"png\") -&gt; None:\n    \"\"\"Save the video segments to the output directory.\n\n    Args:\n        output_dir (str): The path to the output directory.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n    \"\"\"\n    from PIL import Image\n\n    def save_image_from_dict(\n        data, output_path=\"output_image.png\", crs_source=None, **kwargs\n    ):\n        # Find the shape of the first array in the dictionary (assuming all arrays have the same shape)\n        array_shape = next(iter(data.values())).shape[1:]\n\n        # Initialize an empty array with the same shape as the arrays in the dictionary, filled with zeros\n        output_array = np.zeros(array_shape, dtype=np.uint8)\n\n        # Iterate over each key and array in the dictionary\n        for key, array in data.items():\n            # Assign the key value wherever the boolean array is True\n            output_array[array[0]] = key\n\n        if crs_source is None:\n            # Convert the output array to a PIL image\n            image = Image.fromarray(output_array)\n\n            # Save the image\n            image.save(output_path)\n        else:\n            output_path = output_path.replace(\".png\", \".tif\")\n            common.array_to_image(output_array, output_path, crs_source, **kwargs)\n\n    num_frames = len(self.video_segments)\n    num_digits = len(str(num_frames))\n\n    if hasattr(self, \"_tif_source\") and self._tif_source.endswith(\".tif\"):\n        crs_source = self._tif_source\n        filenames = self._tif_names\n    else:\n        crs_source = None\n        filenames = None\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Initialize the tqdm progress bar\n    for frame_idx, video_segment in tqdm(\n        self.video_segments.items(), desc=\"Rendering frames\", total=num_frames\n    ):\n        if filenames is None:\n            output_path = os.path.join(\n                output_dir, f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n            )\n        else:\n            output_path = os.path.join(output_dir, filenames[frame_idx])\n        save_image_from_dict(video_segment, output_path, crs_source)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.save_video_segments_blended","title":"<code>save_video_segments_blended(output_dir, img_ext='png', alpha=0.6, dpi=200, frame_stride=1, output_video=None, fps=30)</code>","text":"<p>Save blended video segments to the output directory and optionally create a video.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory to save the output images.</p> required <code>img_ext</code> <code>str</code> <p>The file extension for the output images. Defaults to \"png\".</p> <code>'png'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the blended masks. Defaults to 0.6.</p> <code>0.6</code> <code>dpi</code> <code>int</code> <p>The DPI (dots per inch) for the output images. Defaults to 200.</p> <code>200</code> <code>frame_stride</code> <code>int</code> <p>The stride for selecting frames to save. Defaults to 1.</p> <code>1</code> <code>output_video</code> <code>Optional[str]</code> <p>The path to the output video file. Defaults to None.</p> <code>None</code> <code>fps</code> <code>int</code> <p>The frames per second for the output video. Defaults to 30.</p> <code>30</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def save_video_segments_blended(\n    self,\n    output_dir: str,\n    img_ext: str = \"png\",\n    alpha: float = 0.6,\n    dpi: int = 200,\n    frame_stride: int = 1,\n    output_video: Optional[str] = None,\n    fps: int = 30,\n) -&gt; None:\n    \"\"\"Save blended video segments to the output directory and optionally create a video.\n\n    Args:\n        output_dir (str): The directory to save the output images.\n        img_ext (str): The file extension for the output images. Defaults to \"png\".\n        alpha (float): The alpha value for the blended masks. Defaults to 0.6.\n\n        dpi (int): The DPI (dots per inch) for the output images. Defaults to 200.\n        frame_stride (int): The stride for selecting frames to save. Defaults to 1.\n        output_video (Optional[str]): The path to the output video file. Defaults to None.\n        fps (int): The frames per second for the output video. Defaults to 30.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from PIL import Image\n\n    def show_mask(mask, ax, obj_id=None, random_color=False):\n        if random_color:\n            color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n        else:\n            cmap = plt.get_cmap(\"tab10\")\n            cmap_idx = 0 if obj_id is None else obj_id\n            color = np.array([*cmap(cmap_idx)[:3], alpha])\n        h, w = mask.shape[-2:]\n        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n        ax.imshow(mask_image)\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    plt.close(\"all\")\n\n    video_segments = self.video_segments\n    video_dir = self.video_path\n    frame_names = self._frame_names\n    num_frames = len(frame_names)\n    num_digits = len(str(num_frames))\n\n    # Initialize the tqdm progress bar\n    for out_frame_idx in tqdm(\n        range(0, len(frame_names), frame_stride), desc=\"Rendering frames\"\n    ):\n        image = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n\n        # Get original image dimensions\n        w, h = image.size\n\n        # Set DPI and calculate figure size based on the original image dimensions\n        figsize = (\n            w / dpi,\n            h / dpi,\n        )\n        figsize = (\n            figsize[0] * 1.3,\n            figsize[1] * 1.3,\n        )\n\n        # Create a figure with the exact size and DPI\n        fig = plt.figure(figsize=figsize, dpi=dpi)\n\n        # Disable axis to prevent whitespace\n        plt.axis(\"off\")\n\n        # Display the original image\n        plt.imshow(image)\n\n        # Overlay masks for each object ID\n        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n            show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n\n        # Save the figure with no borders or extra padding\n        filename = f\"{str(out_frame_idx).zfill(num_digits)}.{img_ext}\"\n        filepath = os.path.join(output_dir, filename)\n        plt.savefig(filepath, dpi=dpi, pad_inches=0, bbox_inches=\"tight\")\n        plt.close(fig)\n\n    if output_video is not None:\n        common.images_to_video(output_dir, output_video, fps=fps)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_image","title":"<code>set_image(image)</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray, Image]</code> <p>The input image as a path, a numpy array, or an Image.</p> required Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.no_grad()\ndef set_image(\n    self,\n    image: Union[str, np.ndarray, Image],\n) -&gt; None:\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (Union[str, np.ndarray, Image]): The input image as a path,\n            a numpy array, or an Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = common.download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray) or isinstance(image, Image):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_image_batch","title":"<code>set_image_batch(image_list)</code>","text":"<p>Set a batch of images for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>List[Union[ndarray, str, Image]]</code> <p>A list of images,</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an input image path does not exist or if the input image type is not supported.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>@torch.no_grad()\ndef set_image_batch(\n    self,\n    image_list: List[Union[np.ndarray, str, Image]],\n) -&gt; None:\n    \"\"\"Set a batch of images for prediction.\n\n    Args:\n        image_list (List[Union[np.ndarray, str, Image]]): A list of images,\n        which can be numpy arrays, file paths, or PIL images.\n\n    Raises:\n        ValueError: If an input image path does not exist or if the input\n            image type is not supported.\n    \"\"\"\n    images = []\n    for image in image_list:\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(image, Image):\n            image = np.array(image)\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        images.append(image)\n\n    self.predictor.set_image_batch(images)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.set_video","title":"<code>set_video(video_path, output_dir=None, frame_rate=None, prefix='')</code>","text":"<p>Set the video path and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>start_frame</code> <code>int</code> <p>The starting frame index. Defaults to 0.</p> required <code>end_frame</code> <code>Optional[int]</code> <p>The ending frame index. Defaults to None.</p> required <code>step</code> <code>int</code> <p>The step size. Defaults to 1.</p> required <code>frame_rate</code> <code>Optional[int]</code> <p>The frame rate. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def set_video(\n    self,\n    video_path: str,\n    output_dir: str = None,\n    frame_rate: Optional[int] = None,\n    prefix: str = \"\",\n) -&gt; None:\n    \"\"\"Set the video path and parameters.\n\n    Args:\n        video_path (str): The path to the video file.\n        start_frame (int, optional): The starting frame index. Defaults to 0.\n        end_frame (Optional[int], optional): The ending frame index. Defaults to None.\n        step (int, optional): The step size. Defaults to 1.\n        frame_rate (Optional[int], optional): The frame rate. Defaults to None.\n    \"\"\"\n\n    if isinstance(video_path, str):\n        if video_path.startswith(\"http\"):\n            video_path = common.download_file(video_path)\n        if os.path.isfile(video_path):\n            if output_dir is None:\n                output_dir = common.make_temp_dir()\n                if not os.path.exists(output_dir):\n                    os.makedirs(output_dir)\n            print(f\"Output directory: {output_dir}\")\n            common.video_to_images(\n                video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n            )\n\n        elif os.path.isdir(video_path):\n            files = sorted(os.listdir(video_path))\n            if len(files) == 0:\n                raise ValueError(f\"No files found in {video_path}.\")\n            elif files[0].endswith(\".tif\"):\n                self._tif_source = os.path.join(video_path, files[0])\n                self._tif_dir = video_path\n                self._tif_names = files\n                video_path = common.geotiff_to_jpg_batch(video_path)\n            output_dir = video_path\n\n        if not os.path.exists(video_path):\n            raise ValueError(f\"Input path {video_path} does not exist.\")\n    else:\n        raise ValueError(\"Input video_path must be a string.\")\n\n    self.video_path = output_dir\n    self._num_images = len(os.listdir(output_dir))\n    self._frame_names = sorted(os.listdir(output_dir))\n    self.inference_state = self.predictor.init_state(video_path=output_dir)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_anns(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    alpha: float = 0.35,\n    output: Optional[str] = None,\n    blend: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n            continue\n        if (\n            hasattr(self, \"_max_size\")\n            and isinstance(self._max_size, int)\n            and ann[\"area\"] &gt; self._max_size\n        ):\n            continue\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    # if \"dpi\" not in kwargs:\n    #     kwargs[\"dpi\"] = 100\n\n    # if \"bbox_inches\" not in kwargs:\n    #     kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = common.blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        common.array_to_image(array, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>fg_color</code> <code>Tuple[int, int, int]</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>Tuple[int, int, int]</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[list, list]</code> <p>Tuple[list, list]: A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_canvas(\n    self,\n    fg_color: Tuple[int, int, int] = (0, 255, 0),\n    bg_color: Tuple[int, int, int] = (0, 0, 255),\n    radius: int = 5,\n) -&gt; Tuple[list, list]:\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        fg_color (Tuple[int, int, int], optional): The color for the foreground points.\n            Defaults to (0, 255, 0).\n        bg_color (Tuple[int, int, int], optional): The color for the background points.\n            Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        Tuple[list, list]: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_images","title":"<code>show_images(path=None)</code>","text":"<p>Show the images in the video.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the images. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_images(self, path: str = None) -&gt; None:\n    \"\"\"Show the images in the video.\n\n    Args:\n        path (str, optional): The path to the images. Defaults to None.\n    \"\"\"\n    if path is None:\n        path = self.video_path\n\n    if path is not None:\n        common.show_image_gui(path)\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_map","title":"<code>show_map(basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>Optional[str]</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The map object.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_map(\n    self,\n    basemap: str = \"SATELLITE\",\n    repeat_mode: bool = True,\n    out_dir: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following:\n            SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for\n            draw control. Defaults to True.\n        out_dir (Optional[str], optional): The path to the output directory.\n            Defaults to None.\n\n    Returns:\n        Any: The map object.\n    \"\"\"\n    return common.sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_masks(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    cmap: str = \"binary_r\",\n    axis: str = \"off\",\n    foreground: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only.\n            Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.objects is None:\n        self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.show_prompts","title":"<code>show_prompts(prompts, frame_idx=0, mask=None, random_color=False, point_crs=None, figsize=(9, 6))</code>","text":"<p>Show the prompts on the image.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Dict[int, Any]</code> <p>A dictionary containing the prompts with points and labels.</p> required <code>frame_idx</code> <code>int</code> <p>The frame index. Defaults to 0.</p> <code>0</code> <code>mask</code> <code>Any</code> <p>The mask. Defaults to None.</p> <code>None</code> <code>random_color</code> <code>bool</code> <p>Whether to use random colors for the masks. Defaults to False.</p> <code>False</code> <code>point_crs</code> <code>Optional[str]</code> <p>The coordinate reference system</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size. Defaults to (9, 6).</p> <code>(9, 6)</code> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def show_prompts(\n    self,\n    prompts: Dict[int, Any],\n    frame_idx: int = 0,\n    mask: Any = None,\n    random_color: bool = False,\n    point_crs: Optional[str] = None,\n    figsize: Tuple[int, int] = (9, 6),\n) -&gt; None:\n    \"\"\"Show the prompts on the image.\n\n    Args:\n        prompts (Dict[int, Any]): A dictionary containing the prompts with\n            points and labels.\n        frame_idx (int, optional): The frame index. Defaults to 0.\n        mask (Any, optional): The mask. Defaults to None.\n        random_color (bool, optional): Whether to use random colors for the\n            masks. Defaults to False.\n        point_crs (Optional[str], optional): The coordinate reference system\n        figsize (Tuple[int, int], optional): The figure size. Defaults to (9, 6).\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    from PIL import Image\n\n    def show_mask(mask, ax, obj_id=None, random_color=random_color):\n        if random_color:\n            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n        else:\n            cmap = plt.get_cmap(\"tab10\")\n            cmap_idx = 0 if obj_id is None else obj_id\n            color = np.array([*cmap(cmap_idx)[:3], 0.6])\n        h, w = mask.shape[-2:]\n        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n        ax.imshow(mask_image)\n\n    def show_points(coords, labels, ax, marker_size=200):\n        pos_points = coords[labels == 1]\n        neg_points = coords[labels == 0]\n        ax.scatter(\n            pos_points[:, 0],\n            pos_points[:, 1],\n            color=\"green\",\n            marker=\"*\",\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n        ax.scatter(\n            neg_points[:, 0],\n            neg_points[:, 1],\n            color=\"red\",\n            marker=\"*\",\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n\n    def show_box(box, ax):\n        x0, y0 = box[0], box[1]\n        w, h = box[2] - box[0], box[3] - box[1]\n        ax.add_patch(\n            plt.Rectangle(\n                (x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2\n            )\n        )\n\n    if point_crs is not None and self._tif_source is not None:\n        for prompt in prompts.values():\n            points = prompt.get(\"points\", None)\n            if points is not None:\n                points = common.coords_to_xy(self._tif_source, points, point_crs)\n                prompt[\"points\"] = points\n            box = prompt.get(\"box\", None)\n            if box is not None:\n                box = common.bbox_to_xy(self._tif_source, box, point_crs)\n                prompt[\"box\"] = box\n\n    prompts = self._convert_prompts(prompts)\n    self.prompts = prompts\n    video_dir = self.video_path\n    frame_names = self._frame_names\n    fig = plt.figure(figsize=figsize)\n    fig.canvas.toolbar_visible = True\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = True\n    plt.title(f\"frame {frame_idx}\")\n    plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n\n    for obj_id, prompt in prompts.items():\n        points = prompt.get(\"points\", None)\n        labels = prompt.get(\"labels\", None)\n        box = prompt.get(\"box\", None)\n        anno_frame_idx = prompt.get(\"frame_idx\", None)\n        if anno_frame_idx == frame_idx:\n            if points is not None:\n                show_points(points, labels, plt.gca())\n            if box is not None:\n                show_box(box, plt.gca())\n            if mask is not None:\n                show_mask(mask, plt.gca(), obj_id=obj_id)\n\n    plt.show()\n</code></pre>"},{"location":"samgeo2/#samgeo.samgeo2.SamGeo2.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args=None)</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>str</code> <p>The data type of the output image. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>save_args</code> <code>Optional[Dict[str, Any]]</code> <p>Optional arguments for saving the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The predicted mask as a numpy array, or None if output is specified.</p> Source code in <code>samgeo/samgeo2.py</code> <pre><code>def tensor_to_numpy(\n    self,\n    index: Optional[int] = None,\n    output: Optional[str] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"uint8\",\n    save_args: Optional[Dict[str, Any]] = None,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (Optional[int], optional): The index of the mask to save.\n            Defaults to None, which will save the mask with the highest score.\n        output (Optional[str], optional): The path to the output image.\n            Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output\n            mask, which is usually a binary mask [0, 1].\n        dtype (str, optional): The data type of the output image. Defaults\n            to \"uint8\".\n        save_args (Optional[Dict[str, Any]], optional): Optional arguments\n            for saving the output image. Defaults to None.\n\n    Returns:\n        Optional[np.ndarray]: The predicted mask as a numpy array, or None\n            if output is specified.\n    \"\"\"\n    if save_args is None:\n        save_args = {}\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 0\n\n    masks = masks[:, index, :, :]\n    if len(masks.shape) == 4 and masks.shape[1] == 1:\n        masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (_, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        common.array_to_image(\n            mask_overlay, output, self.source, dtype=dtype, **save_args\n        )\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"samgeo3/","title":"samgeo3 module","text":"<p>Segmenting remote sensing images with the Segment Anything Model 3 (SAM3). https://github.com/facebookresearch/sam3</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3","title":"<code>SamGeo3</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model 3 (SAM3).</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>class SamGeo3:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model 3 (SAM3).\"\"\"\n\n    def __init__(\n        self,\n        backend=\"meta\",\n        model_id=\"facebook/sam3\",\n        bpe_path=None,\n        device=None,\n        eval_mode=True,\n        checkpoint_path=None,\n        load_from_HF=True,\n        enable_segmentation=True,\n        enable_inst_interactivity=False,\n        compile_mode=False,\n        resolution=1008,\n        confidence_threshold=0.5,\n        mask_threshold=0.5,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SamGeo3 class.\n\n        Args:\n            backend (str): Backend to use ('meta' or 'transformers'). Default is 'meta'.\n            model_id (str): Model ID for Transformers backend (e.g., 'facebook/sam3').\n                Only used when backend='transformers'.\n            bpe_path (str, optional): Path to the BPE tokenizer vocabulary (Meta backend only).\n            device (str, optional): Device to load the model on ('cuda' or 'cpu').\n            eval_mode (bool, optional): Whether to set the model to evaluation mode (Meta backend only).\n            checkpoint_path (str, optional): Optional path to model checkpoint (Meta backend only).\n            load_from_HF (bool, optional): Whether to load the model from HuggingFace (Meta backend only).\n            enable_segmentation (bool, optional): Whether to enable segmentation head (Meta backend only).\n            enable_inst_interactivity (bool, optional): Whether to enable instance interactivity\n                (SAM 1 task) (Meta backend only). Set to True to use predict_inst() and\n                predict_inst_batch() methods for interactive point and box prompts.\n                When True, the model loads additional components for SAM1-style\n                interactive instance segmentation. Defaults to False.\n            compile_mode (bool, optional): To enable compilation, set to \"default\" (Meta backend only).\n            resolution (int, optional): Resolution of the image (Meta backend only).\n            confidence_threshold (float, optional): Confidence threshold for the model.\n            mask_threshold (float, optional): Mask threshold for post-processing (Transformers backend only).\n            **kwargs: Additional keyword arguments.\n\n        Example:\n            &gt;&gt;&gt; # For text-based segmentation\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n            &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n            &gt;&gt;&gt; sam.generate_masks(\"tree\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # For interactive point/box prompts (SAM1-style)\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n            ...     point_coords=np.array([[520, 375]]),\n            ...     point_labels=np.array([1]),\n            ... )\n        \"\"\"\n\n        if backend not in [\"meta\", \"transformers\"]:\n            raise ValueError(\n                f\"Invalid backend '{backend}'. Choose 'meta' or 'transformers'.\"\n            )\n\n        if backend == \"meta\" and not SAM3_META_AVAILABLE:\n            error_msg = (\n                \"Meta SAM3 is not available. Please install it as:\\n\"\n                \"\\tpip install segment-geospatial[samgeo3]\"\n            )\n            if SAM3_META_IMPORT_ERROR is not None:\n                error_msg += f\"\\n\\nUnderlying import error:\\n\\t{SAM3_META_IMPORT_ERROR}\"\n            raise ImportError(error_msg)\n\n        if backend == \"transformers\" and not SAM3_TRANSFORMERS_AVAILABLE:\n            error_msg = (\n                \"Transformers SAM3 is not available. Please install it as:\\n\"\n                \"\\tpip install transformers torch\"\n            )\n            if SAM3_TRANSFORMERS_IMPORT_ERROR is not None:\n                error_msg += (\n                    f\"\\n\\nUnderlying import error:\\n\\t{SAM3_TRANSFORMERS_IMPORT_ERROR}\"\n                )\n            raise ImportError(error_msg)\n\n        if device is None:\n            device = common.get_device()\n\n        print(f\"Using {device} device and {backend} backend\")\n\n        self.backend = backend\n        self.device = device\n        self.confidence_threshold = confidence_threshold\n        self.mask_threshold = mask_threshold\n        self.model_id = model_id\n        self.model_version = \"sam3\"\n\n        # Initialize backend-specific components\n        if backend == \"meta\":\n            self._init_meta_backend(\n                bpe_path=bpe_path,\n                device=device,\n                eval_mode=eval_mode,\n                checkpoint_path=checkpoint_path,\n                load_from_HF=load_from_HF,\n                enable_segmentation=enable_segmentation,\n                enable_inst_interactivity=enable_inst_interactivity,\n                compile_mode=compile_mode,\n                resolution=resolution,\n                confidence_threshold=confidence_threshold,\n            )\n        else:  # transformers\n            self._init_transformers_backend(\n                model_id=model_id,\n                device=device,\n            )\n\n        # Common attributes\n        self.predictor = None\n        self.masks = None\n        self.boxes = None\n        self.scores = None\n        self.logits = None\n        self.objects = None\n        self.prediction = None\n        self.source = None\n        self.image = None\n        self.image_height = None\n        self.image_width = None\n        self.inference_state = None\n\n        # Batch processing attributes\n        self.images_batch = None\n        self.sources_batch = None\n        self.batch_state = None\n        self.batch_results = None\n\n    def _init_meta_backend(\n        self,\n        bpe_path,\n        device,\n        eval_mode,\n        checkpoint_path,\n        load_from_HF,\n        enable_segmentation,\n        enable_inst_interactivity,\n        compile_mode,\n        resolution,\n        confidence_threshold,\n    ):\n        \"\"\"Initialize Meta SAM3 backend.\"\"\"\n        if bpe_path is None:\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            bpe_path = os.path.abspath(\n                os.path.join(current_dir, \"assets\", \"bpe_simple_vocab_16e6.txt.gz\")\n            )\n            if not os.path.exists(bpe_path):\n                bpe_dir = os.path.dirname(bpe_path)\n                os.makedirs(bpe_dir, exist_ok=True)\n                url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/bpe_simple_vocab_16e6.txt.gz\"\n                bpe_path = common.download_file(url, bpe_path, quiet=True)\n\n        if os.environ.get(\"SAM3_CHECKPOINT_PATH\") is not None:\n            checkpoint_path = os.environ.get(\"SAM3_CHECKPOINT_PATH\")\n            load_from_HF = False\n\n        if checkpoint_path is not None:\n            if not os.path.exists(checkpoint_path):\n                raise ValueError(f\"Checkpoint path {checkpoint_path} does not exist.\")\n            load_from_HF = False\n\n        model = build_sam3_image_model(\n            bpe_path=bpe_path,\n            device=device,\n            eval_mode=eval_mode,\n            checkpoint_path=checkpoint_path,\n            load_from_HF=load_from_HF,\n            enable_segmentation=enable_segmentation,\n            enable_inst_interactivity=enable_inst_interactivity,\n            compile=compile_mode,\n        )\n\n        # Ensure the model is on the correct device\n        model = model.to(device)\n\n        self.model = model\n        self.processor = MetaSam3Processor(\n            model,\n            resolution=resolution,\n            device=device,\n            confidence_threshold=confidence_threshold,\n        )\n\n    def _init_transformers_backend(self, model_id, device):\n        \"\"\"Initialize Transformers SAM3 backend.\"\"\"\n        self.model = Sam3Model.from_pretrained(model_id).to(device)\n        self.processor = TransformersSam3Processor.from_pretrained(model_id)\n\n    def set_confidence_threshold(self, threshold: float, state=None):\n        \"\"\"Sets the confidence threshold for the masks.\n        Args:\n            threshold (float): The confidence threshold.\n            state (optional): An optional state object to pass to the processor's set_confidence_threshold method (Meta backend only).\n        \"\"\"\n        self.confidence_threshold = threshold\n        if self.backend == \"meta\":\n            self.inference_state = self.processor.set_confidence_threshold(\n                threshold, state\n            )\n        # For transformers backend, the threshold is stored and used during generate_masks\n\n    def set_image(\n        self,\n        image: Union[str, np.ndarray],\n        state=None,\n        bands: Optional[List[int]] = None,\n    ) -&gt; None:\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (Union[str, np.ndarray, Image]): The input image as a path,\n                a numpy array, or an Image.\n            state (optional): An optional state object to pass to the processor's set_image method (Meta backend only).\n            bands (List[int], optional): List of band indices (1-based) to use for RGB\n                when the input is a GeoTIFF with more than 3 bands. For example,\n                [4, 3, 2] for NIR-R-G false color composite. If None, uses the\n                first 3 bands for multi-band images. Defaults to None.\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            # Check if image is a GeoTIFF and handle band selection\n            if image.lower().endswith((\".tif\", \".tiff\")):\n                import rasterio\n\n                with rasterio.open(image) as src:\n                    if bands is not None:\n                        # Validate band indices (1-based)\n                        if len(bands) != 3:\n                            raise ValueError(\n                                \"bands must contain exactly 3 band indices for RGB.\"\n                            )\n                        for band in bands:\n                            if band &lt; 1 or band &gt; src.count:\n                                raise ValueError(\n                                    f\"Band index {band} is out of range. \"\n                                    f\"Image has {src.count} bands (1-indexed).\"\n                                )\n                        # Read specified bands (rasterio uses 1-based indexing)\n                        array = np.stack([src.read(b) for b in bands], axis=0)\n                    else:\n                        # Read all bands\n                        array = src.read()\n                        # If more than 3 bands, use first 3\n                        if array.shape[0] &gt;= 3:\n                            array = array[:3, :, :]\n                        elif array.shape[0] == 1:\n                            array = np.repeat(array, 3, axis=0)\n                        elif array.shape[0] == 2:\n                            # Repeat the first band to make 3 bands: [band1, band2, band1]\n                            array = np.concatenate([array, array[0:1, :, :]], axis=0)\n                    # Transpose from (bands, height, width) to (height, width, bands)\n                    array = np.transpose(array, (1, 2, 0))\n\n                    # Normalize to 8-bit (0-255) range\n                    array = array.astype(np.float32)\n                    array -= array.min()\n                    if array.max() &gt; 0:\n                        array /= array.max()\n                    array *= 255\n                    image = array.astype(np.uint8)\n            else:\n                image = cv2.imread(image)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            self.image = image\n            self.source = None\n        elif isinstance(image, Image.Image):\n            self.image = np.array(image)\n            self.source = None\n        else:\n            raise ValueError(\n                \"Input image must be either a path, numpy array, or PIL Image.\"\n            )\n\n        self.image_height, self.image_width = self.image.shape[:2]\n\n        # Convert to PIL Image for processing\n        image_for_processor = Image.fromarray(self.image)\n\n        # Set image based on backend\n        if self.backend == \"meta\":\n            # SAM3's processor expects PIL Image or tensor with (C, H, W) format\n            # Numpy arrays from cv2 have (H, W, C) format which causes incorrect dimension extraction\n            self.inference_state = self.processor.set_image(\n                image_for_processor, state=state\n            )\n        else:  # transformers\n            # For Transformers backend, we just store the PIL image\n            # Processing will happen during generate_masks\n            self.pil_image = image_for_processor\n\n    def set_image_batch(\n        self,\n        images: List[Union[str, np.ndarray, Image.Image]],\n        state: Optional[Dict] = None,\n    ) -&gt; None:\n        \"\"\"Set multiple images for batch processing.\n\n        Note: This method is only available for the Meta backend.\n\n        Args:\n            images (List[Union[str, np.ndarray, Image]]): A list of input images.\n                Each image can be a file path, a numpy array, or a PIL Image.\n            state (dict, optional): An optional state object to pass to the\n                processor's set_image_batch method.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n            &gt;&gt;&gt; sam.set_image_batch([\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"])\n            &gt;&gt;&gt; results = sam.generate_masks_batch(\"tree\")\n        \"\"\"\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"Batch image processing is only available for the Meta backend. \"\n                \"Use set_image() for the Transformers backend.\"\n            )\n\n        if not isinstance(images, list) or len(images) == 0:\n            raise ValueError(\"images must be a non-empty list\")\n\n        # Process each image to PIL format\n        pil_images = []\n        sources = []\n        numpy_images = []\n\n        for image in images:\n            if isinstance(image, str):\n                if image.startswith(\"http\"):\n                    image = common.download_file(image)\n\n                if not os.path.exists(image):\n                    raise ValueError(f\"Input path {image} does not exist.\")\n\n                sources.append(image)\n                img = cv2.imread(image)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                numpy_images.append(img)\n                pil_images.append(Image.fromarray(img))\n\n            elif isinstance(image, np.ndarray):\n                sources.append(None)\n                numpy_images.append(image)\n                pil_images.append(Image.fromarray(image))\n\n            elif isinstance(image, Image.Image):\n                sources.append(None)\n                numpy_images.append(np.array(image))\n                pil_images.append(image)\n\n            else:\n                raise ValueError(\n                    \"Each image must be either a path, numpy array, or PIL Image.\"\n                )\n\n        # Store batch information\n        self.images_batch = numpy_images\n        self.sources_batch = sources\n\n        # Call the processor's set_image_batch method\n        self.batch_state = self.processor.set_image_batch(pil_images, state=state)\n\n        print(f\"Set {len(pil_images)} images for batch processing.\")\n\n    def generate_masks_batch(\n        self,\n        prompt: str,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Generate masks for all images in the batch using SAM3.\n\n        Note: This method is only available for the Meta backend.\n\n        Args:\n            prompt (str): The text prompt describing the objects to segment.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n            &gt;&gt;&gt; sam.set_image_batch([\"image1.jpg\", \"image2.jpg\"])\n            &gt;&gt;&gt; results = sam.generate_masks_batch(\"building\")\n            &gt;&gt;&gt; for i, result in enumerate(results):\n            ...     print(f\"Image {i}: Found {len(result['masks'])} objects\")\n        \"\"\"\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"Batch mask generation is only available for the Meta backend.\"\n            )\n\n        if self.batch_state is None:\n            raise ValueError(\n                \"No images set for batch processing. \"\n                \"Please call set_image_batch() first.\"\n            )\n\n        batch_results = []\n        num_images = len(self.images_batch)\n\n        # The batch backbone features are computed once, but text prompting\n        # needs to be done per-image since set_text_prompt expects singular\n        # original_height/original_width keys\n        backbone_out = self.batch_state.get(\"backbone_out\", {})\n\n        for i in range(num_images):\n            # Create a per-image state with the correct singular keys\n            image_state = {\n                \"original_height\": self.batch_state[\"original_heights\"][i],\n                \"original_width\": self.batch_state[\"original_widths\"][i],\n            }\n\n            # Extract backbone features for this specific image\n            # The backbone_out contains batched features, we need to slice them\n            image_backbone_out = self._extract_image_backbone_features(backbone_out, i)\n            image_state[\"backbone_out\"] = image_backbone_out\n\n            # Reset prompts and set text prompt for this image\n            self.processor.reset_all_prompts(image_state)\n            output = self.processor.set_text_prompt(state=image_state, prompt=prompt)\n\n            # Build result for this image\n            result = {\n                \"masks\": output.get(\"masks\", []),\n                \"boxes\": output.get(\"boxes\", []),\n                \"scores\": output.get(\"scores\", []),\n                \"image\": self.images_batch[i],\n                \"source\": self.sources_batch[i],\n            }\n\n            # Convert tensors to numpy\n            result = self._convert_batch_result_to_numpy(result)\n\n            # Filter by size if needed\n            if min_size &gt; 0 or max_size is not None:\n                result = self._filter_batch_result_by_size(result, min_size, max_size)\n\n            batch_results.append(result)\n\n        self.batch_results = batch_results\n\n        # Print summary\n        total_objects = sum(len(r.get(\"masks\", [])) for r in batch_results)\n        print(\n            f\"Processed {num_images} image(s), found {total_objects} total object(s).\"\n        )\n\n    def _extract_image_backbone_features(\n        self, backbone_out: Dict[str, Any], image_index: int\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Extract backbone features for a single image from batched features.\n\n        Args:\n            backbone_out: Batched backbone output from set_image_batch.\n            image_index: Index of the image to extract features for.\n\n        Returns:\n            Dictionary containing backbone features for a single image.\n        \"\"\"\n        import torch\n\n        image_backbone = {}\n\n        for key, value in backbone_out.items():\n            # Skip None values\n            if value is None:\n                image_backbone[key] = None\n                continue\n\n            if key == \"sam2_backbone_out\":\n                # Handle nested sam2 backbone output\n                if not isinstance(value, dict):\n                    image_backbone[key] = value\n                    continue\n\n                sam2_out = {}\n                for sam2_key, sam2_value in value.items():\n                    if sam2_value is None:\n                        sam2_out[sam2_key] = None\n                    elif sam2_key == \"backbone_fpn\":\n                        # backbone_fpn is a list of feature tensors\n                        sam2_out[sam2_key] = [\n                            (\n                                feat[image_index : image_index + 1]\n                                if isinstance(feat, torch.Tensor)\n                                else feat\n                            )\n                            for feat in sam2_value\n                        ]\n                    elif isinstance(sam2_value, torch.Tensor):\n                        sam2_out[sam2_key] = sam2_value[image_index : image_index + 1]\n                    elif isinstance(sam2_value, list):\n                        sam2_out[sam2_key] = [\n                            (\n                                v[image_index : image_index + 1]\n                                if isinstance(v, torch.Tensor)\n                                else v\n                            )\n                            for v in sam2_value\n                        ]\n                    else:\n                        sam2_out[sam2_key] = sam2_value\n                image_backbone[key] = sam2_out\n            elif isinstance(value, torch.Tensor):\n                # Slice the batch dimension\n                image_backbone[key] = value[image_index : image_index + 1]\n            elif isinstance(value, list):\n                # Handle list of tensors\n                image_backbone[key] = [\n                    (\n                        v[image_index : image_index + 1]\n                        if isinstance(v, torch.Tensor)\n                        else v\n                    )\n                    for v in value\n                ]\n            elif isinstance(value, dict):\n                # Recursively handle nested dicts\n                nested = {}\n                for k, v in value.items():\n                    if v is None:\n                        nested[k] = None\n                    elif isinstance(v, torch.Tensor):\n                        nested[k] = v[image_index : image_index + 1]\n                    elif isinstance(v, list):\n                        nested[k] = [\n                            (\n                                item[image_index : image_index + 1]\n                                if isinstance(item, torch.Tensor)\n                                else item\n                            )\n                            for item in v\n                        ]\n                    else:\n                        nested[k] = v\n                image_backbone[key] = nested\n            else:\n                image_backbone[key] = value\n\n        return image_backbone\n\n    def _convert_batch_result_to_numpy(self, result: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Convert masks, boxes, and scores in a batch result to numpy arrays.\n\n        Args:\n            result: Dictionary containing masks, boxes, scores for one image.\n\n        Returns:\n            Dictionary with numpy arrays instead of tensors.\n        \"\"\"\n        import torch\n\n        # Helper to check if a value is non-empty\n        def has_items(val):\n            if val is None:\n                return False\n            if isinstance(val, (list, tuple)):\n                return len(val) &gt; 0\n            if isinstance(val, torch.Tensor):\n                return val.numel() &gt; 0\n            if isinstance(val, np.ndarray):\n                return val.size &gt; 0\n            return bool(val)\n\n        # Convert masks\n        masks = result.get(\"masks\")\n        if has_items(masks):\n            converted_masks = []\n            # Handle case where masks is a single tensor with batch dimension\n            if isinstance(masks, torch.Tensor):\n                # If it's a batched tensor, split into list\n                if masks.dim() &gt;= 3:\n                    for i in range(masks.shape[0]):\n                        converted_masks.append(masks[i].cpu().numpy())\n                else:\n                    converted_masks.append(masks.cpu().numpy())\n            else:\n                # It's already a list\n                for mask in masks:\n                    if hasattr(mask, \"cpu\"):\n                        mask_np = mask.cpu().numpy()\n                    elif hasattr(mask, \"numpy\"):\n                        mask_np = mask.numpy()\n                    else:\n                        mask_np = np.asarray(mask)\n                    converted_masks.append(mask_np)\n            result[\"masks\"] = converted_masks\n\n        # Convert boxes\n        boxes = result.get(\"boxes\")\n        if has_items(boxes):\n            converted_boxes = []\n            if isinstance(boxes, torch.Tensor):\n                # If it's a batched tensor [N, 4], split into list\n                if boxes.dim() == 2:\n                    for i in range(boxes.shape[0]):\n                        converted_boxes.append(boxes[i].cpu().numpy())\n                else:\n                    converted_boxes.append(boxes.cpu().numpy())\n            else:\n                for box in boxes:\n                    if hasattr(box, \"cpu\"):\n                        box_np = box.cpu().numpy()\n                    elif hasattr(box, \"numpy\"):\n                        box_np = box.numpy()\n                    else:\n                        box_np = np.asarray(box)\n                    converted_boxes.append(box_np)\n            result[\"boxes\"] = converted_boxes\n\n        # Convert scores\n        scores = result.get(\"scores\")\n        if has_items(scores):\n            converted_scores = []\n            if isinstance(scores, torch.Tensor):\n                # If it's a 1D tensor of scores\n                scores_np = scores.cpu().numpy()\n                if scores_np.ndim == 0:\n                    converted_scores.append(float(scores_np))\n                else:\n                    for s in scores_np:\n                        converted_scores.append(float(s))\n            else:\n                for score in scores:\n                    if hasattr(score, \"cpu\"):\n                        score_val = (\n                            score.cpu().item()\n                            if hasattr(score, \"numel\") and score.numel() == 1\n                            else score.cpu().numpy()\n                        )\n                    elif hasattr(score, \"item\"):\n                        score_val = score.item()\n                    elif hasattr(score, \"numpy\"):\n                        score_val = score.numpy()\n                    else:\n                        score_val = float(score)\n                    converted_scores.append(score_val)\n            result[\"scores\"] = converted_scores\n\n        return result\n\n    def _filter_batch_result_by_size(\n        self,\n        result: Dict[str, Any],\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Filter masks in a batch result by size.\n\n        Args:\n            result: Dictionary containing masks, boxes, scores for one image.\n            min_size: Minimum mask size in pixels.\n            max_size: Maximum mask size in pixels.\n\n        Returns:\n            Filtered result dictionary.\n        \"\"\"\n        if not result.get(\"masks\"):\n            return result\n\n        filtered_masks = []\n        filtered_boxes = []\n        filtered_scores = []\n\n        masks = result[\"masks\"]\n        boxes = result.get(\"boxes\", [])\n        scores = result.get(\"scores\", [])\n\n        for i, mask in enumerate(masks):\n            # Convert mask to numpy if needed\n            if hasattr(mask, \"cpu\"):\n                mask_np = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask_np = mask.squeeze().numpy()\n            else:\n                mask_np = np.squeeze(mask) if hasattr(mask, \"squeeze\") else mask\n\n            # Ensure mask is 2D\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np[0]\n\n            # Calculate mask size\n            mask_bool = mask_np &gt; 0\n            mask_size = np.sum(mask_bool)\n\n            # Filter by size\n            if mask_size &lt; min_size:\n                continue\n            if max_size is not None and mask_size &gt; max_size:\n                continue\n\n            # Keep this mask\n            filtered_masks.append(masks[i])\n            if i &lt; len(boxes):\n                filtered_boxes.append(boxes[i])\n            if i &lt; len(scores):\n                filtered_scores.append(scores[i])\n\n        result[\"masks\"] = filtered_masks\n        result[\"boxes\"] = filtered_boxes\n        result[\"scores\"] = filtered_scores\n\n        return result\n\n    def save_masks_batch(\n        self,\n        output_dir: str,\n        prefix: str = \"mask\",\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        dtype: str = \"uint8\",\n        **kwargs: Any,\n    ) -&gt; List[str]:\n        \"\"\"Save masks from batch processing to files.\n\n        Args:\n            output_dir (str): Directory to save the mask files.\n            prefix (str): Prefix for output filenames. Files will be named\n                \"{prefix}_{index}.tif\" or \"{prefix}_{index}.png\".\n            unique (bool): If True, each mask gets a unique value (1, 2, 3, ...).\n                If False, all masks are combined into a binary mask.\n            min_size (int): Minimum mask size in pixels.\n            max_size (int, optional): Maximum mask size in pixels.\n            dtype (str): Data type for the output array.\n            **kwargs: Additional arguments passed to common.array_to_image().\n\n        Returns:\n            List[str]: List of paths to saved mask files.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n            &gt;&gt;&gt; sam.set_image_batch([\"img1.tif\", \"img2.tif\"])\n            &gt;&gt;&gt; sam.generate_masks_batch(\"building\")\n            &gt;&gt;&gt; saved_files = sam.save_masks_batch(\"output/\", prefix=\"building_mask\")\n        \"\"\"\n        if self.batch_results is None:\n            raise ValueError(\n                \"No batch results found. Please run generate_masks_batch() first.\"\n            )\n\n        os.makedirs(output_dir, exist_ok=True)\n        saved_files = []\n\n        for i, result in enumerate(self.batch_results):\n            masks = result.get(\"masks\", [])\n            source = result.get(\"source\")\n            image = result.get(\"image\")\n\n            if not masks:\n                print(f\"No masks for image {i + 1}, skipping.\")\n                continue\n\n            # Get image dimensions\n            if image is not None:\n                height, width = image.shape[:2]\n            else:\n                # Try to get from first mask\n                mask = masks[0]\n                if hasattr(mask, \"shape\"):\n                    if mask.ndim &gt; 2:\n                        height, width = mask.shape[-2:]\n                    else:\n                        height, width = mask.shape\n                else:\n                    raise ValueError(f\"Cannot determine dimensions for image {i}\")\n\n            # Create combined mask array\n            mask_array = np.zeros(\n                (height, width), dtype=np.uint32 if unique else np.uint8\n            )\n\n            valid_mask_count = 0\n            for j, mask in enumerate(masks):\n                # Convert to numpy\n                if hasattr(mask, \"cpu\"):\n                    mask_np = mask.squeeze().cpu().numpy()\n                elif hasattr(mask, \"numpy\"):\n                    mask_np = mask.squeeze().numpy()\n                else:\n                    mask_np = np.squeeze(mask) if hasattr(mask, \"squeeze\") else mask\n\n                if mask_np.ndim &gt; 2:\n                    mask_np = mask_np[0]\n\n                mask_bool = mask_np &gt; 0\n                mask_size = np.sum(mask_bool)\n\n                if mask_size &lt; min_size:\n                    continue\n                if max_size is not None and mask_size &gt; max_size:\n                    continue\n\n                if unique:\n                    mask_array[mask_bool] = valid_mask_count + 1\n                else:\n                    mask_array[mask_bool] = 255\n\n                valid_mask_count += 1\n\n            if valid_mask_count == 0:\n                print(f\"No valid masks for image {i + 1} after filtering.\")\n                continue\n\n            # Convert dtype\n            if unique and valid_mask_count &gt; np.iinfo(np.dtype(dtype)).max:\n                print(\n                    f\"Warning: {valid_mask_count} masks exceed {dtype} range. Consider using uint16 or uint32.\"\n                )\n            mask_array = mask_array.astype(dtype)\n\n            # Determine output path and extension\n            if source is not None and source.lower().endswith((\".tif\", \".tiff\")):\n                ext = \".tif\"\n            else:\n                ext = \".png\"\n\n            output_path = os.path.join(output_dir, f\"{prefix}_{i + 1}{ext}\")\n\n            # Save\n            common.array_to_image(\n                mask_array, output_path, source, dtype=dtype, **kwargs\n            )\n            saved_files.append(output_path)\n            print(\n                f\"Saved {valid_mask_count} mask(s) for image {i + 1} to {output_path}\"\n            )\n\n        return saved_files\n\n    def show_anns_batch(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        show_bbox: bool = True,\n        show_score: bool = True,\n        output_dir: Optional[str] = None,\n        prefix: str = \"anns\",\n        blend: bool = True,\n        alpha: float = 0.5,\n        ncols: int = 2,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show annotations for all images in the batch.\n\n        Args:\n            figsize (tuple): Figure size for each subplot.\n            axis (str): Whether to show axis.\n            show_bbox (bool): Whether to show bounding boxes.\n            show_score (bool): Whether to show confidence scores.\n            output_dir (str, optional): Directory to save annotation images.\n                If None, displays the figure.\n            prefix (str): Prefix for output filenames.\n            blend (bool): Whether to show image as background.\n            alpha (float): Alpha value for mask overlay.\n            ncols (int): Number of columns in the grid display.\n            **kwargs: Additional arguments for saving.\n        \"\"\"\n        if self.batch_results is None:\n            raise ValueError(\n                \"No batch results found. Please run generate_masks_batch() first.\"\n            )\n\n        num_images = len(self.batch_results)\n\n        if output_dir is not None:\n            os.makedirs(output_dir, exist_ok=True)\n            # Save each image separately\n            for i, result in enumerate(self.batch_results):\n                self._show_single_ann(\n                    result,\n                    figsize=figsize,\n                    axis=axis,\n                    show_bbox=show_bbox,\n                    show_score=show_score,\n                    blend=blend,\n                    alpha=alpha,\n                    output=os.path.join(output_dir, f\"{prefix}_{i + 1}.png\"),\n                    **kwargs,\n                )\n        else:\n            # Display in grid\n            nrows = (num_images + ncols - 1) // ncols\n            fig, axes = plt.subplots(\n                nrows, ncols, figsize=(figsize[0] * ncols, figsize[1] * nrows)\n            )\n\n            if num_images == 1 or ncols == 1 or nrows == 1:\n                axes = np.array([axes]).flatten()\n            else:\n                axes = axes.flatten()\n            for i, result in enumerate(self.batch_results):\n                ax = axes[i]\n                self._show_single_ann(\n                    result,\n                    figsize=figsize,\n                    axis=axis,\n                    show_bbox=show_bbox,\n                    show_score=show_score,\n                    blend=blend,\n                    alpha=alpha,\n                    ax=ax,\n                )\n                ax.set_title(f\"Image {i + 1}\")\n\n            # Hide unused subplots\n            for j in range(num_images, len(axes)):\n                axes[j].axis(\"off\")\n\n            plt.tight_layout()\n            plt.show()\n\n    def _show_single_ann(\n        self,\n        result: Dict[str, Any],\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        show_bbox: bool = True,\n        show_score: bool = True,\n        blend: bool = True,\n        alpha: float = 0.5,\n        output: Optional[str] = None,\n        ax: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show annotations for a single batch result.\n\n        Args:\n            result: Dictionary containing masks, boxes, scores, and image.\n            figsize: Figure size.\n            axis: Whether to show axis.\n            show_bbox: Whether to show bounding boxes.\n            show_score: Whether to show scores.\n            blend: Whether to blend with original image.\n            alpha: Alpha for mask overlay.\n            output: Path to save the figure.\n            ax: Matplotlib axis to plot on.\n            **kwargs: Additional arguments for saving.\n        \"\"\"\n        image = result.get(\"image\")\n        masks = result.get(\"masks\", [])\n        boxes = result.get(\"boxes\", [])\n        scores = result.get(\"scores\", [])\n\n        if image is None or len(masks) == 0:\n            return\n\n        if ax is None:\n            fig = plt.figure(figsize=figsize)\n            ax = plt.gca()\n            own_figure = True\n        else:\n            own_figure = False\n\n        img_pil = Image.fromarray(image)\n\n        if blend:\n            ax.imshow(img_pil)\n        else:\n            white_background = np.ones_like(image) * 255\n            ax.imshow(white_background)\n\n        h, w = image.shape[:2]\n        COLORS = generate_colors(n_colors=128, n_samples=5000)\n\n        for i in range(len(masks)):\n            color = COLORS[i % len(COLORS)]\n\n            mask = masks[i]\n            if hasattr(mask, \"cpu\"):\n                mask = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask = mask.squeeze().numpy()\n            else:\n                mask = np.squeeze(mask)\n\n            if mask.ndim &gt; 2:\n                mask = mask[0]\n\n            plot_mask(mask, color=color, alpha=alpha, ax=ax)\n\n            if show_bbox and i &lt; len(boxes):\n                score = scores[i] if i &lt; len(scores) else 0.0\n                if hasattr(score, \"item\"):\n                    prob = score.item()\n                else:\n                    prob = float(score)\n\n                if show_score:\n                    text = f\"(id={i}, {prob=:.2f})\"\n                else:\n                    text = f\"(id={i})\"\n\n                box = boxes[i]\n                if hasattr(box, \"cpu\"):\n                    box = box.cpu().numpy()\n                elif hasattr(box, \"numpy\"):\n                    box = box.numpy()\n\n                plot_bbox(\n                    h,\n                    w,\n                    box,\n                    text=text,\n                    box_format=\"XYXY\",\n                    color=color,\n                    relative_coords=False,\n                    ax=ax,\n                )\n\n        ax.axis(axis)\n\n        if output is not None and own_figure:\n            save_kwargs = {\"bbox_inches\": \"tight\", \"pad_inches\": 0.1, \"dpi\": 100}\n            save_kwargs.update(kwargs)\n            plt.savefig(output, **save_kwargs)\n            print(f\"Saved annotations to {output}\")\n            plt.close(fig)\n        elif own_figure:\n            plt.show()\n\n    def generate_masks(\n        self,\n        prompt: str,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        quiet: bool = False,\n        **kwargs: Any,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Generate masks for the input image using SAM3.\n\n        Args:\n            prompt (str): The text prompt describing the objects to segment.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n            quiet (bool): If True, suppress progress messages. Defaults to False.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n        \"\"\"\n        if self.backend == \"meta\":\n            self.processor.reset_all_prompts(self.inference_state)\n            output = self.processor.set_text_prompt(\n                state=self.inference_state, prompt=prompt\n            )\n\n            self.masks = output[\"masks\"]\n            self.boxes = output[\"boxes\"]\n            self.scores = output[\"scores\"]\n        else:  # transformers\n            if not hasattr(self, \"pil_image\"):\n                raise ValueError(\"No image set. Please call set_image() first.\")\n\n            # Prepare inputs\n            inputs = self.processor(\n                images=self.pil_image, text=prompt, return_tensors=\"pt\"\n            ).to(self.device)\n\n            # Get original sizes for post-processing\n            original_sizes = inputs.get(\"original_sizes\")\n            if original_sizes is not None:\n                original_sizes = original_sizes.tolist()\n            else:\n                original_sizes = [[self.image_height, self.image_width]]\n\n            # Run inference\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n\n            # Post-process results\n            results = self.processor.post_process_instance_segmentation(\n                outputs,\n                threshold=self.confidence_threshold,\n                mask_threshold=self.mask_threshold,\n                target_sizes=original_sizes,\n            )[0]\n\n            # Convert results to match Meta backend format\n            self.masks = results[\"masks\"]\n            self.boxes = results[\"boxes\"]\n            self.scores = results[\"scores\"]\n\n        # Convert tensors to numpy to free GPU memory\n        self._convert_results_to_numpy()\n\n        # Filter masks by size if min_size or max_size is specified\n        if min_size &gt; 0 or max_size is not None:\n            self._filter_masks_by_size(min_size, max_size)\n\n        num_objects = len(self.masks)\n        if not quiet:\n            if num_objects == 0:\n                print(\"No objects found. Please try a different prompt.\")\n            elif num_objects == 1:\n                print(\"Found one object.\")\n            else:\n                print(f\"Found {num_objects} objects.\")\n\n    def generate_masks_tiled(\n        self,\n        source: str,\n        prompt: str,\n        output: str,\n        tile_size: int = 1024,\n        overlap: int = 128,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        unique: bool = True,\n        dtype: str = \"uint32\",\n        bands: Optional[List[int]] = None,\n        batch_size: int = 1,\n        verbose: bool = True,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Generate masks for large GeoTIFF images using a sliding window approach.\n\n        This method processes large images tile by tile to avoid GPU memory issues.\n        The tiles are processed with overlap to ensure seamless mask merging at\n        boundaries. Each detected object gets a unique ID that is consistent\n        across the entire image.\n\n        Args:\n            source (str): Path to the input GeoTIFF image.\n            prompt (str): The text prompt describing the objects to segment.\n            output (str): Path to the output GeoTIFF file.\n            tile_size (int): Size of each tile in pixels. Defaults to 1024.\n            overlap (int): Overlap between adjacent tiles in pixels. Defaults to 128.\n                Higher overlap helps with better boundary merging but increases\n                processing time.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n            unique (bool): If True, each mask gets a unique value. If False, binary\n                mask (0 or 1). Defaults to True.\n            dtype (str): Data type for the output array. Use 'uint32' for large\n                numbers of objects, 'uint16' for up to 65535 objects, or 'uint8'\n                for up to 255 objects. Defaults to 'uint32'.\n            bands (List[int], optional): List of band indices (1-based) to use for RGB\n                when the input has more than 3 bands. If None, uses first 3 bands.\n            batch_size (int): Number of tiles to process at once (future use).\n                Defaults to 1.\n            verbose (bool): Whether to print progress information. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: Path to the output GeoTIFF file.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n            &gt;&gt;&gt; sam.generate_masks_tiled(\n            ...     source=\"large_satellite_image.tif\",\n            ...     prompt=\"building\",\n            ...     output=\"buildings_mask.tif\",\n            ...     tile_size=1024,\n            ...     overlap=128,\n            ... )\n        \"\"\"\n        import rasterio\n        from rasterio.windows import Window\n\n        if not source.lower().endswith((\".tif\", \".tiff\")):\n            raise ValueError(\"Source must be a GeoTIFF file for tiled processing.\")\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Source file not found: {source}\")\n\n        if tile_size &lt;= overlap:\n            raise ValueError(\"tile_size must be greater than overlap\")\n\n        # Open the source file to get metadata\n        with rasterio.open(source) as src:\n            img_height = src.height\n            img_width = src.width\n            profile = src.profile.copy()\n\n        if verbose:\n            print(f\"Processing image: {img_width} x {img_height} pixels\")\n            print(f\"Tile size: {tile_size}, Overlap: {overlap}\")\n\n        # Calculate the number of tiles\n        step = tile_size - overlap\n        n_tiles_x = max(1, (img_width - overlap + step - 1) // step)\n        n_tiles_y = max(1, (img_height - overlap + step - 1) // step)\n        total_tiles = n_tiles_x * n_tiles_y\n\n        if verbose:\n            print(f\"Total tiles to process: {total_tiles} ({n_tiles_x} x {n_tiles_y})\")\n\n        # Determine output dtype\n        if dtype == \"uint8\":\n            np_dtype = np.uint8\n            max_objects = 255\n        elif dtype == \"uint16\":\n            np_dtype = np.uint16\n            max_objects = 65535\n        elif dtype == \"uint32\":\n            np_dtype = np.uint32\n            max_objects = 4294967295\n        else:\n            np_dtype = np.uint32\n            max_objects = 4294967295\n\n        # Create output array in memory (for smaller images) or use memory-mapped file\n        # For very large images, you might want to use rasterio windowed writing\n        output_mask = np.zeros((img_height, img_width), dtype=np_dtype)\n\n        # Track unique object IDs across all tiles\n        current_max_id = 0\n        total_objects = 0\n\n        # Process each tile\n        tile_iterator = tqdm(\n            range(total_tiles),\n            desc=\"Processing tiles\",\n            disable=not verbose,\n        )\n\n        for tile_idx in tile_iterator:\n            # Calculate tile position\n            tile_y = tile_idx // n_tiles_x\n            tile_x = tile_idx % n_tiles_x\n\n            # Calculate window coordinates\n            x_start = tile_x * step\n            y_start = tile_y * step\n\n            # Ensure we don't go beyond image bounds\n            x_end = min(x_start + tile_size, img_width)\n            y_end = min(y_start + tile_size, img_height)\n\n            # Adjust start if we're at the edge\n            if x_end - x_start &lt; tile_size and x_start &gt; 0:\n                x_start = max(0, x_end - tile_size)\n            if y_end - y_start &lt; tile_size and y_start &gt; 0:\n                y_start = max(0, y_end - tile_size)\n\n            window_width = x_end - x_start\n            window_height = y_end - y_start\n\n            # Read tile from source\n            with rasterio.open(source) as src:\n                window = Window(x_start, y_start, window_width, window_height)\n                if bands is not None:\n                    tile_data = np.stack(\n                        [src.read(b, window=window) for b in bands], axis=0\n                    )\n                else:\n                    tile_data = src.read(window=window)\n                    if tile_data.shape[0] &gt;= 3:\n                        tile_data = tile_data[:3, :, :]\n                    elif tile_data.shape[0] == 1:\n                        tile_data = np.repeat(tile_data, 3, axis=0)\n                    elif tile_data.shape[0] == 2:\n                        tile_data = np.concatenate(\n                            [tile_data, tile_data[0:1, :, :]], axis=0\n                        )\n\n            # Transpose to (height, width, channels)\n            tile_data = np.transpose(tile_data, (1, 2, 0))\n\n            # Normalize to 8-bit\n            tile_data = tile_data.astype(np.float32)\n            tile_data -= tile_data.min()\n            if tile_data.max() &gt; 0:\n                tile_data /= tile_data.max()\n            tile_data *= 255\n            tile_image = tile_data.astype(np.uint8)\n\n            # Process the tile\n            try:\n                # Set image for the tile\n                self.image = tile_image\n                self.image_height, self.image_width = tile_image.shape[:2]\n                self.source = None  # Don't need georef for individual tiles\n\n                # Initialize inference state for this tile\n                pil_image = Image.fromarray(tile_image)\n                self.pil_image = pil_image\n\n                if self.backend == \"meta\":\n                    self.inference_state = self.processor.set_image(pil_image)\n                else:\n                    # For transformers backend, process directly\n                    pass\n\n                # Generate masks for this tile (quiet=True to avoid per-tile messages)\n                self.generate_masks(\n                    prompt, min_size=min_size, max_size=max_size, quiet=True\n                )\n\n                # Get masks for this tile\n                tile_masks = self.masks\n\n                if tile_masks is not None and len(tile_masks) &gt; 0:\n                    # Create a mask array for this tile\n                    tile_mask_array = np.zeros(\n                        (window_height, window_width), dtype=np_dtype\n                    )\n\n                    for mask in tile_masks:\n                        # Convert mask to numpy\n                        if hasattr(mask, \"cpu\"):\n                            mask_np = mask.squeeze().cpu().numpy()\n                        elif hasattr(mask, \"numpy\"):\n                            mask_np = mask.squeeze().numpy()\n                        else:\n                            mask_np = (\n                                mask.squeeze() if hasattr(mask, \"squeeze\") else mask\n                            )\n\n                        if mask_np.ndim &gt; 2:\n                            mask_np = mask_np[0]\n\n                        # Resize mask to tile size if needed\n                        if mask_np.shape != (window_height, window_width):\n                            mask_np = cv2.resize(\n                                mask_np.astype(np.float32),\n                                (window_width, window_height),\n                                interpolation=cv2.INTER_NEAREST,\n                            )\n\n                        mask_bool = mask_np &gt; 0\n                        mask_size = np.sum(mask_bool)\n\n                        # Filter by size\n                        if mask_size &lt; min_size:\n                            continue\n                        if max_size is not None and mask_size &gt; max_size:\n                            continue\n\n                        if unique:\n                            current_max_id += 1\n                            if current_max_id &gt; max_objects:\n                                raise ValueError(\n                                    f\"Maximum number of objects ({max_objects}) exceeded. \"\n                                    \"Consider using a larger dtype or reducing the number of objects.\"\n                                )\n                            tile_mask_array[mask_bool] = current_max_id\n                        else:\n                            tile_mask_array[mask_bool] = 1\n\n                        total_objects += 1\n\n                    # Merge tile mask into output mask\n                    # For overlapping regions, use the tile's values if they are non-zero\n                    # This simple approach works well for most cases\n                    self._merge_tile_mask(\n                        output_mask,\n                        tile_mask_array,\n                        x_start,\n                        y_start,\n                        x_end,\n                        y_end,\n                        overlap,\n                        tile_x,\n                        tile_y,\n                        n_tiles_x,\n                        n_tiles_y,\n                    )\n\n            except Exception as e:\n                if verbose:\n                    print(f\"Warning: Failed to process tile ({tile_x}, {tile_y}): {e}\")\n                continue\n\n            # Clear GPU memory\n            self.masks = None\n            self.boxes = None\n            self.scores = None\n            if hasattr(self, \"inference_state\"):\n                self.inference_state = None\n            # Additionally clear PyTorch CUDA cache, if available, to free GPU memory\n            try:\n                import torch\n\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            except ImportError:\n                # If torch is not installed, skip CUDA cache clearing\n                pass\n        # Update output profile\n        profile.update(\n            {\n                \"count\": 1,\n                \"dtype\": dtype,\n                \"compress\": \"deflate\",\n            }\n        )\n\n        # Save the output\n        with rasterio.open(output, \"w\", **profile) as dst:\n            dst.write(output_mask, 1)\n\n        if verbose:\n            print(f\"Saved mask to {output}\")\n            print(f\"Total objects found: {total_objects}\")\n\n        # Store result for potential visualization\n        self.objects = output_mask\n        self.source = source\n\n        return output\n\n    def _merge_tile_mask(\n        self,\n        output_mask: np.ndarray,\n        tile_mask: np.ndarray,\n        x_start: int,\n        y_start: int,\n        x_end: int,\n        y_end: int,\n        overlap: int,\n        tile_x: int,\n        tile_y: int,\n        n_tiles_x: int,\n        n_tiles_y: int,\n    ) -&gt; None:\n        \"\"\"\n        Merge a tile mask into the output mask, handling overlapping regions.\n\n        For overlapping regions, this uses a blending approach where we prioritize\n        the current tile's mask in the non-overlapping core region, and for the\n        overlap region, we keep existing values unless they are zero.\n\n        Args:\n            output_mask: The full output mask array.\n            tile_mask: The mask from the current tile.\n            x_start, y_start: Start coordinates of the tile in the output.\n            x_end, y_end: End coordinates of the tile in the output.\n            overlap: The overlap size.\n            tile_x, tile_y: Tile indices.\n            n_tiles_x, n_tiles_y: Total number of tiles in each direction.\n        \"\"\"\n        tile_height = y_end - y_start\n        tile_width = x_end - x_start\n\n        # Calculate the core region (non-overlapping part)\n        # The overlap should be split between adjacent tiles\n        left_overlap = overlap // 2 if tile_x &gt; 0 else 0\n        right_overlap = overlap // 2 if tile_x &lt; n_tiles_x - 1 else 0\n        top_overlap = overlap // 2 if tile_y &gt; 0 else 0\n        bottom_overlap = overlap // 2 if tile_y &lt; n_tiles_y - 1 else 0\n\n        # Core region in tile coordinates\n        core_x_start = left_overlap\n        core_x_end = tile_width - right_overlap\n        core_y_start = top_overlap\n        core_y_end = tile_height - bottom_overlap\n\n        # Copy core region (always overwrite)\n        out_y_start = y_start + core_y_start\n        out_y_end = y_start + core_y_end\n        out_x_start = x_start + core_x_start\n        out_x_end = x_start + core_x_end\n\n        output_mask[out_y_start:out_y_end, out_x_start:out_x_end] = tile_mask[\n            core_y_start:core_y_end, core_x_start:core_x_end\n        ]\n\n        # Handle overlap regions - only update if output is zero\n        # Top overlap\n        if top_overlap &gt; 0:\n            region = output_mask[y_start : y_start + top_overlap, out_x_start:out_x_end]\n            tile_region = tile_mask[0:top_overlap, core_x_start:core_x_end]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Bottom overlap\n        if bottom_overlap &gt; 0:\n            region = output_mask[out_y_end:y_end, out_x_start:out_x_end]\n            tile_region = tile_mask[core_y_end:tile_height, core_x_start:core_x_end]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Left overlap\n        if left_overlap &gt; 0:\n            region = output_mask[\n                out_y_start:out_y_end, x_start : x_start + left_overlap\n            ]\n            tile_region = tile_mask[core_y_start:core_y_end, 0:left_overlap]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Right overlap\n        if right_overlap &gt; 0:\n            region = output_mask[out_y_start:out_y_end, out_x_end:x_end]\n            tile_region = tile_mask[core_y_start:core_y_end, core_x_end:tile_width]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Corner overlaps\n        # Top-left\n        if top_overlap &gt; 0 and left_overlap &gt; 0:\n            region = output_mask[\n                y_start : y_start + top_overlap, x_start : x_start + left_overlap\n            ]\n            tile_region = tile_mask[0:top_overlap, 0:left_overlap]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Top-right\n        if top_overlap &gt; 0 and right_overlap &gt; 0:\n            region = output_mask[y_start : y_start + top_overlap, out_x_end:x_end]\n            tile_region = tile_mask[0:top_overlap, core_x_end:tile_width]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Bottom-left\n        if bottom_overlap &gt; 0 and left_overlap &gt; 0:\n            region = output_mask[out_y_end:y_end, x_start : x_start + left_overlap]\n            tile_region = tile_mask[core_y_end:tile_height, 0:left_overlap]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n        # Bottom-right\n        if bottom_overlap &gt; 0 and right_overlap &gt; 0:\n            region = output_mask[out_y_end:y_end, out_x_end:x_end]\n            tile_region = tile_mask[core_y_end:tile_height, core_x_end:tile_width]\n            mask = region == 0\n            region[mask] = tile_region[mask]\n\n    def generate_masks_by_boxes(\n        self,\n        boxes: List[List[float]],\n        box_labels: Optional[List[bool]] = None,\n        box_crs: Optional[str] = None,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate masks using bounding box prompts.\n\n        Args:\n            boxes (List[List[float]]): List of bounding boxes in XYXY format\n                [[xmin, ymin, xmax, ymax], ...].\n                If box_crs is None: pixel coordinates.\n                If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n            box_labels (List[bool], optional): List of boolean labels for each box.\n                True for positive prompt (include), False for negative prompt (exclude).\n                If None, all boxes are treated as positive prompts.\n            box_crs (str, optional): Coordinate reference system for box coordinates\n                (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n                If None, boxes are assumed to be in pixel coordinates.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing masks, boxes, and scores.\n\n        Example:\n            # For pixel coordinates:\n            boxes = [[100, 200, 300, 400]]\n            sam.generate_masks_by_boxes(boxes)\n\n            # For geographic coordinates (GeoTIFF):\n            boxes = [[-122.5, 37.7, -122.4, 37.8]]  # [lon_min, lat_min, lon_max, lat_max]\n            sam.generate_masks_by_boxes(boxes, box_crs=\"EPSG:4326\")\n        \"\"\"\n        if self.backend == \"meta\":\n            if self.inference_state is None:\n                raise ValueError(\"No image set. Please call set_image() first.\")\n        else:  # transformers\n            if not hasattr(self, \"pil_image\"):\n                raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if box_labels is None:\n            box_labels = [True] * len(boxes)\n\n        if len(boxes) != len(box_labels):\n            raise ValueError(\n                f\"Number of boxes ({len(boxes)}) must match number of labels ({len(box_labels)})\"\n            )\n\n        # Transform boxes from CRS to pixel coordinates if needed\n        if box_crs is not None and self.source is not None:\n            pixel_boxes = []\n            for box in boxes:\n                xmin, ymin, xmax, ymax = box\n\n                # Transform min corner\n                min_coords = np.array([[xmin, ymin]])\n                min_xy, _ = common.coords_to_xy(\n                    self.source, min_coords, box_crs, return_out_of_bounds=True\n                )\n\n                # Transform max corner\n                max_coords = np.array([[xmax, ymax]])\n                max_xy, _ = common.coords_to_xy(\n                    self.source, max_coords, box_crs, return_out_of_bounds=True\n                )\n\n                # Convert to pixel coordinates and ensure correct min/max order\n                # (geographic y increases north, pixel y increases down)\n                x1_px = min_xy[0][0]\n                y1_px = min_xy[0][1]\n                x2_px = max_xy[0][0]\n                y2_px = max_xy[0][1]\n\n                # Ensure we have correct min/max values\n                x_min_px = min(x1_px, x2_px)\n                y_min_px = min(y1_px, y2_px)\n                x_max_px = max(x1_px, x2_px)\n                y_max_px = max(y1_px, y2_px)\n\n                pixel_boxes.append([x_min_px, y_min_px, x_max_px, y_max_px])\n\n            boxes = pixel_boxes\n\n        # Get image dimensions\n        width = self.image_width\n        height = self.image_height\n\n        if self.backend == \"meta\":\n            # Reset all prompts\n            self.processor.reset_all_prompts(self.inference_state)\n\n            # Process each box\n            for box, label in zip(boxes, box_labels):\n                # Convert XYXY to CxCyWH format\n                xmin, ymin, xmax, ymax = box\n                w = xmax - xmin\n                h = ymax - ymin\n                cx = xmin + w / 2\n                cy = ymin + h / 2\n\n                # Normalize to [0, 1] range\n                norm_box = [cx / width, cy / height, w / width, h / height]\n\n                # Add geometric prompt\n                self.inference_state = self.processor.add_geometric_prompt(\n                    state=self.inference_state, box=norm_box, label=label\n                )\n\n            # Get the masks from the inference state\n            output = self.inference_state\n\n            self.masks = output[\"masks\"]\n            self.boxes = output[\"boxes\"]\n            self.scores = output[\"scores\"]\n        else:  # transformers\n            # For Transformers backend, process boxes with the processor\n            # Convert boxes to the format expected by Transformers\n            # Transformers expects boxes in XYXY format with 3 levels of nesting:\n            # [image level, box level, box coordinates]\n            # Also convert numpy types to Python native types\n            input_boxes = [\n                [[float(coord) for coord in box] for box in boxes]\n            ]  # Wrap in list for image level and convert to float\n\n            # Prepare inputs with boxes\n            inputs = self.processor(\n                images=self.pil_image, input_boxes=input_boxes, return_tensors=\"pt\"\n            ).to(self.device)\n\n            # Get original sizes for post-processing\n            original_sizes = inputs.get(\"original_sizes\")\n            if original_sizes is not None:\n                original_sizes = original_sizes.tolist()\n            else:\n                original_sizes = [[self.image_height, self.image_width]]\n\n            # Run inference\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n\n            # Post-process results\n            results = self.processor.post_process_instance_segmentation(\n                outputs,\n                threshold=self.confidence_threshold,\n                mask_threshold=self.mask_threshold,\n                target_sizes=original_sizes,\n            )[0]\n\n            # Convert results to match Meta backend format\n            self.masks = results[\"masks\"]\n            self.boxes = results[\"boxes\"]\n            self.scores = results[\"scores\"]\n\n        # Convert tensors to numpy to free GPU memory\n        self._convert_results_to_numpy()\n\n        # Filter masks by size if min_size or max_size is specified\n        if min_size &gt; 0 or max_size is not None:\n            self._filter_masks_by_size(min_size, max_size)\n\n        num_objects = len(self.masks)\n        if num_objects == 0:\n            print(\"No objects found. Please check your box prompts.\")\n        elif num_objects == 1:\n            print(\"Found one object.\")\n        else:\n            print(f\"Found {num_objects} objects.\")\n\n    def generate_masks_by_points(\n        self,\n        point_coords: List[List[float]],\n        point_labels: Optional[List[int]] = None,\n        point_crs: Optional[str] = None,\n        multimask_output: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate masks using point prompts.\n\n        This is a high-level method that wraps predict_inst() for ease of use.\n        It stores the results in self.masks, self.scores, and self.boxes for\n        subsequent use with show_anns(), show_masks(), and save_masks().\n\n        Note: This method requires the model to be initialized with\n        `enable_inst_interactivity=True` (Meta backend only).\n\n        Args:\n            point_coords (List[List[float]]): List of point coordinates [[x, y], ...].\n                If point_crs is None: pixel coordinates.\n                If point_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n            point_labels (List[int], optional): List of labels for each point.\n                1 = foreground (include), 0 = background (exclude).\n                If None, all points are treated as foreground (label=1).\n            point_crs (str, optional): Coordinate reference system for point coordinates\n                (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n                If None, points are assumed to be in pixel coordinates.\n            multimask_output (bool): If True, the model returns 3 masks and the best\n                one is selected by score. Recommended for ambiguous prompts like single\n                points. If False, returns single mask directly. Defaults to True.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n            **kwargs: Additional keyword arguments passed to predict_inst().\n\n        Returns:\n            Dict[str, Any]: Dictionary containing masks, scores, and logits.\n\n        Example:\n            # Initialize with instance interactivity enabled\n            sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            sam.set_image(\"image.jpg\")\n\n            # Single foreground point (pixel coordinates)\n            sam.generate_masks_by_points([[520, 375]])\n            sam.show_anns()\n            sam.save_masks(\"mask.png\")\n\n            # Multiple points with labels\n            sam.generate_masks_by_points(\n                [[500, 375], [1125, 625]],\n                point_labels=[1, 0]  # foreground, background\n            )\n\n            # Geographic coordinates (GeoTIFF)\n            sam.generate_masks_by_points(\n                [[-122.258, 37.871]],\n                point_crs=\"EPSG:4326\"\n            )\n        \"\"\"\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"generate_masks_by_points is only available for the Meta backend. \"\n                \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n            )\n\n        if self.inference_state is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if (\n            not hasattr(self.model, \"inst_interactive_predictor\")\n            or self.model.inst_interactive_predictor is None\n        ):\n            raise ValueError(\n                \"Instance interactivity not enabled. Please initialize with \"\n                \"enable_inst_interactivity=True.\"\n            )\n\n        # Convert to numpy array if it's a list\n        point_coords = np.array(point_coords)\n\n        # Default all points to foreground if no labels provided\n        if point_labels is None:\n            point_labels = np.ones(len(point_coords), dtype=np.int32)\n        else:\n            point_labels = np.array(point_labels, dtype=np.int32)\n\n        if len(point_coords) != len(point_labels):\n            raise ValueError(\n                f\"Number of points ({len(point_coords)}) must match number of labels ({len(point_labels)})\"\n            )\n\n        # Call predict_inst with the prompts\n        masks, scores, logits = self.predict_inst(\n            point_coords=point_coords,\n            point_labels=point_labels,\n            multimask_output=multimask_output,\n            point_crs=point_crs,\n            **kwargs,\n        )\n\n        # If multimask_output=True, select the best mask by score\n        if multimask_output and len(masks) &gt; 1:\n            best_idx = np.argmax(scores)\n            masks = masks[best_idx : best_idx + 1]\n            scores = scores[best_idx : best_idx + 1]\n            logits = logits[best_idx : best_idx + 1]\n\n        # Store results in the format expected by show_anns/show_masks/save_masks\n        self.masks = (\n            [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n        )\n        self.scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n        self.logits = logits\n\n        # Compute bounding boxes from masks\n        self.boxes = []\n        for mask in self.masks:\n            if mask.ndim &gt; 2:\n                mask = mask.squeeze()\n            ys, xs = np.where(mask &gt; 0)\n            if len(xs) &gt; 0 and len(ys) &gt; 0:\n                self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n            else:\n                self.boxes.append(np.array([0, 0, 0, 0]))\n\n        # Filter masks by size if min_size or max_size is specified\n        if min_size &gt; 0 or max_size is not None:\n            self._filter_masks_by_size(min_size, max_size)\n\n        num_objects = len(self.masks)\n        if num_objects == 0:\n            print(\"No objects found. Please check your point prompts.\")\n        elif num_objects == 1:\n            print(\"Found one object.\")\n        else:\n            print(f\"Found {num_objects} objects.\")\n\n    def generate_masks_by_boxes_inst(\n        self,\n        boxes: Union[List[List[float]], str, \"gpd.GeoDataFrame\", dict, np.ndarray],\n        box_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        multimask_output: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        dtype: str = \"uint8\",\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate masks using bounding box prompts with instance interactivity.\n\n        This is a high-level method that wraps predict_inst() for ease of use.\n        It stores the results in self.masks, self.scores, and self.boxes for\n        subsequent use with show_anns(), show_masks(), and save_masks().\n\n        Note: This method requires the model to be initialized with\n        `enable_inst_interactivity=True` (Meta backend only).\n\n        Args:\n            boxes (List[List[float]] | str | GeoDataFrame): Bounding boxes for\n                segmentation. Can be:\n                - A list of [xmin, ymin, xmax, ymax] coordinates\n                - A file path to a vector file (GeoJSON, Shapefile, etc.)\n                - A GeoDataFrame with polygon geometries\n                If box_crs is None: pixel coordinates (for list input).\n                If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n            box_crs (str, optional): Coordinate reference system for box coordinates\n                (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n                If None, boxes are assumed to be in pixel coordinates.\n            output (str, optional): Path to save the output mask as a GeoTIFF.\n                If None, masks are stored in memory only.\n            multimask_output (bool): If True, the model returns 3 masks per box and the\n                best one is selected by score. If False, returns single mask directly.\n                Defaults to True.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out. Defaults to None (no maximum).\n            dtype (str): Data type for the output mask array. Defaults to \"uint8\".\n            **kwargs: Additional keyword arguments passed to predict_inst() or save_masks().\n\n        Returns:\n            Dict[str, Any]: Dictionary containing masks, scores, and logits.\n\n        Example:\n            &gt;&gt;&gt; # Initialize with instance interactivity enabled\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt; sam.set_image(\"satellite.tif\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Single box (pixel coordinates)\n            &gt;&gt;&gt; sam.generate_masks_by_boxes_inst([[425, 600, 700, 875]])\n            &gt;&gt;&gt; sam.show_anns()\n            &gt;&gt;&gt; sam.save_masks(\"mask.png\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Multiple boxes with geographic coordinates\n            &gt;&gt;&gt; sam.generate_masks_by_boxes_inst([\n            ...     [-117.5995, 47.6518, -117.5988, 47.652],\n            ...     [-117.5987, 47.6518, -117.5979, 47.652],\n            ... ], box_crs=\"EPSG:4326\", output=\"mask.tif\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using a vector file (GeoJSON, Shapefile, etc.)\n            &gt;&gt;&gt; sam.generate_masks_by_boxes_inst(\n            ...     \"building_bboxes.geojson\",\n            ...     box_crs=\"EPSG:4326\",\n            ...     output=\"building_masks.tif\",\n            ...     dtype=\"uint16\",\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using a GeoDataFrame\n            &gt;&gt;&gt; import geopandas as gpd\n            &gt;&gt;&gt; gdf = gpd.read_file(\"polygons.geojson\")\n            &gt;&gt;&gt; sam.generate_masks_by_boxes_inst(\n            ...     gdf,\n            ...     box_crs=\"EPSG:4326\",\n            ...     output=\"masks.tif\",\n            ... )\n        \"\"\"\n        import geopandas as gpd\n\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"generate_masks_by_boxes_inst is only available for the Meta backend. \"\n                \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n            )\n\n        if self.inference_state is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if (\n            not hasattr(self.model, \"inst_interactive_predictor\")\n            or self.model.inst_interactive_predictor is None\n        ):\n            raise ValueError(\n                \"Instance interactivity not enabled. Please initialize with \"\n                \"enable_inst_interactivity=True.\"\n            )\n\n        # Process boxes based on input type\n        if isinstance(boxes, dict):\n            # GeoJSON-like dict\n            boxes = gpd.GeoDataFrame.from_features(boxes)\n\n        if isinstance(boxes, (str, gpd.GeoDataFrame)):\n            # File path or GeoDataFrame\n            if isinstance(boxes, str):\n                gdf = gpd.read_file(boxes)\n            else:\n                gdf = boxes\n\n            if gdf.crs is None and box_crs is not None:\n                gdf.crs = box_crs\n            elif gdf.crs is not None and box_crs is None:\n                box_crs = str(gdf.crs)\n\n            # Extract bounding boxes from geometries\n            boxes_list = gdf.geometry.apply(lambda geom: list(geom.bounds)).tolist()\n        elif isinstance(boxes, list):\n            boxes_list = boxes\n        elif isinstance(boxes, np.ndarray):\n            boxes_list = boxes.tolist()\n        else:\n            raise ValueError(\n                \"boxes must be a list, GeoDataFrame, file path, or numpy array.\"\n            )\n\n        # Filter boxes that are out of image bounds if box_crs is provided\n        if box_crs is not None and self.source is not None:\n            import rasterio\n            from rasterio.warp import transform_bounds\n\n            with rasterio.open(self.source) as src:\n                img_bounds = transform_bounds(src.crs, box_crs, *src.bounds)\n\n            xmin_img, ymin_img, xmax_img, ymax_img = img_bounds\n\n            valid_boxes = []\n            filtered_count = 0\n            for box in boxes_list:\n                xmin, ymin, xmax, ymax = box\n                # Check if box overlaps with image bounds\n                if (\n                    xmax &gt; xmin_img\n                    and xmin &lt; xmax_img\n                    and ymax &gt; ymin_img\n                    and ymin &lt; ymax_img\n                ):\n                    valid_boxes.append(box)\n                else:\n                    filtered_count += 1\n\n            if filtered_count &gt; 0:\n                print(\n                    f\"Filtered {filtered_count} boxes outside image bounds. \"\n                    f\"Using {len(valid_boxes)} valid boxes.\"\n                )\n\n            if len(valid_boxes) == 0:\n                print(\"No valid boxes found within image bounds.\")\n                self.masks = []\n                self.scores = []\n                self.boxes = []\n                return\n\n            boxes_list = valid_boxes\n\n        # Convert to numpy array\n        boxes_arr = np.array(boxes_list)\n\n        # Call predict_inst with box prompts\n        masks, scores, logits = self.predict_inst(\n            box=boxes_arr,\n            multimask_output=multimask_output,\n            box_crs=box_crs,\n        )\n\n        # Handle batch output shape (BxCxHxW) vs single (CxHxW)\n        if masks.ndim == 4:\n            # Multiple boxes - flatten into list of masks\n            # Each box produces C masks, take the best one per box\n            all_masks = []\n            all_scores = []\n            for i in range(masks.shape[0]):\n                if multimask_output and masks.shape[1] &gt; 1:\n                    best_idx = np.argmax(scores[i])\n                    all_masks.append(masks[i, best_idx])\n                    all_scores.append(scores[i, best_idx])\n                else:\n                    all_masks.append(masks[i, 0] if masks.shape[1] &gt; 0 else masks[i])\n                    all_scores.append(scores[i, 0] if len(scores[i]) &gt; 0 else scores[i])\n            masks = all_masks\n            scores = all_scores\n        else:\n            # Single box\n            if multimask_output and len(masks) &gt; 1:\n                best_idx = np.argmax(scores)\n                masks = [masks[best_idx]]\n                scores = [scores[best_idx]]\n            else:\n                masks = (\n                    [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n                )\n                scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n\n        # Store results\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        # Compute bounding boxes from masks\n        self.boxes = []\n        for mask in self.masks:\n            if hasattr(mask, \"ndim\") and mask.ndim &gt; 2:\n                mask = mask.squeeze()\n            mask_arr = np.asarray(mask)\n            ys, xs = np.where(mask_arr &gt; 0)\n            if len(xs) &gt; 0 and len(ys) &gt; 0:\n                self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n            else:\n                self.boxes.append(np.array([0, 0, 0, 0]))\n\n        # Filter masks by size if min_size or max_size is specified\n        if min_size &gt; 0 or max_size is not None:\n            self._filter_masks_by_size(min_size, max_size)\n\n        num_objects = len(self.masks)\n        if num_objects == 0:\n            print(\"No objects found. Please check your box prompts.\")\n        elif num_objects == 1:\n            print(\"Found one object.\")\n        else:\n            print(f\"Found {num_objects} objects.\")\n\n        # Save masks if output path is provided\n        if output is not None:\n            self.save_masks(\n                output,\n                min_size=min_size,\n                max_size=max_size,\n                dtype=dtype,\n                **kwargs,\n            )\n\n    def _convert_results_to_numpy(self) -&gt; None:\n        \"\"\"Convert masks, boxes, and scores from tensors to numpy arrays.\n\n        This frees GPU memory by moving data to CPU and converting to numpy.\n        \"\"\"\n        if self.masks is None:\n            return\n\n        # Convert masks to numpy\n        converted_masks = []\n        for mask in self.masks:\n            if hasattr(mask, \"cpu\"):\n                # PyTorch tensor on GPU\n                mask_np = mask.cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                # PyTorch tensor on CPU\n                mask_np = mask.numpy()\n            else:\n                # Already numpy or other array-like\n                mask_np = np.asarray(mask)\n            converted_masks.append(mask_np)\n        self.masks = converted_masks\n\n        # Convert boxes to numpy\n        if self.boxes is not None:\n            converted_boxes = []\n            for box in self.boxes:\n                if hasattr(box, \"cpu\"):\n                    box_np = box.cpu().numpy()\n                elif hasattr(box, \"numpy\"):\n                    box_np = box.numpy()\n                else:\n                    box_np = np.asarray(box)\n                converted_boxes.append(box_np)\n            self.boxes = converted_boxes\n\n        # Convert scores to numpy/float\n        if self.scores is not None:\n            converted_scores = []\n            for score in self.scores:\n                if hasattr(score, \"cpu\"):\n                    score_val = (\n                        score.cpu().item()\n                        if score.numel() == 1\n                        else score.cpu().numpy()\n                    )\n                elif hasattr(score, \"item\"):\n                    score_val = score.item()\n                elif hasattr(score, \"numpy\"):\n                    score_val = score.numpy()\n                else:\n                    score_val = float(score)\n                converted_scores.append(score_val)\n            self.scores = converted_scores\n\n    def _filter_masks_by_size(\n        self, min_size: int = 0, max_size: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"Filter masks by size.\n\n        Args:\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out.\n        \"\"\"\n        if self.masks is None or len(self.masks) == 0:\n            return\n\n        filtered_masks = []\n        filtered_boxes = []\n        filtered_scores = []\n\n        for i, mask in enumerate(self.masks):\n            # Convert mask to numpy array if it's a tensor\n            if hasattr(mask, \"cpu\"):\n                mask_np = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask_np = mask.squeeze().numpy()\n            else:\n                mask_np = mask.squeeze() if hasattr(mask, \"squeeze\") else mask\n\n            # Ensure mask is 2D\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np[0]\n\n            # Convert to boolean and calculate mask size\n            mask_bool = mask_np &gt; 0\n            mask_size = np.sum(mask_bool)\n\n            # Filter by size\n            if mask_size &lt; min_size:\n                continue\n            if max_size is not None and mask_size &gt; max_size:\n                continue\n\n            # Keep this mask\n            filtered_masks.append(self.masks[i])\n            if self.boxes is not None and len(self.boxes) &gt; i:\n                filtered_boxes.append(self.boxes[i])\n            if self.scores is not None and len(self.scores) &gt; i:\n                filtered_scores.append(self.scores[i])\n\n        # Update the stored masks, boxes, and scores\n        self.masks = filtered_masks\n        self.boxes = filtered_boxes if filtered_boxes else self.boxes\n        self.scores = filtered_scores if filtered_scores else self.scores\n\n    def show_boxes(\n        self,\n        boxes: List[List[float]],\n        box_labels: Optional[List[bool]] = None,\n        box_crs: Optional[str] = None,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        positive_color: Tuple[int, int, int] = (0, 255, 0),\n        negative_color: Tuple[int, int, int] = (255, 0, 0),\n        thickness: int = 3,\n    ) -&gt; None:\n        \"\"\"\n        Visualize bounding boxes on the image.\n\n        Args:\n            boxes (List[List[float]]): List of bounding boxes in XYXY format\n                [[xmin, ymin, xmax, ymax], ...].\n                If box_crs is None: pixel coordinates.\n                If box_crs is specified: coordinates in the given CRS.\n            box_labels (List[bool], optional): List of boolean labels for each box.\n                True (positive) shown in green, False (negative) shown in red.\n                If None, all boxes shown in green.\n            box_crs (str, optional): Coordinate reference system for box coordinates\n                (e.g., \"EPSG:4326\"). If None, boxes are in pixel coordinates.\n            figsize (Tuple[int, int]): Figure size for display.\n            axis (str): Whether to show axis (\"on\" or \"off\").\n            positive_color (Tuple[int, int, int]): RGB color for positive boxes.\n            negative_color (Tuple[int, int, int]): RGB color for negative boxes.\n            thickness (int): Line thickness for box borders.\n        \"\"\"\n        if self.image is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if box_labels is None:\n            box_labels = [True] * len(boxes)\n\n        # Transform boxes from CRS to pixel coordinates if needed\n        if box_crs is not None and self.source is not None:\n            pixel_boxes = []\n            for box in boxes:\n                xmin, ymin, xmax, ymax = box\n\n                # Transform min corner\n                min_coords = np.array([[xmin, ymin]])\n                min_xy, _ = common.coords_to_xy(\n                    self.source, min_coords, box_crs, return_out_of_bounds=True\n                )\n\n                # Transform max corner\n                max_coords = np.array([[xmax, ymax]])\n                max_xy, _ = common.coords_to_xy(\n                    self.source, max_coords, box_crs, return_out_of_bounds=True\n                )\n\n                # Convert to pixel coordinates and ensure correct min/max order\n                # (geographic y increases north, pixel y increases down)\n                x1_px = min_xy[0][0]\n                y1_px = min_xy[0][1]\n                x2_px = max_xy[0][0]\n                y2_px = max_xy[0][1]\n\n                # Ensure we have correct min/max values\n                x_min_px = min(x1_px, x2_px)\n                y_min_px = min(y1_px, y2_px)\n                x_max_px = max(x1_px, x2_px)\n                y_max_px = max(y1_px, y2_px)\n\n                pixel_boxes.append([x_min_px, y_min_px, x_max_px, y_max_px])\n\n            boxes = pixel_boxes\n\n        # Convert image to PIL if needed\n        if isinstance(self.image, np.ndarray):\n            img = Image.fromarray(self.image)\n        else:\n            img = self.image\n\n        # Draw each box\n        for box, label in zip(boxes, box_labels):\n            # Convert XYXY to XYWH for drawing\n            xmin, ymin, xmax, ymax = box\n            box_xywh = [xmin, ymin, xmax - xmin, ymax - ymin]\n\n            # Choose color based on label\n            color = positive_color if label else negative_color\n\n            # Draw box\n            img = draw_box_on_image(img, box_xywh, color=color, thickness=thickness)\n\n        # Display\n        plt.figure(figsize=figsize)\n        plt.imshow(img)\n        plt.axis(axis)\n        plt.show()\n\n    def show_points(\n        self,\n        point_coords: List[List[float]],\n        point_labels: Optional[List[int]] = None,\n        point_crs: Optional[str] = None,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        foreground_color: str = \"green\",\n        background_color: str = \"red\",\n        marker: str = \"*\",\n        marker_size: int = 375,\n    ) -&gt; None:\n        \"\"\"\n        Visualize point prompts on the image.\n\n        Args:\n            point_coords (List[List[float]]): List of point coordinates [[x, y], ...].\n                If point_crs is None: pixel coordinates.\n                If point_crs is specified: coordinates in the given CRS.\n            point_labels (List[int], optional): List of labels for each point.\n                1 = foreground (shown in green), 0 = background (shown in red).\n                If None, all points shown as foreground.\n            point_crs (str, optional): Coordinate reference system for point coordinates\n                (e.g., \"EPSG:4326\"). If None, points are in pixel coordinates.\n            figsize (Tuple[int, int]): Figure size for display.\n            axis (str): Whether to show axis (\"on\" or \"off\").\n            foreground_color (str): Color for foreground points (label=1).\n            background_color (str): Color for background points (label=0).\n            marker (str): Marker style for points.\n            marker_size (int): Size of the markers.\n\n        Example:\n            sam.show_points([[520, 375]], [1])  # Single foreground point\n            sam.show_points([[500, 375], [600, 400]], [1, 0])  # Mixed points\n        \"\"\"\n        if self.image is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n\n        # Convert to numpy arrays\n        point_coords = np.array(point_coords)\n        point_labels = np.array(point_labels)\n\n        # Transform points from CRS to pixel coordinates if needed\n        if point_crs is not None and self.source is not None:\n            point_coords, _ = common.coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        # Display image\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        # Plot foreground points\n        fg_mask = point_labels == 1\n        if np.any(fg_mask):\n            plt.scatter(\n                point_coords[fg_mask, 0],\n                point_coords[fg_mask, 1],\n                color=foreground_color,\n                marker=marker,\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n\n        # Plot background points\n        bg_mask = point_labels == 0\n        if np.any(bg_mask):\n            plt.scatter(\n                point_coords[bg_mask, 0],\n                point_coords[bg_mask, 1],\n                color=background_color,\n                marker=marker,\n                s=marker_size,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n\n        plt.axis(axis)\n        plt.show()\n\n    def save_masks(\n        self,\n        output: Optional[str] = None,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        dtype: str = \"uint8\",\n        save_scores: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the generated masks to a file or generate mask array for visualization.\n\n        If the input image is a GeoTIFF, the output will be saved as a GeoTIFF\n        with the same georeferencing information. Otherwise, it will be saved as PNG.\n\n        Args:\n            output (str, optional): The path to the output file. If None, only generates\n                the mask array in memory (self.objects) without saving to disk.\n            unique (bool): If True, each mask gets a unique value (1, 2, 3, ...).\n                If False, all masks are combined into a binary mask (0 or 255).\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger than\n                this will be filtered out.\n            dtype (str): Data type for the output array.\n            save_scores (str, optional): If provided, saves a confidence score map\n                to this path. Each pixel will have the confidence score of its mask.\n                The output format (GeoTIFF or PNG) follows the same logic as the mask output.\n            **kwargs: Additional keyword arguments passed to common.array_to_image().\n        \"\"\"\n        if self.masks is None or len(self.masks) == 0:\n            raise ValueError(\"No masks found. Please run generate_masks() first.\")\n\n        if save_scores is not None and self.scores is None:\n            raise ValueError(\"No scores found. Cannot save scores.\")\n\n        # Create empty array for combined masks\n        mask_array = np.zeros(\n            (self.image_height, self.image_width),\n            dtype=np.uint32 if unique else np.uint8,\n        )\n\n        # Create empty array for scores if requested\n        if save_scores is not None:\n            scores_array = np.zeros(\n                (self.image_height, self.image_width), dtype=np.float32\n            )\n\n        # Process each mask\n        valid_mask_count = 0\n        mask_index = 0\n        for mask in self.masks:\n            # Convert mask to numpy array if it's a tensor\n            if hasattr(mask, \"cpu\"):\n                mask_np = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask_np = mask.squeeze().numpy()\n            else:\n                mask_np = mask.squeeze() if hasattr(mask, \"squeeze\") else mask\n\n            # Ensure mask is 2D\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np[0]\n\n            # Convert to boolean\n            mask_bool = mask_np &gt; 0\n\n            # Calculate mask size\n            mask_size = np.sum(mask_bool)\n\n            # Filter by size\n            if mask_size &lt; min_size:\n                mask_index += 1\n                continue\n            if max_size is not None and mask_size &gt; max_size:\n                mask_index += 1\n                continue\n\n            # Get confidence score for this mask\n            if save_scores is not None:\n                if hasattr(self.scores[mask_index], \"item\"):\n                    score = self.scores[mask_index].item()\n                else:\n                    score = float(self.scores[mask_index])\n\n            # Add mask to array\n            if unique:\n                # Assign unique value to each mask (starting from 1)\n                mask_value = valid_mask_count + 1\n                mask_array[mask_bool] = mask_value\n            else:\n                # Binary mask: all foreground pixels are 255\n                mask_array[mask_bool] = 255\n\n            # Add score to scores array\n            if save_scores is not None:\n                scores_array[mask_bool] = score\n\n            valid_mask_count += 1\n            mask_index += 1\n\n        if valid_mask_count == 0:\n            print(\"No masks met the size criteria.\")\n            return\n\n        # Convert to requested dtype\n        if dtype == \"uint8\":\n            if unique and valid_mask_count &gt; 255:\n                print(\n                    f\"Warning: {valid_mask_count} masks found, but uint8 can only represent 255 unique values. Consider using dtype='uint16'.\"\n                )\n            mask_array = mask_array.astype(np.uint8)\n        elif dtype == \"uint16\":\n            mask_array = mask_array.astype(np.uint16)\n        elif dtype == \"int32\":\n            mask_array = mask_array.astype(np.int32)\n        else:\n            mask_array = mask_array.astype(dtype)\n\n        # Store the mask array for visualization\n        self.objects = mask_array\n\n        # Only save to file if output path is provided\n        if output is not None:\n            # Save using common utility which handles GeoTIFF georeferencing\n            common.array_to_image(\n                mask_array, output, self.source, dtype=dtype, **kwargs\n            )\n            print(f\"Saved {valid_mask_count} mask(s) to {output}\")\n\n            # Save scores if requested\n            if save_scores is not None:\n                common.array_to_image(\n                    scores_array, save_scores, self.source, dtype=\"float32\", **kwargs\n                )\n                print(f\"Saved confidence scores to {save_scores}\")\n\n    def save_prediction(\n        self,\n        output: str,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: str = \"float32\",\n        vector: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (Optional[int]): The index of the mask to save.\n            mask_multiplier (int): The mask multiplier for the output mask.\n            dtype (str): The data type of the output image.\n            vector (Optional[str]): The path to the output vector file.\n            simplify_tolerance (Optional[float]): The maximum allowed geometry displacement.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            common.raster_to_vector(\n                output, vector, simplify_tolerance=simplify_tolerance\n            )\n\n    def show_masks(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        cmap: str = \"tab20\",\n        axis: str = \"off\",\n        unique: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple): The figure size.\n            cmap (str): The colormap. Default is \"tab20\" for showing unique objects.\n                Use \"binary_r\" for binary masks when unique=False.\n                Other good options: \"viridis\", \"nipy_spectral\", \"rainbow\".\n            axis (str): Whether to show the axis.\n            unique (bool): If True, each mask gets a unique color value. If False, binary mask.\n            **kwargs: Additional keyword arguments passed to save_masks() for filtering\n                (e.g., min_size, max_size, dtype).\n        \"\"\"\n\n        # Always regenerate mask array to ensure it matches the unique parameter\n        # This prevents showing stale cached binary masks when unique=True is requested\n        self.save_masks(output=None, unique=unique, **kwargs)\n\n        if self.objects is None:\n            # save_masks would have printed a message if no masks met criteria\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap, interpolation=\"nearest\")\n        plt.axis(axis)\n\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        show_bbox: bool = True,\n        show_score: bool = True,\n        output: Optional[str] = None,\n        blend: bool = True,\n        alpha: float = 0.5,\n        font_scale: float = 0.8,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        This method uses OpenCV for fast rendering, which is significantly faster\n        than matplotlib when there are many objects to plot.\n\n        Args:\n            figsize (tuple): The figure size (used for display).\n            axis (str): Whether to show the axis.\n            show_bbox (bool): Whether to show the bounding box.\n            show_score (bool): Whether to show the score.\n            output (str, optional): The path to the output image. If provided, the\n                figure will be saved instead of displayed.\n            blend (bool): Whether to show the input image as background. If False,\n                only annotations will be shown on a white background.\n            alpha (float): The alpha value for the annotations.\n            font_scale (float): The font scale for labels. Defaults to 0.8.\n            **kwargs: Additional keyword arguments (kept for backward compatibility).\n        \"\"\"\n\n        if self.image is None:\n            print(\"Please run set_image() first.\")\n            return\n\n        if self.masks is None or len(self.masks) == 0:\n            return\n\n        # Create the blended image using OpenCV (much faster than matplotlib)\n        blended = self._render_anns_opencv(\n            show_bbox=show_bbox,\n            show_score=show_score,\n            blend=blend,\n            alpha=alpha,\n            font_scale=font_scale,\n        )\n\n        if output is not None:\n            # Save directly using OpenCV\n            cv2.imwrite(output, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n            print(f\"Saved annotations to {output}\")\n        else:\n            # Display the image\n            self._display_image(blended, figsize=figsize, axis=axis)\n\n    def _render_anns_opencv(\n        self,\n        show_bbox: bool = True,\n        show_score: bool = True,\n        blend: bool = True,\n        alpha: float = 0.5,\n        font_scale: float = 0.8,\n    ) -&gt; np.ndarray:\n        \"\"\"Render annotations using OpenCV for fast performance.\n\n        Args:\n            show_bbox (bool): Whether to show the bounding box.\n            show_score (bool): Whether to show the score.\n            blend (bool): Whether to show the input image as background.\n            alpha (float): The alpha value for the annotations.\n            font_scale (float): The font scale for labels.\n\n        Returns:\n            np.ndarray: The rendered image as RGB numpy array.\n        \"\"\"\n        # Get image dimensions\n        h, w = self.image.shape[:2]\n\n        # Create base image\n        if blend:\n            frame_np = self.image.astype(np.float32)\n        else:\n            frame_np = np.ones((h, w, 3), dtype=np.float32) * 255\n\n        nb_objects = len(self.scores)\n\n        # Use the same color generation as the original method for consistency\n        COLORS = generate_colors(n_colors=128, n_samples=5000)\n        # Convert from 0-1 float RGB to 0-255 int RGB for OpenCV\n        colors_rgb = [\n            (int(c[0] * 255), int(c[1] * 255), int(c[2] * 255)) for c in COLORS\n        ]\n\n        # Create overlay for all masks\n        overlay = np.zeros((h, w, 3), dtype=np.float32)\n        mask_combined = np.zeros((h, w), dtype=np.float32)\n        labels_to_draw = []\n\n        for i in range(nb_objects):\n            color = colors_rgb[i % len(colors_rgb)]\n\n            # Handle both tensor and numpy array formats\n            mask = self.masks[i]\n            if hasattr(mask, \"cpu\"):\n                mask_np = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask_np = mask.squeeze().numpy()\n            else:\n                mask_np = np.squeeze(mask)\n\n            # Ensure mask is 2D\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np[0]\n\n            # Resize mask if it doesn't match frame dimensions\n            if mask_np.shape != (h, w):\n                mask_np = cv2.resize(\n                    mask_np.astype(np.float32),\n                    (w, h),\n                    interpolation=cv2.INTER_NEAREST,\n                )\n\n            # Add color to overlay where mask is present\n            mask_bool = mask_np &gt; 0\n            for c in range(3):\n                overlay[:, :, c] = np.where(mask_bool, color[c], overlay[:, :, c])\n            mask_combined = np.maximum(mask_combined, mask_np.astype(np.float32))\n\n            # Collect label info for bounding boxes\n            if show_bbox and self.boxes is not None:\n                # Handle score extraction\n                score = self.scores[i]\n                if hasattr(score, \"item\"):\n                    prob = score.item()\n                else:\n                    prob = float(score)\n\n                if show_score:\n                    text = f\"(id={i}, {prob=:.2f})\"\n                else:\n                    text = f\"(id={i})\"\n\n                # Handle box extraction\n                box = self.boxes[i]\n                if hasattr(box, \"cpu\"):\n                    box = box.cpu().numpy()\n                elif hasattr(box, \"numpy\"):\n                    box = box.numpy()\n                else:\n                    box = np.array(box)\n\n                labels_to_draw.append((box, text, color))\n\n        # Blend overlay with frame\n        mask_3d = mask_combined[:, :, np.newaxis]\n        blended = frame_np * (1 - mask_3d * alpha) + overlay * (mask_3d * alpha)\n        blended = np.clip(blended, 0, 255).astype(np.uint8)\n\n        # Draw bounding boxes and labels using OpenCV\n        for box, text, color in labels_to_draw:\n            x1, y1, x2, y2 = map(lambda v: int(round(v)), box[:4])\n            # Clip bounding box coordinates to image boundaries\n            x1, y1 = max(0, x1), max(0, y1)\n            x2, y2 = min(w, x2), min(h, y2)\n\n            # Draw bounding box\n            cv2.rectangle(blended, (x1, y1), (x2, y2), color, 2)\n\n            # Get text size for background rectangle\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            thickness = max(1, int(font_scale * 2.5))\n            (text_w, text_h), baseline = cv2.getTextSize(\n                text, font, font_scale, thickness\n            )\n\n            # Draw background rectangle for text\n            pad = 2\n            text_x = x1\n            text_y = y1 - 5\n            if text_y - text_h - pad &lt; 0:\n                text_y = y2 + text_h + 5\n\n            # Semi-transparent background for text\n            bg_x1 = text_x - pad\n            bg_y1 = text_y - text_h - pad\n            bg_x2 = text_x + text_w + pad\n            bg_y2 = text_y + pad + baseline\n\n            # Clip to image boundaries\n            bg_x1, bg_y1 = max(0, bg_x1), max(0, bg_y1)\n            bg_x2, bg_y2 = min(w, bg_x2), min(h, bg_y2)\n\n            if bg_x2 &gt; bg_x1 and bg_y2 &gt; bg_y1:\n                sub_img = blended[bg_y1:bg_y2, bg_x1:bg_x2].astype(np.float32)\n                bg_color = np.array(color, dtype=np.float32)\n                blend_rect = (sub_img * 0.3 + bg_color * 0.7).astype(np.uint8)\n                blended[bg_y1:bg_y2, bg_x1:bg_x2] = blend_rect\n\n            # Draw text\n            cv2.putText(\n                blended,\n                text,\n                (text_x, text_y),\n                font,\n                font_scale,\n                (255, 255, 255),\n                thickness,\n                cv2.LINE_AA,\n            )\n\n        return blended\n\n    def _display_image(\n        self,\n        image: np.ndarray,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n    ) -&gt; None:\n        \"\"\"Display an image, using IPython display if available for better performance.\n\n        Args:\n            image (np.ndarray): The image to display (RGB format).\n            figsize (tuple): The figure size.\n            axis (str): Whether to show the axis.\n        \"\"\"\n        try:\n            # Try to use IPython display for better notebook performance\n            from IPython.display import display\n\n            # Save to temporary file and display\n            temp_dir = common.make_temp_dir()\n            temp_path = os.path.join(temp_dir, \"temp_anns.png\")\n            cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n            # Display using PIL Image (works well in notebooks)\n            img_display = Image.open(temp_path)\n\n            # Resize for display based on figsize (assuming 100 DPI)\n            display_width = figsize[0] * 100\n            aspect_ratio = image.shape[0] / image.shape[1]\n            display_height = int(display_width * aspect_ratio)\n            img_display = img_display.resize(\n                (display_width, display_height), Image.Resampling.LANCZOS\n            )\n\n            display(img_display)\n\n        except ImportError:\n            # Fall back to matplotlib for non-notebook environments\n            plt.figure(figsize=figsize)\n            plt.imshow(image)\n            plt.axis(axis)\n            plt.show()\n\n    def raster_to_vector(\n        self,\n        raster: str,\n        vector: str,\n        simplify_tolerance: Optional[float] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Convert a raster image file to a vector dataset.\n\n        Args:\n            raster (str): The path to the raster image.\n            vector (str): The path to the output vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n        \"\"\"\n        common.raster_to_vector(\n            raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def show_map(\n        self,\n        basemap=\"Esri.WorldImagery\",\n        out_dir=None,\n        min_size=10,\n        max_size=None,\n        prompt=\"text\",\n        **kwargs,\n    ):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. Valid options include \"Esri.WorldImagery\", \"OpenStreetMap\", \"HYBRID\", \"ROADMAP\", \"TERRAIN\", etc. See the leafmap documentation for a full list of supported basemaps.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n            min_size (int, optional): The minimum size of the object. Defaults to 10.\n            max_size (int, optional): The maximum size of the object. Defaults to None.\n            prompt (str, optional): The prompt type. Defaults to \"text\".\n                Valid options include \"text\" and \"point\".\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        if prompt.lower() == \"text\":\n            return common.text_sam_gui(\n                self,\n                basemap=basemap,\n                out_dir=out_dir,\n                box_threshold=self.confidence_threshold,\n                text_threshold=self.mask_threshold,\n                min_size=min_size,\n                max_size=max_size,\n                **kwargs,\n            )\n        elif prompt.lower() == \"point\":\n            return common.sam_map_gui(\n                self,\n                basemap=basemap,\n                out_dir=out_dir,\n                min_size=min_size,\n                max_size=max_size,\n                **kwargs,\n            )\n        else:\n            raise ValueError(f\"Invalid prompt: {prompt}. Please use 'text' or 'point'.\")\n\n    def show_canvas(\n        self,\n        fg_color: Tuple[int, int, int] = (0, 255, 0),\n        bg_color: Tuple[int, int, int] = (0, 0, 255),\n        radius: int = 5,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            fg_color (Tuple[int, int, int]): The color for foreground points.\n            bg_color (Tuple[int, int, int]): The color for background points.\n            radius (int): The radius of the points.\n\n        Returns:\n            Tuple of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n        return fg_points, bg_points\n\n    def predict_inst(\n        self,\n        point_coords: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        point_labels: Optional[Union[np.ndarray, List[int]]] = None,\n        box: Optional[Union[np.ndarray, List[float], List[List[float]]]] = None,\n        mask_input: Optional[np.ndarray] = None,\n        multimask_output: bool = True,\n        return_logits: bool = False,\n        normalize_coords: bool = True,\n        point_crs: Optional[str] = None,\n        box_crs: Optional[str] = None,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Predict masks for the given input prompts using SAM3's interactive instance\n        segmentation (SAM1-style task). This enables point and box prompts for\n        precise object segmentation.\n\n        Note: This method requires the model to be initialized with\n        `enable_inst_interactivity=True` (Meta backend only).\n\n        Args:\n            point_coords (np.ndarray or List, optional): A Nx2 array or list of point\n                prompts to the model. Each point is in (X, Y) pixel coordinates.\n                Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].\n            point_labels (np.ndarray or List, optional): A length N array or list of\n                labels for the point prompts. 1 indicates a foreground point and\n                0 indicates a background point.\n            box (np.ndarray or List, optional): A length 4 array/list or Bx4 array/list\n                of box prompt(s) to the model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the\n                model, typically coming from a previous prediction iteration.\n                Has form 1xHxW (or BxHxW for batched), where H=W=256 for SAM.\n            multimask_output (bool): If True, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will\n                often produce better masks than a single prediction. If only a\n                single mask is needed, the model's predicted quality score can\n                be used to select the best mask. For non-ambiguous prompts, such\n                as multiple input prompts, multimask_output=False can give\n                better results. Defaults to True.\n            return_logits (bool): If True, returns un-thresholded mask logits\n                instead of binary masks. Defaults to False.\n            normalize_coords (bool): If True, the point coordinates will be\n                normalized to the range [0, 1] and point_coords is expected to\n                be w.r.t. image dimensions. Defaults to True.\n            point_crs (str, optional): Coordinate reference system for point\n                coordinates (e.g., \"EPSG:4326\"). Only used if the source image\n                is a GeoTIFF. If None, points are in pixel coordinates.\n            box_crs (str, optional): Coordinate reference system for box\n                coordinates (e.g., \"EPSG:4326\"). Only used if the source image\n                is a GeoTIFF. If None, box is in pixel coordinates.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]:\n                - masks: The output masks in CxHxW format (or BxCxHxW for batched\n                  box input), where C is the number of masks, and (H, W) is the\n                  original image size.\n                - scores: An array of length C (or BxC) containing the model's\n                  predictions for the quality of each mask.\n                - logits: An array of shape CxHxW (or BxCxHxW), where C is the\n                  number of masks and H=W=256. These low resolution logits can\n                  be passed to a subsequent iteration as mask input.\n\n        Example:\n            &gt;&gt;&gt; # Initialize with instance interactivity enabled\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Single point prompt\n            &gt;&gt;&gt; point_coords = np.array([[520, 375]])\n            &gt;&gt;&gt; point_labels = np.array([1])\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n            ...     point_coords=point_coords,\n            ...     point_labels=point_labels,\n            ...     multimask_output=True,\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Select best mask based on score\n            &gt;&gt;&gt; best_mask_idx = np.argmax(scores)\n            &gt;&gt;&gt; best_mask = masks[best_mask_idx]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Refine with additional points\n            &gt;&gt;&gt; point_coords = np.array([[500, 375], [1125, 625]])\n            &gt;&gt;&gt; point_labels = np.array([1, 0])  # foreground and background\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n            ...     point_coords=point_coords,\n            ...     point_labels=point_labels,\n            ...     mask_input=logits[best_mask_idx:best_mask_idx+1],  # Use previous best\n            ...     multimask_output=False,\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Box prompt\n            &gt;&gt;&gt; box = np.array([425, 600, 700, 875])\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(box=box, multimask_output=False)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Combined box and point prompt\n            &gt;&gt;&gt; box = np.array([425, 600, 700, 875])\n            &gt;&gt;&gt; point_coords = np.array([[575, 750]])\n            &gt;&gt;&gt; point_labels = np.array([0])  # Exclude this region\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n            ...     point_coords=point_coords,\n            ...     point_labels=point_labels,\n            ...     box=box,\n            ...     multimask_output=False,\n            ... )\n        \"\"\"\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"predict_inst is only available for the Meta backend. \"\n                \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n            )\n\n        if self.inference_state is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if (\n            not hasattr(self.model, \"inst_interactive_predictor\")\n            or self.model.inst_interactive_predictor is None\n        ):\n            raise ValueError(\n                \"Instance interactivity not enabled. Please initialize with \"\n                \"enable_inst_interactivity=True.\"\n            )\n\n        # Convert lists to numpy arrays\n        if point_coords is not None and not isinstance(point_coords, np.ndarray):\n            point_coords = np.array(point_coords)\n        if point_labels is not None and not isinstance(point_labels, np.ndarray):\n            point_labels = np.array(point_labels)\n        if box is not None and not isinstance(box, np.ndarray):\n            box = np.array(box)\n\n        # Transform point coordinates from CRS to pixel coordinates if needed\n        if (\n            point_coords is not None\n            and point_crs is not None\n            and self.source is not None\n        ):\n            point_coords = np.array(point_coords)\n            point_coords, _ = common.coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        # Transform box coordinates from CRS to pixel coordinates if needed\n        if box is not None and box_crs is not None and self.source is not None:\n            box = np.array(box)\n            if box.ndim == 1:\n                # Single box [xmin, ymin, xmax, ymax]\n                xmin, ymin, xmax, ymax = box\n                min_coords = np.array([[xmin, ymin]])\n                max_coords = np.array([[xmax, ymax]])\n                min_xy, _ = common.coords_to_xy(\n                    self.source, min_coords, box_crs, return_out_of_bounds=True\n                )\n                max_xy, _ = common.coords_to_xy(\n                    self.source, max_coords, box_crs, return_out_of_bounds=True\n                )\n                x1_px, y1_px = min_xy[0]\n                x2_px, y2_px = max_xy[0]\n                box = np.array(\n                    [\n                        min(x1_px, x2_px),\n                        min(y1_px, y2_px),\n                        max(x1_px, x2_px),\n                        max(y1_px, y2_px),\n                    ]\n                )\n            else:\n                # Multiple boxes [B, 4]\n                transformed_boxes = []\n                for b in box:\n                    xmin, ymin, xmax, ymax = b\n                    min_coords = np.array([[xmin, ymin]])\n                    max_coords = np.array([[xmax, ymax]])\n                    min_xy, _ = common.coords_to_xy(\n                        self.source, min_coords, box_crs, return_out_of_bounds=True\n                    )\n                    max_xy, _ = common.coords_to_xy(\n                        self.source, max_coords, box_crs, return_out_of_bounds=True\n                    )\n                    x1_px, y1_px = min_xy[0]\n                    x2_px, y2_px = max_xy[0]\n                    transformed_boxes.append(\n                        [\n                            min(x1_px, x2_px),\n                            min(y1_px, y2_px),\n                            max(x1_px, x2_px),\n                            max(y1_px, y2_px),\n                        ]\n                    )\n                box = np.array(transformed_boxes)\n\n        # Call the model's predict_inst method\n        masks, scores, logits = self.model.predict_inst(\n            self.inference_state,\n            point_coords=point_coords,\n            point_labels=point_labels,\n            box=box,\n            mask_input=mask_input,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        # Store results\n        self.masks = (\n            [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n        )\n        self.scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n        self.logits = logits\n\n        return masks, scores, logits\n\n    def predict_inst_batch(\n        self,\n        point_coords_batch: Optional[List[np.ndarray]] = None,\n        point_labels_batch: Optional[List[np.ndarray]] = None,\n        box_batch: Optional[List[np.ndarray]] = None,\n        mask_input_batch: Optional[List[np.ndarray]] = None,\n        multimask_output: bool = True,\n        return_logits: bool = False,\n        normalize_coords: bool = True,\n    ) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n        \"\"\"\n        Predict masks for batched input prompts using SAM3's interactive instance\n        segmentation. This is used when the model is set with multiple images via\n        `set_image_batch()`.\n\n        Note: This method requires the model to be initialized with\n        `enable_inst_interactivity=True` (Meta backend only).\n\n        Args:\n            point_coords_batch (List[np.ndarray], optional): List of Nx2 arrays of\n                point prompts for each image in the batch. Each point is in (X, Y)\n                pixel coordinates.\n            point_labels_batch (List[np.ndarray], optional): List of length N arrays\n                of labels for the point prompts for each image. 1 indicates a\n                foreground point and 0 indicates a background point.\n            box_batch (List[np.ndarray], optional): List of Bx4 arrays of box\n                prompts for each image in the batch, in XYXY format.\n            mask_input_batch (List[np.ndarray], optional): List of low resolution\n                mask inputs for each image, typically from previous iterations.\n            multimask_output (bool): If True, the model will return three masks\n                per prompt. Defaults to True.\n            return_logits (bool): If True, returns un-thresholded mask logits\n                instead of binary masks. Defaults to False.\n            normalize_coords (bool): If True, coordinates are normalized to [0, 1].\n                Defaults to True.\n\n        Returns:\n            Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n                - masks_batch: List of mask arrays for each image\n                - scores_batch: List of score arrays for each image\n                - logits_batch: List of logit arrays for each image\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load batch of images\n            &gt;&gt;&gt; image1 = Image.open(\"truck.jpg\")\n            &gt;&gt;&gt; image2 = Image.open(\"groceries.jpg\")\n            &gt;&gt;&gt; sam.set_image_batch([image1, image2])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Define box prompts for each image\n            &gt;&gt;&gt; image1_boxes = np.array([\n            ...     [75, 275, 1725, 850],\n            ...     [425, 600, 700, 875],\n            ... ])\n            &gt;&gt;&gt; image2_boxes = np.array([\n            ...     [450, 170, 520, 350],\n            ...     [350, 190, 450, 350],\n            ... ])\n            &gt;&gt;&gt; boxes_batch = [image1_boxes, image2_boxes]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Predict masks for all images\n            &gt;&gt;&gt; masks_batch, scores_batch, logits_batch = sam.predict_inst_batch(\n            ...     box_batch=boxes_batch,\n            ...     multimask_output=False,\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Or use point prompts\n            &gt;&gt;&gt; image1_pts = np.array([[[500, 375]], [[650, 750]]])  # Bx1x2\n            &gt;&gt;&gt; image1_labels = np.array([[1], [1]])\n            &gt;&gt;&gt; image2_pts = np.array([[[400, 300]], [[630, 300]]])\n            &gt;&gt;&gt; image2_labels = np.array([[1], [1]])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; masks_batch, scores_batch, logits_batch = sam.predict_inst_batch(\n            ...     point_coords_batch=[image1_pts, image2_pts],\n            ...     point_labels_batch=[image1_labels, image2_labels],\n            ...     multimask_output=True,\n            ... )\n        \"\"\"\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"predict_inst_batch is only available for the Meta backend. \"\n                \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n            )\n\n        if self.batch_state is None:\n            raise ValueError(\n                \"No images set for batch processing. \"\n                \"Please call set_image_batch() first.\"\n            )\n\n        if (\n            not hasattr(self.model, \"inst_interactive_predictor\")\n            or self.model.inst_interactive_predictor is None\n        ):\n            raise ValueError(\n                \"Instance interactivity not enabled. Please initialize with \"\n                \"enable_inst_interactivity=True.\"\n            )\n\n        # Call the model's predict_inst_batch method\n        masks_batch, scores_batch, logits_batch = self.model.predict_inst_batch(\n            self.batch_state,\n            point_coords_batch,\n            point_labels_batch,\n            box_batch=box_batch,\n            mask_input_batch=mask_input_batch,\n            multimask_output=multimask_output,\n            return_logits=return_logits,\n            normalize_coords=normalize_coords,\n        )\n\n        return masks_batch, scores_batch, logits_batch\n\n    def generate_masks_by_points_patch(\n        self,\n        point_coords_batch: Union[List[List[float]], str, Any] = None,\n        point_labels_batch: Optional[List[int]] = None,\n        point_crs: Optional[str] = None,\n        output: Optional[str] = None,\n        index: Optional[int] = None,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        dtype: str = \"int32\",\n        multimask_output: bool = False,\n        return_results: bool = False,\n        **kwargs: Any,\n    ) -&gt; Optional[Tuple[List[Dict], List[np.ndarray], List[np.ndarray]]]:\n        \"\"\"\n        Generate masks for multiple point prompts using batch processing.\n\n        This method is similar to SamGeo2.predict_by_points but uses SAM3's\n        predict_inst_batch for efficient batch processing of point prompts on\n        a single image. Each point is treated as a separate prompt to generate\n        a separate mask.\n\n        Note: This method requires the model to be initialized with\n        `enable_inst_interactivity=True` (Meta backend only).\n\n        Args:\n            point_coords_batch (List[List[float]] | str | GeoDataFrame): Point\n                coordinates for batch processing. Can be:\n                - A list of [x, y] coordinates\n                - A file path to a vector file (GeoJSON, Shapefile, etc.)\n                - A GeoDataFrame with point geometries\n            point_labels_batch (List[int], optional): Labels for each point.\n                1 = foreground (include), 0 = background (exclude).\n                If None, all points are treated as foreground (label=1).\n            point_crs (str, optional): Coordinate reference system for point\n                coordinates (e.g., \"EPSG:4326\"). If provided, coordinates will\n                be converted from the CRS to pixel coordinates. Required when\n                using geographic coordinates with a GeoTIFF source image.\n            output (str, optional): Path to save the output mask as a GeoTIFF.\n                If None, masks are stored in memory only.\n            index (int, optional): If multimask_output is True, select this\n                specific mask index from each prediction. If None, selects the\n                mask with the highest score for each point.\n            unique (bool): If True, each mask gets a unique integer value\n                (1, 2, 3, ...). If False, all masks are combined into a binary\n                mask. Defaults to True.\n            min_size (int): Minimum mask size in pixels. Masks smaller than this\n                will be filtered out. Defaults to 0.\n            max_size (int, optional): Maximum mask size in pixels. Masks larger\n                than this will be filtered out. Defaults to None (no maximum).\n            dtype (str): Data type for the output mask array. Defaults to \"int32\".\n            multimask_output (bool): If True, the model returns 3 masks per prompt.\n                Defaults to False for cleaner batch results.\n            return_results (bool): If True, returns the masks, scores, and logits.\n                Defaults to False.\n            **kwargs: Additional keyword arguments passed to save_masks().\n\n        Returns:\n            If return_results is True:\n                Tuple[List[Dict], List[np.ndarray], List[np.ndarray]]: Tuple of\n                    (output_masks, scores, logits) where output_masks is a list\n                    of dictionaries with 'segmentation' and 'area' keys.\n            If return_results is False:\n                None\n\n        Example:\n            &gt;&gt;&gt; # Initialize with instance interactivity enabled\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt; sam.set_image(\"satellite.tif\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using a list of geographic coordinates\n            &gt;&gt;&gt; point_coords = [\n            ...     [-117.599896, 47.655345],\n            ...     [-117.59992, 47.655167],\n            ...     [-117.599928, 47.654974],\n            ...     [-117.599518, 47.655337],\n            ... ]\n            &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n            ...     point_coords_batch=point_coords,\n            ...     point_crs=\"EPSG:4326\",\n            ...     output=\"masks.tif\",\n            ...     dtype=\"uint8\",\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using a vector file (GeoJSON, Shapefile, etc.)\n            &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n            ...     point_coords_batch=\"building_centroids.geojson\",\n            ...     point_crs=\"EPSG:4326\",\n            ...     output=\"building_masks.tif\",\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using a GeoDataFrame\n            &gt;&gt;&gt; import geopandas as gpd\n            &gt;&gt;&gt; gdf = gpd.read_file(\"points.geojson\")\n            &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n            ...     point_coords_batch=gdf,\n            ...     point_crs=\"EPSG:4326\",\n            ...     output=\"masks.tif\",\n            ... )\n        \"\"\"\n        import geopandas as gpd\n\n        if self.backend != \"meta\":\n            raise NotImplementedError(\n                \"generate_masks_by_points_patch is only available for the Meta backend. \"\n                \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n            )\n\n        if self.source is None:\n            raise ValueError(\n                \"No source image set. This method requires a file path to be provided \"\n                \"to set_image() for georeferencing support.\"\n            )\n\n        if self.inference_state is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        if (\n            not hasattr(self.model, \"inst_interactive_predictor\")\n            or self.model.inst_interactive_predictor is None\n        ):\n            raise ValueError(\n                \"Instance interactivity not enabled. Please initialize with \"\n                \"enable_inst_interactivity=True.\"\n            )\n\n        # Process point coordinates based on input type\n        if isinstance(point_coords_batch, dict):\n            # GeoJSON-like dict\n            point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n        if isinstance(point_coords_batch, str) or isinstance(\n            point_coords_batch, gpd.GeoDataFrame\n        ):\n            # File path or GeoDataFrame\n            if isinstance(point_coords_batch, str):\n                gdf = gpd.read_file(point_coords_batch)\n            else:\n                gdf = point_coords_batch\n\n            if gdf.crs is None and point_crs is not None:\n                gdf.crs = point_crs\n\n            # Extract point coordinates from geometry\n            points_list = gdf.geometry.apply(lambda geom: [geom.x, geom.y]).tolist()\n            coordinates_array = np.array([[point] for point in points_list])\n\n            # Convert coordinates to pixel coordinates\n            points, out_of_bounds = common.coords_to_xy(\n                self.source, coordinates_array, gdf.crs, return_out_of_bounds=True\n            )\n            num_points = points.shape[0]\n\n            # Filter labels to match filtered coordinates\n            if point_labels_batch is not None and len(out_of_bounds) &gt; 0:\n                valid_indices = [\n                    i for i in range(len(points_list)) if i not in out_of_bounds\n                ]\n                point_labels_batch = [point_labels_batch[i] for i in valid_indices]\n\n            if point_labels_batch is None:\n                labels = np.array([[1] for _ in range(num_points)])\n            else:\n                labels = np.array([[label] for label in point_labels_batch])\n\n        elif isinstance(point_coords_batch, list):\n            if point_crs is not None:\n                # Convert from CRS to pixel coordinates\n                point_coords_array = np.array(point_coords_batch)\n                point_coords_xy, out_of_bounds = common.coords_to_xy(\n                    self.source,\n                    point_coords_array,\n                    point_crs,\n                    return_out_of_bounds=True,\n                )\n                # Filter labels to match filtered coordinates\n                if point_labels_batch is not None:\n                    # Create a mask for valid (in-bounds) indices\n                    valid_indices = [\n                        i\n                        for i in range(len(point_coords_batch))\n                        if i not in out_of_bounds\n                    ]\n                    point_labels_batch = [point_labels_batch[i] for i in valid_indices]\n            else:\n                point_coords_xy = np.array(point_coords_batch)\n\n            # Format points for batch processing: each point as separate prompt\n            points = np.array([[point] for point in point_coords_xy])\n            num_points = len(points)\n\n            if point_labels_batch is None:\n                labels = np.array([[1] for _ in range(num_points)])\n            elif isinstance(point_labels_batch, list):\n                labels = np.array([[label] for label in point_labels_batch])\n            else:\n                labels = point_labels_batch\n\n        elif isinstance(point_coords_batch, np.ndarray):\n            points = point_coords_batch\n            labels = point_labels_batch\n            if labels is None:\n                num_points = points.shape[0]\n                labels = np.array([[1] for _ in range(num_points)])\n        else:\n            raise ValueError(\n                \"point_coords_batch must be a list, GeoDataFrame, file path, or numpy array.\"\n            )\n\n        # Set up batch state for single image batch processing\n        # Use set_image_batch with the current image\n        if self.images_batch is None:\n            pil_image = Image.fromarray(self.image)\n            self.batch_state = self.processor.set_image_batch([pil_image], state=None)\n            self.images_batch = [self.image]\n            self.sources_batch = [self.source]\n\n        # Call predict_inst_batch with the formatted points\n        # predict_inst_batch expects batches per image, we have one image with multiple points\n        masks_batch, scores_batch, logits_batch = self.model.predict_inst_batch(\n            self.batch_state,\n            [points],  # One entry for our single image\n            [labels],  # One entry for our single image\n            box_batch=None,\n            mask_input_batch=None,\n            multimask_output=multimask_output,\n            return_logits=False,\n            normalize_coords=True,\n        )\n\n        # Extract results for our single image\n        masks = masks_batch[0]\n        scores = scores_batch[0]\n        logits = logits_batch[0]\n\n        # Handle multimask output - select best mask per prompt if index not specified\n        if multimask_output and index is not None:\n            masks = masks[:, index, :, :]\n        elif multimask_output and masks.ndim == 4:\n            # Select best mask for each prompt based on scores\n            best_masks = []\n            best_scores = []\n            for i in range(masks.shape[0]):\n                best_idx = np.argmax(scores[i])\n                best_masks.append(masks[i, best_idx])\n                best_scores.append(scores[i, best_idx])\n            masks = np.array(best_masks)\n            scores = np.array(best_scores)\n\n        if masks.ndim &gt; 3:\n            masks = masks.squeeze()\n\n        # Ensure masks is 3D (num_masks, H, W)\n        if masks.ndim == 2:\n            masks = masks[np.newaxis, ...]  # Add batch dimension\n\n        # Store results in the format expected by show_anns/show_masks/save_masks\n        self.masks = [masks[i] for i in range(len(masks))]\n        self.scores = scores if isinstance(scores, list) else list(scores.flatten())\n        self.logits = logits\n\n        # Create output mask list with segmentation dict format for return value\n        output_masks = []\n        sums = np.sum(masks, axis=(1, 2))\n        for idx, mask in enumerate(masks):\n            item = {\n                \"segmentation\": mask.astype(\"bool\"),\n                \"area\": sums[idx],\n            }\n            output_masks.append(item)\n\n        # Compute bounding boxes from masks\n        self.boxes = []\n        for mask in self.masks:\n            if mask.ndim &gt; 2:\n                mask = mask.squeeze()\n            ys, xs = np.where(mask &gt; 0)\n            if len(xs) &gt; 0 and len(ys) &gt; 0:\n                self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n            else:\n                self.boxes.append(np.array([0, 0, 0, 0]))\n\n        # Save masks if output path is provided\n        if output is not None:\n            self.save_masks(\n                output,\n                unique=unique,\n                min_size=min_size,\n                max_size=max_size,\n                dtype=dtype,\n                **kwargs,\n            )\n\n        num_objects = len(self.masks)\n        if num_objects == 0:\n            print(\"No objects found. Please check your point prompts.\")\n        elif num_objects == 1:\n            print(\"Found one object.\")\n        else:\n            print(f\"Found {num_objects} objects.\")\n\n        if return_results:\n            return output_masks, scores, logits\n\n    def show_inst_masks(\n        self,\n        masks: np.ndarray,\n        scores: np.ndarray,\n        point_coords: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        point_labels: Optional[Union[np.ndarray, List[int]]] = None,\n        box_coords: Optional[Union[np.ndarray, List[float]]] = None,\n        figsize: Tuple[int, int] = (10, 10),\n        borders: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Display masks from predict_inst results with optional point and box overlays.\n\n        Args:\n            masks (np.ndarray): Masks from predict_inst, shape CxHxW.\n            scores (np.ndarray): Scores from predict_inst, shape C.\n            point_coords (np.ndarray or List, optional): Point coordinates used for prompts.\n                Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].\n            point_labels (np.ndarray or List, optional): Point labels (1=foreground, 0=background).\n            box_coords (np.ndarray or List, optional): Box coordinates used for prompt.\n                Can be a numpy array or a Python list like [x1, y1, x2, y2].\n            figsize (Tuple[int, int]): Figure size for each mask display.\n            borders (bool): Whether to draw contour borders on masks.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n            &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n            &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n            ...     point_coords=[[520, 375]],\n            ...     point_labels=[1],\n            ... )\n            &gt;&gt;&gt; sam.show_inst_masks(\n            ...     masks, scores,\n            ...     point_coords=[[520, 375]],\n            ...     point_labels=[1],\n            ... )\n        \"\"\"\n        if self.image is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        # Convert lists to numpy arrays\n        if point_coords is not None and not isinstance(point_coords, np.ndarray):\n            point_coords = np.array(point_coords)\n        if point_labels is not None and not isinstance(point_labels, np.ndarray):\n            point_labels = np.array(point_labels)\n        if box_coords is not None and not isinstance(box_coords, np.ndarray):\n            box_coords = np.array(box_coords)\n\n        # Sort by score (descending)\n        sorted_ind = np.argsort(scores)[::-1]\n        masks = masks[sorted_ind]\n        scores = scores[sorted_ind]\n\n        for i, (mask, score) in enumerate(zip(masks, scores)):\n            fig = plt.figure(figsize=figsize)\n            plt.imshow(self.image)\n\n            # Show mask\n            h, w = mask.shape[-2:]\n            mask_uint8 = mask.astype(np.uint8)\n            color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n            mask_image = mask_uint8.reshape(h, w, 1) * color.reshape(1, 1, -1)\n\n            if borders:\n                contours, _ = cv2.findContours(\n                    mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n                )\n                contours = [\n                    cv2.approxPolyDP(contour, epsilon=0.01, closed=True)\n                    for contour in contours\n                ]\n                mask_image = cv2.drawContours(\n                    mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2\n                )\n\n            plt.gca().imshow(mask_image)\n\n            # Show points if provided\n            if point_coords is not None and point_labels is not None:\n                pos_points = point_coords[point_labels == 1]\n                neg_points = point_coords[point_labels == 0]\n                plt.scatter(\n                    pos_points[:, 0],\n                    pos_points[:, 1],\n                    color=\"green\",\n                    marker=\"*\",\n                    s=375,\n                    edgecolor=\"white\",\n                    linewidth=1.25,\n                )\n                plt.scatter(\n                    neg_points[:, 0],\n                    neg_points[:, 1],\n                    color=\"red\",\n                    marker=\"*\",\n                    s=375,\n                    edgecolor=\"white\",\n                    linewidth=1.25,\n                )\n\n            # Show box if provided\n            if box_coords is not None:\n                x0, y0 = box_coords[0], box_coords[1]\n                box_w, box_h = (\n                    box_coords[2] - box_coords[0],\n                    box_coords[3] - box_coords[1],\n                )\n                plt.gca().add_patch(\n                    plt.Rectangle(\n                        (x0, y0),\n                        box_w,\n                        box_h,\n                        edgecolor=\"green\",\n                        facecolor=(0, 0, 0, 0),\n                        lw=2,\n                    )\n                )\n\n            if len(scores) &gt; 1:\n                plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n\n            plt.axis(\"off\")\n            plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.__init__","title":"<code>__init__(backend='meta', model_id='facebook/sam3', bpe_path=None, device=None, eval_mode=True, checkpoint_path=None, load_from_HF=True, enable_segmentation=True, enable_inst_interactivity=False, compile_mode=False, resolution=1008, confidence_threshold=0.5, mask_threshold=0.5, **kwargs)</code>","text":"<p>Initializes the SamGeo3 class.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend to use ('meta' or 'transformers'). Default is 'meta'.</p> <code>'meta'</code> <code>model_id</code> <code>str</code> <p>Model ID for Transformers backend (e.g., 'facebook/sam3'). Only used when backend='transformers'.</p> <code>'facebook/sam3'</code> <code>bpe_path</code> <code>str</code> <p>Path to the BPE tokenizer vocabulary (Meta backend only).</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to load the model on ('cuda' or 'cpu').</p> <code>None</code> <code>eval_mode</code> <code>bool</code> <p>Whether to set the model to evaluation mode (Meta backend only).</p> <code>True</code> <code>checkpoint_path</code> <code>str</code> <p>Optional path to model checkpoint (Meta backend only).</p> <code>None</code> <code>load_from_HF</code> <code>bool</code> <p>Whether to load the model from HuggingFace (Meta backend only).</p> <code>True</code> <code>enable_segmentation</code> <code>bool</code> <p>Whether to enable segmentation head (Meta backend only).</p> <code>True</code> <code>enable_inst_interactivity</code> <code>bool</code> <p>Whether to enable instance interactivity (SAM 1 task) (Meta backend only). Set to True to use predict_inst() and predict_inst_batch() methods for interactive point and box prompts. When True, the model loads additional components for SAM1-style interactive instance segmentation. Defaults to False.</p> <code>False</code> <code>compile_mode</code> <code>bool</code> <p>To enable compilation, set to \"default\" (Meta backend only).</p> <code>False</code> <code>resolution</code> <code>int</code> <p>Resolution of the image (Meta backend only).</p> <code>1008</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for the model.</p> <code>0.5</code> <code>mask_threshold</code> <code>float</code> <p>Mask threshold for post-processing (Transformers backend only).</p> <code>0.5</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def __init__(\n    self,\n    backend=\"meta\",\n    model_id=\"facebook/sam3\",\n    bpe_path=None,\n    device=None,\n    eval_mode=True,\n    checkpoint_path=None,\n    load_from_HF=True,\n    enable_segmentation=True,\n    enable_inst_interactivity=False,\n    compile_mode=False,\n    resolution=1008,\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the SamGeo3 class.\n\n    Args:\n        backend (str): Backend to use ('meta' or 'transformers'). Default is 'meta'.\n        model_id (str): Model ID for Transformers backend (e.g., 'facebook/sam3').\n            Only used when backend='transformers'.\n        bpe_path (str, optional): Path to the BPE tokenizer vocabulary (Meta backend only).\n        device (str, optional): Device to load the model on ('cuda' or 'cpu').\n        eval_mode (bool, optional): Whether to set the model to evaluation mode (Meta backend only).\n        checkpoint_path (str, optional): Optional path to model checkpoint (Meta backend only).\n        load_from_HF (bool, optional): Whether to load the model from HuggingFace (Meta backend only).\n        enable_segmentation (bool, optional): Whether to enable segmentation head (Meta backend only).\n        enable_inst_interactivity (bool, optional): Whether to enable instance interactivity\n            (SAM 1 task) (Meta backend only). Set to True to use predict_inst() and\n            predict_inst_batch() methods for interactive point and box prompts.\n            When True, the model loads additional components for SAM1-style\n            interactive instance segmentation. Defaults to False.\n        compile_mode (bool, optional): To enable compilation, set to \"default\" (Meta backend only).\n        resolution (int, optional): Resolution of the image (Meta backend only).\n        confidence_threshold (float, optional): Confidence threshold for the model.\n        mask_threshold (float, optional): Mask threshold for post-processing (Transformers backend only).\n        **kwargs: Additional keyword arguments.\n\n    Example:\n        &gt;&gt;&gt; # For text-based segmentation\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n        &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n        &gt;&gt;&gt; sam.generate_masks(\"tree\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # For interactive point/box prompts (SAM1-style)\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n        ...     point_coords=np.array([[520, 375]]),\n        ...     point_labels=np.array([1]),\n        ... )\n    \"\"\"\n\n    if backend not in [\"meta\", \"transformers\"]:\n        raise ValueError(\n            f\"Invalid backend '{backend}'. Choose 'meta' or 'transformers'.\"\n        )\n\n    if backend == \"meta\" and not SAM3_META_AVAILABLE:\n        error_msg = (\n            \"Meta SAM3 is not available. Please install it as:\\n\"\n            \"\\tpip install segment-geospatial[samgeo3]\"\n        )\n        if SAM3_META_IMPORT_ERROR is not None:\n            error_msg += f\"\\n\\nUnderlying import error:\\n\\t{SAM3_META_IMPORT_ERROR}\"\n        raise ImportError(error_msg)\n\n    if backend == \"transformers\" and not SAM3_TRANSFORMERS_AVAILABLE:\n        error_msg = (\n            \"Transformers SAM3 is not available. Please install it as:\\n\"\n            \"\\tpip install transformers torch\"\n        )\n        if SAM3_TRANSFORMERS_IMPORT_ERROR is not None:\n            error_msg += (\n                f\"\\n\\nUnderlying import error:\\n\\t{SAM3_TRANSFORMERS_IMPORT_ERROR}\"\n            )\n        raise ImportError(error_msg)\n\n    if device is None:\n        device = common.get_device()\n\n    print(f\"Using {device} device and {backend} backend\")\n\n    self.backend = backend\n    self.device = device\n    self.confidence_threshold = confidence_threshold\n    self.mask_threshold = mask_threshold\n    self.model_id = model_id\n    self.model_version = \"sam3\"\n\n    # Initialize backend-specific components\n    if backend == \"meta\":\n        self._init_meta_backend(\n            bpe_path=bpe_path,\n            device=device,\n            eval_mode=eval_mode,\n            checkpoint_path=checkpoint_path,\n            load_from_HF=load_from_HF,\n            enable_segmentation=enable_segmentation,\n            enable_inst_interactivity=enable_inst_interactivity,\n            compile_mode=compile_mode,\n            resolution=resolution,\n            confidence_threshold=confidence_threshold,\n        )\n    else:  # transformers\n        self._init_transformers_backend(\n            model_id=model_id,\n            device=device,\n        )\n\n    # Common attributes\n    self.predictor = None\n    self.masks = None\n    self.boxes = None\n    self.scores = None\n    self.logits = None\n    self.objects = None\n    self.prediction = None\n    self.source = None\n    self.image = None\n    self.image_height = None\n    self.image_width = None\n    self.inference_state = None\n\n    # Batch processing attributes\n    self.images_batch = None\n    self.sources_batch = None\n    self.batch_state = None\n    self.batch_results = None\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.__init__--for-text-based-segmentation","title":"For text-based segmentation","text":"<p>sam = SamGeo3(backend=\"meta\") sam.set_image(\"image.jpg\") sam.generate_masks(\"tree\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.__init__--for-interactive-pointbox-prompts-sam1-style","title":"For interactive point/box prompts (SAM1-style)","text":"<p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"image.jpg\") masks, scores, logits = sam.predict_inst( ...     point_coords=np.array([[520, 375]]), ...     point_labels=np.array([1]), ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks","title":"<code>generate_masks(prompt, min_size=0, max_size=None, quiet=False, **kwargs)</code>","text":"<p>Generate masks for the input image using SAM3.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt describing the objects to segment.</p> required <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress messages. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing the generated masks.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks(\n    self,\n    prompt: str,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    quiet: bool = False,\n    **kwargs: Any,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate masks for the input image using SAM3.\n\n    Args:\n        prompt (str): The text prompt describing the objects to segment.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n        quiet (bool): If True, suppress progress messages. Defaults to False.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries containing the generated masks.\n    \"\"\"\n    if self.backend == \"meta\":\n        self.processor.reset_all_prompts(self.inference_state)\n        output = self.processor.set_text_prompt(\n            state=self.inference_state, prompt=prompt\n        )\n\n        self.masks = output[\"masks\"]\n        self.boxes = output[\"boxes\"]\n        self.scores = output[\"scores\"]\n    else:  # transformers\n        if not hasattr(self, \"pil_image\"):\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n        # Prepare inputs\n        inputs = self.processor(\n            images=self.pil_image, text=prompt, return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Get original sizes for post-processing\n        original_sizes = inputs.get(\"original_sizes\")\n        if original_sizes is not None:\n            original_sizes = original_sizes.tolist()\n        else:\n            original_sizes = [[self.image_height, self.image_width]]\n\n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        # Post-process results\n        results = self.processor.post_process_instance_segmentation(\n            outputs,\n            threshold=self.confidence_threshold,\n            mask_threshold=self.mask_threshold,\n            target_sizes=original_sizes,\n        )[0]\n\n        # Convert results to match Meta backend format\n        self.masks = results[\"masks\"]\n        self.boxes = results[\"boxes\"]\n        self.scores = results[\"scores\"]\n\n    # Convert tensors to numpy to free GPU memory\n    self._convert_results_to_numpy()\n\n    # Filter masks by size if min_size or max_size is specified\n    if min_size &gt; 0 or max_size is not None:\n        self._filter_masks_by_size(min_size, max_size)\n\n    num_objects = len(self.masks)\n    if not quiet:\n        if num_objects == 0:\n            print(\"No objects found. Please try a different prompt.\")\n        elif num_objects == 1:\n            print(\"Found one object.\")\n        else:\n            print(f\"Found {num_objects} objects.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_batch","title":"<code>generate_masks_batch(prompt, min_size=0, max_size=None)</code>","text":"<p>Generate masks for all images in the batch using SAM3.</p> <p>Note: This method is only available for the Meta backend.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt describing the objects to segment.</p> required <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> Example <p>sam = SamGeo3(backend=\"meta\") sam.set_image_batch([\"image1.jpg\", \"image2.jpg\"]) results = sam.generate_masks_batch(\"building\") for i, result in enumerate(results): ...     print(f\"Image {i}: Found {len(result['masks'])} objects\")</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_batch(\n    self,\n    prompt: str,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Generate masks for all images in the batch using SAM3.\n\n    Note: This method is only available for the Meta backend.\n\n    Args:\n        prompt (str): The text prompt describing the objects to segment.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n        &gt;&gt;&gt; sam.set_image_batch([\"image1.jpg\", \"image2.jpg\"])\n        &gt;&gt;&gt; results = sam.generate_masks_batch(\"building\")\n        &gt;&gt;&gt; for i, result in enumerate(results):\n        ...     print(f\"Image {i}: Found {len(result['masks'])} objects\")\n    \"\"\"\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"Batch mask generation is only available for the Meta backend.\"\n        )\n\n    if self.batch_state is None:\n        raise ValueError(\n            \"No images set for batch processing. \"\n            \"Please call set_image_batch() first.\"\n        )\n\n    batch_results = []\n    num_images = len(self.images_batch)\n\n    # The batch backbone features are computed once, but text prompting\n    # needs to be done per-image since set_text_prompt expects singular\n    # original_height/original_width keys\n    backbone_out = self.batch_state.get(\"backbone_out\", {})\n\n    for i in range(num_images):\n        # Create a per-image state with the correct singular keys\n        image_state = {\n            \"original_height\": self.batch_state[\"original_heights\"][i],\n            \"original_width\": self.batch_state[\"original_widths\"][i],\n        }\n\n        # Extract backbone features for this specific image\n        # The backbone_out contains batched features, we need to slice them\n        image_backbone_out = self._extract_image_backbone_features(backbone_out, i)\n        image_state[\"backbone_out\"] = image_backbone_out\n\n        # Reset prompts and set text prompt for this image\n        self.processor.reset_all_prompts(image_state)\n        output = self.processor.set_text_prompt(state=image_state, prompt=prompt)\n\n        # Build result for this image\n        result = {\n            \"masks\": output.get(\"masks\", []),\n            \"boxes\": output.get(\"boxes\", []),\n            \"scores\": output.get(\"scores\", []),\n            \"image\": self.images_batch[i],\n            \"source\": self.sources_batch[i],\n        }\n\n        # Convert tensors to numpy\n        result = self._convert_batch_result_to_numpy(result)\n\n        # Filter by size if needed\n        if min_size &gt; 0 or max_size is not None:\n            result = self._filter_batch_result_by_size(result, min_size, max_size)\n\n        batch_results.append(result)\n\n    self.batch_results = batch_results\n\n    # Print summary\n    total_objects = sum(len(r.get(\"masks\", [])) for r in batch_results)\n    print(\n        f\"Processed {num_images} image(s), found {total_objects} total object(s).\"\n    )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes","title":"<code>generate_masks_by_boxes(boxes, box_labels=None, box_crs=None, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks using bounding box prompts.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>List[List[float]]</code> <p>List of bounding boxes in XYXY format [[xmin, ymin, xmax, ymax], ...]. If box_crs is None: pixel coordinates. If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").</p> required <code>box_labels</code> <code>List[bool]</code> <p>List of boolean labels for each box. True for positive prompt (include), False for negative prompt (exclude). If None, all boxes are treated as positive prompts.</p> <code>None</code> <code>box_crs</code> <code>str</code> <p>Coordinate reference system for box coordinates (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF. If None, boxes are assumed to be in pixel coordinates.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing masks, boxes, and scores.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_by_boxes(\n    self,\n    boxes: List[List[float]],\n    box_labels: Optional[List[bool]] = None,\n    box_crs: Optional[str] = None,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate masks using bounding box prompts.\n\n    Args:\n        boxes (List[List[float]]): List of bounding boxes in XYXY format\n            [[xmin, ymin, xmax, ymax], ...].\n            If box_crs is None: pixel coordinates.\n            If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n        box_labels (List[bool], optional): List of boolean labels for each box.\n            True for positive prompt (include), False for negative prompt (exclude).\n            If None, all boxes are treated as positive prompts.\n        box_crs (str, optional): Coordinate reference system for box coordinates\n            (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n            If None, boxes are assumed to be in pixel coordinates.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing masks, boxes, and scores.\n\n    Example:\n        # For pixel coordinates:\n        boxes = [[100, 200, 300, 400]]\n        sam.generate_masks_by_boxes(boxes)\n\n        # For geographic coordinates (GeoTIFF):\n        boxes = [[-122.5, 37.7, -122.4, 37.8]]  # [lon_min, lat_min, lon_max, lat_max]\n        sam.generate_masks_by_boxes(boxes, box_crs=\"EPSG:4326\")\n    \"\"\"\n    if self.backend == \"meta\":\n        if self.inference_state is None:\n            raise ValueError(\"No image set. Please call set_image() first.\")\n    else:  # transformers\n        if not hasattr(self, \"pil_image\"):\n            raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if box_labels is None:\n        box_labels = [True] * len(boxes)\n\n    if len(boxes) != len(box_labels):\n        raise ValueError(\n            f\"Number of boxes ({len(boxes)}) must match number of labels ({len(box_labels)})\"\n        )\n\n    # Transform boxes from CRS to pixel coordinates if needed\n    if box_crs is not None and self.source is not None:\n        pixel_boxes = []\n        for box in boxes:\n            xmin, ymin, xmax, ymax = box\n\n            # Transform min corner\n            min_coords = np.array([[xmin, ymin]])\n            min_xy, _ = common.coords_to_xy(\n                self.source, min_coords, box_crs, return_out_of_bounds=True\n            )\n\n            # Transform max corner\n            max_coords = np.array([[xmax, ymax]])\n            max_xy, _ = common.coords_to_xy(\n                self.source, max_coords, box_crs, return_out_of_bounds=True\n            )\n\n            # Convert to pixel coordinates and ensure correct min/max order\n            # (geographic y increases north, pixel y increases down)\n            x1_px = min_xy[0][0]\n            y1_px = min_xy[0][1]\n            x2_px = max_xy[0][0]\n            y2_px = max_xy[0][1]\n\n            # Ensure we have correct min/max values\n            x_min_px = min(x1_px, x2_px)\n            y_min_px = min(y1_px, y2_px)\n            x_max_px = max(x1_px, x2_px)\n            y_max_px = max(y1_px, y2_px)\n\n            pixel_boxes.append([x_min_px, y_min_px, x_max_px, y_max_px])\n\n        boxes = pixel_boxes\n\n    # Get image dimensions\n    width = self.image_width\n    height = self.image_height\n\n    if self.backend == \"meta\":\n        # Reset all prompts\n        self.processor.reset_all_prompts(self.inference_state)\n\n        # Process each box\n        for box, label in zip(boxes, box_labels):\n            # Convert XYXY to CxCyWH format\n            xmin, ymin, xmax, ymax = box\n            w = xmax - xmin\n            h = ymax - ymin\n            cx = xmin + w / 2\n            cy = ymin + h / 2\n\n            # Normalize to [0, 1] range\n            norm_box = [cx / width, cy / height, w / width, h / height]\n\n            # Add geometric prompt\n            self.inference_state = self.processor.add_geometric_prompt(\n                state=self.inference_state, box=norm_box, label=label\n            )\n\n        # Get the masks from the inference state\n        output = self.inference_state\n\n        self.masks = output[\"masks\"]\n        self.boxes = output[\"boxes\"]\n        self.scores = output[\"scores\"]\n    else:  # transformers\n        # For Transformers backend, process boxes with the processor\n        # Convert boxes to the format expected by Transformers\n        # Transformers expects boxes in XYXY format with 3 levels of nesting:\n        # [image level, box level, box coordinates]\n        # Also convert numpy types to Python native types\n        input_boxes = [\n            [[float(coord) for coord in box] for box in boxes]\n        ]  # Wrap in list for image level and convert to float\n\n        # Prepare inputs with boxes\n        inputs = self.processor(\n            images=self.pil_image, input_boxes=input_boxes, return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Get original sizes for post-processing\n        original_sizes = inputs.get(\"original_sizes\")\n        if original_sizes is not None:\n            original_sizes = original_sizes.tolist()\n        else:\n            original_sizes = [[self.image_height, self.image_width]]\n\n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        # Post-process results\n        results = self.processor.post_process_instance_segmentation(\n            outputs,\n            threshold=self.confidence_threshold,\n            mask_threshold=self.mask_threshold,\n            target_sizes=original_sizes,\n        )[0]\n\n        # Convert results to match Meta backend format\n        self.masks = results[\"masks\"]\n        self.boxes = results[\"boxes\"]\n        self.scores = results[\"scores\"]\n\n    # Convert tensors to numpy to free GPU memory\n    self._convert_results_to_numpy()\n\n    # Filter masks by size if min_size or max_size is specified\n    if min_size &gt; 0 or max_size is not None:\n        self._filter_masks_by_size(min_size, max_size)\n\n    num_objects = len(self.masks)\n    if num_objects == 0:\n        print(\"No objects found. Please check your box prompts.\")\n    elif num_objects == 1:\n        print(\"Found one object.\")\n    else:\n        print(f\"Found {num_objects} objects.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes--for-pixel-coordinates","title":"For pixel coordinates:","text":"<p>boxes = [[100, 200, 300, 400]] sam.generate_masks_by_boxes(boxes)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes--for-geographic-coordinates-geotiff","title":"For geographic coordinates (GeoTIFF):","text":"<p>boxes = [[-122.5, 37.7, -122.4, 37.8]]  # [lon_min, lat_min, lon_max, lat_max] sam.generate_masks_by_boxes(boxes, box_crs=\"EPSG:4326\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst","title":"<code>generate_masks_by_boxes_inst(boxes, box_crs=None, output=None, multimask_output=True, min_size=0, max_size=None, dtype='uint8', **kwargs)</code>","text":"<p>Generate masks using bounding box prompts with instance interactivity.</p> <p>This is a high-level method that wraps predict_inst() for ease of use. It stores the results in self.masks, self.scores, and self.boxes for subsequent use with show_anns(), show_masks(), and save_masks().</p> <p>Note: This method requires the model to be initialized with <code>enable_inst_interactivity=True</code> (Meta backend only).</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>List[List[float]] | str | GeoDataFrame</code> <p>Bounding boxes for segmentation. Can be: - A list of [xmin, ymin, xmax, ymax] coordinates - A file path to a vector file (GeoJSON, Shapefile, etc.) - A GeoDataFrame with polygon geometries If box_crs is None: pixel coordinates (for list input). If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").</p> required <code>box_crs</code> <code>str</code> <p>Coordinate reference system for box coordinates (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF. If None, boxes are assumed to be in pixel coordinates.</p> <code>None</code> <code>output</code> <code>str</code> <p>Path to save the output mask as a GeoTIFF. If None, masks are stored in memory only.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If True, the model returns 3 masks per box and the best one is selected by score. If False, returns single mask directly. Defaults to True.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type for the output mask array. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to predict_inst() or save_masks().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing masks, scores, and logits.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_by_boxes_inst(\n    self,\n    boxes: Union[List[List[float]], str, \"gpd.GeoDataFrame\", dict, np.ndarray],\n    box_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    multimask_output: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    dtype: str = \"uint8\",\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate masks using bounding box prompts with instance interactivity.\n\n    This is a high-level method that wraps predict_inst() for ease of use.\n    It stores the results in self.masks, self.scores, and self.boxes for\n    subsequent use with show_anns(), show_masks(), and save_masks().\n\n    Note: This method requires the model to be initialized with\n    `enable_inst_interactivity=True` (Meta backend only).\n\n    Args:\n        boxes (List[List[float]] | str | GeoDataFrame): Bounding boxes for\n            segmentation. Can be:\n            - A list of [xmin, ymin, xmax, ymax] coordinates\n            - A file path to a vector file (GeoJSON, Shapefile, etc.)\n            - A GeoDataFrame with polygon geometries\n            If box_crs is None: pixel coordinates (for list input).\n            If box_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n        box_crs (str, optional): Coordinate reference system for box coordinates\n            (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n            If None, boxes are assumed to be in pixel coordinates.\n        output (str, optional): Path to save the output mask as a GeoTIFF.\n            If None, masks are stored in memory only.\n        multimask_output (bool): If True, the model returns 3 masks per box and the\n            best one is selected by score. If False, returns single mask directly.\n            Defaults to True.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n        dtype (str): Data type for the output mask array. Defaults to \"uint8\".\n        **kwargs: Additional keyword arguments passed to predict_inst() or save_masks().\n\n    Returns:\n        Dict[str, Any]: Dictionary containing masks, scores, and logits.\n\n    Example:\n        &gt;&gt;&gt; # Initialize with instance interactivity enabled\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt; sam.set_image(\"satellite.tif\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Single box (pixel coordinates)\n        &gt;&gt;&gt; sam.generate_masks_by_boxes_inst([[425, 600, 700, 875]])\n        &gt;&gt;&gt; sam.show_anns()\n        &gt;&gt;&gt; sam.save_masks(\"mask.png\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Multiple boxes with geographic coordinates\n        &gt;&gt;&gt; sam.generate_masks_by_boxes_inst([\n        ...     [-117.5995, 47.6518, -117.5988, 47.652],\n        ...     [-117.5987, 47.6518, -117.5979, 47.652],\n        ... ], box_crs=\"EPSG:4326\", output=\"mask.tif\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using a vector file (GeoJSON, Shapefile, etc.)\n        &gt;&gt;&gt; sam.generate_masks_by_boxes_inst(\n        ...     \"building_bboxes.geojson\",\n        ...     box_crs=\"EPSG:4326\",\n        ...     output=\"building_masks.tif\",\n        ...     dtype=\"uint16\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using a GeoDataFrame\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; gdf = gpd.read_file(\"polygons.geojson\")\n        &gt;&gt;&gt; sam.generate_masks_by_boxes_inst(\n        ...     gdf,\n        ...     box_crs=\"EPSG:4326\",\n        ...     output=\"masks.tif\",\n        ... )\n    \"\"\"\n    import geopandas as gpd\n\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"generate_masks_by_boxes_inst is only available for the Meta backend. \"\n            \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n        )\n\n    if self.inference_state is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if (\n        not hasattr(self.model, \"inst_interactive_predictor\")\n        or self.model.inst_interactive_predictor is None\n    ):\n        raise ValueError(\n            \"Instance interactivity not enabled. Please initialize with \"\n            \"enable_inst_interactivity=True.\"\n        )\n\n    # Process boxes based on input type\n    if isinstance(boxes, dict):\n        # GeoJSON-like dict\n        boxes = gpd.GeoDataFrame.from_features(boxes)\n\n    if isinstance(boxes, (str, gpd.GeoDataFrame)):\n        # File path or GeoDataFrame\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n        else:\n            gdf = boxes\n\n        if gdf.crs is None and box_crs is not None:\n            gdf.crs = box_crs\n        elif gdf.crs is not None and box_crs is None:\n            box_crs = str(gdf.crs)\n\n        # Extract bounding boxes from geometries\n        boxes_list = gdf.geometry.apply(lambda geom: list(geom.bounds)).tolist()\n    elif isinstance(boxes, list):\n        boxes_list = boxes\n    elif isinstance(boxes, np.ndarray):\n        boxes_list = boxes.tolist()\n    else:\n        raise ValueError(\n            \"boxes must be a list, GeoDataFrame, file path, or numpy array.\"\n        )\n\n    # Filter boxes that are out of image bounds if box_crs is provided\n    if box_crs is not None and self.source is not None:\n        import rasterio\n        from rasterio.warp import transform_bounds\n\n        with rasterio.open(self.source) as src:\n            img_bounds = transform_bounds(src.crs, box_crs, *src.bounds)\n\n        xmin_img, ymin_img, xmax_img, ymax_img = img_bounds\n\n        valid_boxes = []\n        filtered_count = 0\n        for box in boxes_list:\n            xmin, ymin, xmax, ymax = box\n            # Check if box overlaps with image bounds\n            if (\n                xmax &gt; xmin_img\n                and xmin &lt; xmax_img\n                and ymax &gt; ymin_img\n                and ymin &lt; ymax_img\n            ):\n                valid_boxes.append(box)\n            else:\n                filtered_count += 1\n\n        if filtered_count &gt; 0:\n            print(\n                f\"Filtered {filtered_count} boxes outside image bounds. \"\n                f\"Using {len(valid_boxes)} valid boxes.\"\n            )\n\n        if len(valid_boxes) == 0:\n            print(\"No valid boxes found within image bounds.\")\n            self.masks = []\n            self.scores = []\n            self.boxes = []\n            return\n\n        boxes_list = valid_boxes\n\n    # Convert to numpy array\n    boxes_arr = np.array(boxes_list)\n\n    # Call predict_inst with box prompts\n    masks, scores, logits = self.predict_inst(\n        box=boxes_arr,\n        multimask_output=multimask_output,\n        box_crs=box_crs,\n    )\n\n    # Handle batch output shape (BxCxHxW) vs single (CxHxW)\n    if masks.ndim == 4:\n        # Multiple boxes - flatten into list of masks\n        # Each box produces C masks, take the best one per box\n        all_masks = []\n        all_scores = []\n        for i in range(masks.shape[0]):\n            if multimask_output and masks.shape[1] &gt; 1:\n                best_idx = np.argmax(scores[i])\n                all_masks.append(masks[i, best_idx])\n                all_scores.append(scores[i, best_idx])\n            else:\n                all_masks.append(masks[i, 0] if masks.shape[1] &gt; 0 else masks[i])\n                all_scores.append(scores[i, 0] if len(scores[i]) &gt; 0 else scores[i])\n        masks = all_masks\n        scores = all_scores\n    else:\n        # Single box\n        if multimask_output and len(masks) &gt; 1:\n            best_idx = np.argmax(scores)\n            masks = [masks[best_idx]]\n            scores = [scores[best_idx]]\n        else:\n            masks = (\n                [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n            )\n            scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n\n    # Store results\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    # Compute bounding boxes from masks\n    self.boxes = []\n    for mask in self.masks:\n        if hasattr(mask, \"ndim\") and mask.ndim &gt; 2:\n            mask = mask.squeeze()\n        mask_arr = np.asarray(mask)\n        ys, xs = np.where(mask_arr &gt; 0)\n        if len(xs) &gt; 0 and len(ys) &gt; 0:\n            self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n        else:\n            self.boxes.append(np.array([0, 0, 0, 0]))\n\n    # Filter masks by size if min_size or max_size is specified\n    if min_size &gt; 0 or max_size is not None:\n        self._filter_masks_by_size(min_size, max_size)\n\n    num_objects = len(self.masks)\n    if num_objects == 0:\n        print(\"No objects found. Please check your box prompts.\")\n    elif num_objects == 1:\n        print(\"Found one object.\")\n    else:\n        print(f\"Found {num_objects} objects.\")\n\n    # Save masks if output path is provided\n    if output is not None:\n        self.save_masks(\n            output,\n            min_size=min_size,\n            max_size=max_size,\n            dtype=dtype,\n            **kwargs,\n        )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst--initialize-with-instance-interactivity-enabled","title":"Initialize with instance interactivity enabled","text":"<p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"satellite.tif\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst--single-box-pixel-coordinates","title":"Single box (pixel coordinates)","text":"<p>sam.generate_masks_by_boxes_inst([[425, 600, 700, 875]]) sam.show_anns() sam.save_masks(\"mask.png\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst--multiple-boxes-with-geographic-coordinates","title":"Multiple boxes with geographic coordinates","text":"<p>sam.generate_masks_by_boxes_inst([ ...     [-117.5995, 47.6518, -117.5988, 47.652], ...     [-117.5987, 47.6518, -117.5979, 47.652], ... ], box_crs=\"EPSG:4326\", output=\"mask.tif\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst--using-a-vector-file-geojson-shapefile-etc","title":"Using a vector file (GeoJSON, Shapefile, etc.)","text":"<p>sam.generate_masks_by_boxes_inst( ...     \"building_bboxes.geojson\", ...     box_crs=\"EPSG:4326\", ...     output=\"building_masks.tif\", ...     dtype=\"uint16\", ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_boxes_inst--using-a-geodataframe","title":"Using a GeoDataFrame","text":"<p>import geopandas as gpd gdf = gpd.read_file(\"polygons.geojson\") sam.generate_masks_by_boxes_inst( ...     gdf, ...     box_crs=\"EPSG:4326\", ...     output=\"masks.tif\", ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points","title":"<code>generate_masks_by_points(point_coords, point_labels=None, point_crs=None, multimask_output=True, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Generate masks using point prompts.</p> <p>This is a high-level method that wraps predict_inst() for ease of use. It stores the results in self.masks, self.scores, and self.boxes for subsequent use with show_anns(), show_masks(), and save_masks().</p> <p>Note: This method requires the model to be initialized with <code>enable_inst_interactivity=True</code> (Meta backend only).</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>List[List[float]]</code> <p>List of point coordinates [[x, y], ...]. If point_crs is None: pixel coordinates. If point_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").</p> required <code>point_labels</code> <code>List[int]</code> <p>List of labels for each point. 1 = foreground (include), 0 = background (exclude). If None, all points are treated as foreground (label=1).</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>Coordinate reference system for point coordinates (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF. If None, points are assumed to be in pixel coordinates.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If True, the model returns 3 masks and the best one is selected by score. Recommended for ambiguous prompts like single points. If False, returns single mask directly. Defaults to True.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to predict_inst().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing masks, scores, and logits.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_by_points(\n    self,\n    point_coords: List[List[float]],\n    point_labels: Optional[List[int]] = None,\n    point_crs: Optional[str] = None,\n    multimask_output: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate masks using point prompts.\n\n    This is a high-level method that wraps predict_inst() for ease of use.\n    It stores the results in self.masks, self.scores, and self.boxes for\n    subsequent use with show_anns(), show_masks(), and save_masks().\n\n    Note: This method requires the model to be initialized with\n    `enable_inst_interactivity=True` (Meta backend only).\n\n    Args:\n        point_coords (List[List[float]]): List of point coordinates [[x, y], ...].\n            If point_crs is None: pixel coordinates.\n            If point_crs is specified: coordinates in the given CRS (e.g., \"EPSG:4326\").\n        point_labels (List[int], optional): List of labels for each point.\n            1 = foreground (include), 0 = background (exclude).\n            If None, all points are treated as foreground (label=1).\n        point_crs (str, optional): Coordinate reference system for point coordinates\n            (e.g., \"EPSG:4326\" for lat/lon). Only used if the source image is a GeoTIFF.\n            If None, points are assumed to be in pixel coordinates.\n        multimask_output (bool): If True, the model returns 3 masks and the best\n            one is selected by score. Recommended for ambiguous prompts like single\n            points. If False, returns single mask directly. Defaults to True.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n        **kwargs: Additional keyword arguments passed to predict_inst().\n\n    Returns:\n        Dict[str, Any]: Dictionary containing masks, scores, and logits.\n\n    Example:\n        # Initialize with instance interactivity enabled\n        sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        sam.set_image(\"image.jpg\")\n\n        # Single foreground point (pixel coordinates)\n        sam.generate_masks_by_points([[520, 375]])\n        sam.show_anns()\n        sam.save_masks(\"mask.png\")\n\n        # Multiple points with labels\n        sam.generate_masks_by_points(\n            [[500, 375], [1125, 625]],\n            point_labels=[1, 0]  # foreground, background\n        )\n\n        # Geographic coordinates (GeoTIFF)\n        sam.generate_masks_by_points(\n            [[-122.258, 37.871]],\n            point_crs=\"EPSG:4326\"\n        )\n    \"\"\"\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"generate_masks_by_points is only available for the Meta backend. \"\n            \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n        )\n\n    if self.inference_state is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if (\n        not hasattr(self.model, \"inst_interactive_predictor\")\n        or self.model.inst_interactive_predictor is None\n    ):\n        raise ValueError(\n            \"Instance interactivity not enabled. Please initialize with \"\n            \"enable_inst_interactivity=True.\"\n        )\n\n    # Convert to numpy array if it's a list\n    point_coords = np.array(point_coords)\n\n    # Default all points to foreground if no labels provided\n    if point_labels is None:\n        point_labels = np.ones(len(point_coords), dtype=np.int32)\n    else:\n        point_labels = np.array(point_labels, dtype=np.int32)\n\n    if len(point_coords) != len(point_labels):\n        raise ValueError(\n            f\"Number of points ({len(point_coords)}) must match number of labels ({len(point_labels)})\"\n        )\n\n    # Call predict_inst with the prompts\n    masks, scores, logits = self.predict_inst(\n        point_coords=point_coords,\n        point_labels=point_labels,\n        multimask_output=multimask_output,\n        point_crs=point_crs,\n        **kwargs,\n    )\n\n    # If multimask_output=True, select the best mask by score\n    if multimask_output and len(masks) &gt; 1:\n        best_idx = np.argmax(scores)\n        masks = masks[best_idx : best_idx + 1]\n        scores = scores[best_idx : best_idx + 1]\n        logits = logits[best_idx : best_idx + 1]\n\n    # Store results in the format expected by show_anns/show_masks/save_masks\n    self.masks = (\n        [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n    )\n    self.scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n    self.logits = logits\n\n    # Compute bounding boxes from masks\n    self.boxes = []\n    for mask in self.masks:\n        if mask.ndim &gt; 2:\n            mask = mask.squeeze()\n        ys, xs = np.where(mask &gt; 0)\n        if len(xs) &gt; 0 and len(ys) &gt; 0:\n            self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n        else:\n            self.boxes.append(np.array([0, 0, 0, 0]))\n\n    # Filter masks by size if min_size or max_size is specified\n    if min_size &gt; 0 or max_size is not None:\n        self._filter_masks_by_size(min_size, max_size)\n\n    num_objects = len(self.masks)\n    if num_objects == 0:\n        print(\"No objects found. Please check your point prompts.\")\n    elif num_objects == 1:\n        print(\"Found one object.\")\n    else:\n        print(f\"Found {num_objects} objects.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points--initialize-with-instance-interactivity-enabled","title":"Initialize with instance interactivity enabled","text":"<p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"image.jpg\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points--single-foreground-point-pixel-coordinates","title":"Single foreground point (pixel coordinates)","text":"<p>sam.generate_masks_by_points([[520, 375]]) sam.show_anns() sam.save_masks(\"mask.png\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points--multiple-points-with-labels","title":"Multiple points with labels","text":"<p>sam.generate_masks_by_points(     [[500, 375], [1125, 625]],     point_labels=[1, 0]  # foreground, background )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points--geographic-coordinates-geotiff","title":"Geographic coordinates (GeoTIFF)","text":"<p>sam.generate_masks_by_points(     [[-122.258, 37.871]],     point_crs=\"EPSG:4326\" )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points_patch","title":"<code>generate_masks_by_points_patch(point_coords_batch=None, point_labels_batch=None, point_crs=None, output=None, index=None, unique=True, min_size=0, max_size=None, dtype='int32', multimask_output=False, return_results=False, **kwargs)</code>","text":"<p>Generate masks for multiple point prompts using batch processing.</p> <p>This method is similar to SamGeo2.predict_by_points but uses SAM3's predict_inst_batch for efficient batch processing of point prompts on a single image. Each point is treated as a separate prompt to generate a separate mask.</p> <p>Note: This method requires the model to be initialized with <code>enable_inst_interactivity=True</code> (Meta backend only).</p> <p>Parameters:</p> Name Type Description Default <code>point_coords_batch</code> <code>List[List[float]] | str | GeoDataFrame</code> <p>Point coordinates for batch processing. Can be: - A list of [x, y] coordinates - A file path to a vector file (GeoJSON, Shapefile, etc.) - A GeoDataFrame with point geometries</p> <code>None</code> <code>point_labels_batch</code> <code>List[int]</code> <p>Labels for each point. 1 = foreground (include), 0 = background (exclude). If None, all points are treated as foreground (label=1).</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>Coordinate reference system for point coordinates (e.g., \"EPSG:4326\"). If provided, coordinates will be converted from the CRS to pixel coordinates. Required when using geographic coordinates with a GeoTIFF source image.</p> <code>None</code> <code>output</code> <code>str</code> <p>Path to save the output mask as a GeoTIFF. If None, masks are stored in memory only.</p> <code>None</code> <code>index</code> <code>int</code> <p>If multimask_output is True, select this specific mask index from each prediction. If None, selects the mask with the highest score for each point.</p> <code>None</code> <code>unique</code> <code>bool</code> <p>If True, each mask gets a unique integer value (1, 2, 3, ...). If False, all masks are combined into a binary mask. Defaults to True.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type for the output mask array. Defaults to \"int32\".</p> <code>'int32'</code> <code>multimask_output</code> <code>bool</code> <p>If True, the model returns 3 masks per prompt. Defaults to False for cleaner batch results.</p> <code>False</code> <code>return_results</code> <code>bool</code> <p>If True, returns the masks, scores, and logits. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_masks().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Tuple[List[Dict], List[ndarray], List[ndarray]]]</code> <p>If return_results is True: Tuple[List[Dict], List[np.ndarray], List[np.ndarray]]: Tuple of     (output_masks, scores, logits) where output_masks is a list     of dictionaries with 'segmentation' and 'area' keys.</p> <code>Optional[Tuple[List[Dict], List[ndarray], List[ndarray]]]</code> <p>If return_results is False: None</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_by_points_patch(\n    self,\n    point_coords_batch: Union[List[List[float]], str, Any] = None,\n    point_labels_batch: Optional[List[int]] = None,\n    point_crs: Optional[str] = None,\n    output: Optional[str] = None,\n    index: Optional[int] = None,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    dtype: str = \"int32\",\n    multimask_output: bool = False,\n    return_results: bool = False,\n    **kwargs: Any,\n) -&gt; Optional[Tuple[List[Dict], List[np.ndarray], List[np.ndarray]]]:\n    \"\"\"\n    Generate masks for multiple point prompts using batch processing.\n\n    This method is similar to SamGeo2.predict_by_points but uses SAM3's\n    predict_inst_batch for efficient batch processing of point prompts on\n    a single image. Each point is treated as a separate prompt to generate\n    a separate mask.\n\n    Note: This method requires the model to be initialized with\n    `enable_inst_interactivity=True` (Meta backend only).\n\n    Args:\n        point_coords_batch (List[List[float]] | str | GeoDataFrame): Point\n            coordinates for batch processing. Can be:\n            - A list of [x, y] coordinates\n            - A file path to a vector file (GeoJSON, Shapefile, etc.)\n            - A GeoDataFrame with point geometries\n        point_labels_batch (List[int], optional): Labels for each point.\n            1 = foreground (include), 0 = background (exclude).\n            If None, all points are treated as foreground (label=1).\n        point_crs (str, optional): Coordinate reference system for point\n            coordinates (e.g., \"EPSG:4326\"). If provided, coordinates will\n            be converted from the CRS to pixel coordinates. Required when\n            using geographic coordinates with a GeoTIFF source image.\n        output (str, optional): Path to save the output mask as a GeoTIFF.\n            If None, masks are stored in memory only.\n        index (int, optional): If multimask_output is True, select this\n            specific mask index from each prediction. If None, selects the\n            mask with the highest score for each point.\n        unique (bool): If True, each mask gets a unique integer value\n            (1, 2, 3, ...). If False, all masks are combined into a binary\n            mask. Defaults to True.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger\n            than this will be filtered out. Defaults to None (no maximum).\n        dtype (str): Data type for the output mask array. Defaults to \"int32\".\n        multimask_output (bool): If True, the model returns 3 masks per prompt.\n            Defaults to False for cleaner batch results.\n        return_results (bool): If True, returns the masks, scores, and logits.\n            Defaults to False.\n        **kwargs: Additional keyword arguments passed to save_masks().\n\n    Returns:\n        If return_results is True:\n            Tuple[List[Dict], List[np.ndarray], List[np.ndarray]]: Tuple of\n                (output_masks, scores, logits) where output_masks is a list\n                of dictionaries with 'segmentation' and 'area' keys.\n        If return_results is False:\n            None\n\n    Example:\n        &gt;&gt;&gt; # Initialize with instance interactivity enabled\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt; sam.set_image(\"satellite.tif\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using a list of geographic coordinates\n        &gt;&gt;&gt; point_coords = [\n        ...     [-117.599896, 47.655345],\n        ...     [-117.59992, 47.655167],\n        ...     [-117.599928, 47.654974],\n        ...     [-117.599518, 47.655337],\n        ... ]\n        &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n        ...     point_coords_batch=point_coords,\n        ...     point_crs=\"EPSG:4326\",\n        ...     output=\"masks.tif\",\n        ...     dtype=\"uint8\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using a vector file (GeoJSON, Shapefile, etc.)\n        &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n        ...     point_coords_batch=\"building_centroids.geojson\",\n        ...     point_crs=\"EPSG:4326\",\n        ...     output=\"building_masks.tif\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using a GeoDataFrame\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; gdf = gpd.read_file(\"points.geojson\")\n        &gt;&gt;&gt; sam.generate_masks_by_points_patch(\n        ...     point_coords_batch=gdf,\n        ...     point_crs=\"EPSG:4326\",\n        ...     output=\"masks.tif\",\n        ... )\n    \"\"\"\n    import geopandas as gpd\n\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"generate_masks_by_points_patch is only available for the Meta backend. \"\n            \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n        )\n\n    if self.source is None:\n        raise ValueError(\n            \"No source image set. This method requires a file path to be provided \"\n            \"to set_image() for georeferencing support.\"\n        )\n\n    if self.inference_state is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if (\n        not hasattr(self.model, \"inst_interactive_predictor\")\n        or self.model.inst_interactive_predictor is None\n    ):\n        raise ValueError(\n            \"Instance interactivity not enabled. Please initialize with \"\n            \"enable_inst_interactivity=True.\"\n        )\n\n    # Process point coordinates based on input type\n    if isinstance(point_coords_batch, dict):\n        # GeoJSON-like dict\n        point_coords_batch = gpd.GeoDataFrame.from_features(point_coords_batch)\n\n    if isinstance(point_coords_batch, str) or isinstance(\n        point_coords_batch, gpd.GeoDataFrame\n    ):\n        # File path or GeoDataFrame\n        if isinstance(point_coords_batch, str):\n            gdf = gpd.read_file(point_coords_batch)\n        else:\n            gdf = point_coords_batch\n\n        if gdf.crs is None and point_crs is not None:\n            gdf.crs = point_crs\n\n        # Extract point coordinates from geometry\n        points_list = gdf.geometry.apply(lambda geom: [geom.x, geom.y]).tolist()\n        coordinates_array = np.array([[point] for point in points_list])\n\n        # Convert coordinates to pixel coordinates\n        points, out_of_bounds = common.coords_to_xy(\n            self.source, coordinates_array, gdf.crs, return_out_of_bounds=True\n        )\n        num_points = points.shape[0]\n\n        # Filter labels to match filtered coordinates\n        if point_labels_batch is not None and len(out_of_bounds) &gt; 0:\n            valid_indices = [\n                i for i in range(len(points_list)) if i not in out_of_bounds\n            ]\n            point_labels_batch = [point_labels_batch[i] for i in valid_indices]\n\n        if point_labels_batch is None:\n            labels = np.array([[1] for _ in range(num_points)])\n        else:\n            labels = np.array([[label] for label in point_labels_batch])\n\n    elif isinstance(point_coords_batch, list):\n        if point_crs is not None:\n            # Convert from CRS to pixel coordinates\n            point_coords_array = np.array(point_coords_batch)\n            point_coords_xy, out_of_bounds = common.coords_to_xy(\n                self.source,\n                point_coords_array,\n                point_crs,\n                return_out_of_bounds=True,\n            )\n            # Filter labels to match filtered coordinates\n            if point_labels_batch is not None:\n                # Create a mask for valid (in-bounds) indices\n                valid_indices = [\n                    i\n                    for i in range(len(point_coords_batch))\n                    if i not in out_of_bounds\n                ]\n                point_labels_batch = [point_labels_batch[i] for i in valid_indices]\n        else:\n            point_coords_xy = np.array(point_coords_batch)\n\n        # Format points for batch processing: each point as separate prompt\n        points = np.array([[point] for point in point_coords_xy])\n        num_points = len(points)\n\n        if point_labels_batch is None:\n            labels = np.array([[1] for _ in range(num_points)])\n        elif isinstance(point_labels_batch, list):\n            labels = np.array([[label] for label in point_labels_batch])\n        else:\n            labels = point_labels_batch\n\n    elif isinstance(point_coords_batch, np.ndarray):\n        points = point_coords_batch\n        labels = point_labels_batch\n        if labels is None:\n            num_points = points.shape[0]\n            labels = np.array([[1] for _ in range(num_points)])\n    else:\n        raise ValueError(\n            \"point_coords_batch must be a list, GeoDataFrame, file path, or numpy array.\"\n        )\n\n    # Set up batch state for single image batch processing\n    # Use set_image_batch with the current image\n    if self.images_batch is None:\n        pil_image = Image.fromarray(self.image)\n        self.batch_state = self.processor.set_image_batch([pil_image], state=None)\n        self.images_batch = [self.image]\n        self.sources_batch = [self.source]\n\n    # Call predict_inst_batch with the formatted points\n    # predict_inst_batch expects batches per image, we have one image with multiple points\n    masks_batch, scores_batch, logits_batch = self.model.predict_inst_batch(\n        self.batch_state,\n        [points],  # One entry for our single image\n        [labels],  # One entry for our single image\n        box_batch=None,\n        mask_input_batch=None,\n        multimask_output=multimask_output,\n        return_logits=False,\n        normalize_coords=True,\n    )\n\n    # Extract results for our single image\n    masks = masks_batch[0]\n    scores = scores_batch[0]\n    logits = logits_batch[0]\n\n    # Handle multimask output - select best mask per prompt if index not specified\n    if multimask_output and index is not None:\n        masks = masks[:, index, :, :]\n    elif multimask_output and masks.ndim == 4:\n        # Select best mask for each prompt based on scores\n        best_masks = []\n        best_scores = []\n        for i in range(masks.shape[0]):\n            best_idx = np.argmax(scores[i])\n            best_masks.append(masks[i, best_idx])\n            best_scores.append(scores[i, best_idx])\n        masks = np.array(best_masks)\n        scores = np.array(best_scores)\n\n    if masks.ndim &gt; 3:\n        masks = masks.squeeze()\n\n    # Ensure masks is 3D (num_masks, H, W)\n    if masks.ndim == 2:\n        masks = masks[np.newaxis, ...]  # Add batch dimension\n\n    # Store results in the format expected by show_anns/show_masks/save_masks\n    self.masks = [masks[i] for i in range(len(masks))]\n    self.scores = scores if isinstance(scores, list) else list(scores.flatten())\n    self.logits = logits\n\n    # Create output mask list with segmentation dict format for return value\n    output_masks = []\n    sums = np.sum(masks, axis=(1, 2))\n    for idx, mask in enumerate(masks):\n        item = {\n            \"segmentation\": mask.astype(\"bool\"),\n            \"area\": sums[idx],\n        }\n        output_masks.append(item)\n\n    # Compute bounding boxes from masks\n    self.boxes = []\n    for mask in self.masks:\n        if mask.ndim &gt; 2:\n            mask = mask.squeeze()\n        ys, xs = np.where(mask &gt; 0)\n        if len(xs) &gt; 0 and len(ys) &gt; 0:\n            self.boxes.append(np.array([xs.min(), ys.min(), xs.max(), ys.max()]))\n        else:\n            self.boxes.append(np.array([0, 0, 0, 0]))\n\n    # Save masks if output path is provided\n    if output is not None:\n        self.save_masks(\n            output,\n            unique=unique,\n            min_size=min_size,\n            max_size=max_size,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    num_objects = len(self.masks)\n    if num_objects == 0:\n        print(\"No objects found. Please check your point prompts.\")\n    elif num_objects == 1:\n        print(\"Found one object.\")\n    else:\n        print(f\"Found {num_objects} objects.\")\n\n    if return_results:\n        return output_masks, scores, logits\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points_patch--initialize-with-instance-interactivity-enabled","title":"Initialize with instance interactivity enabled","text":"<p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"satellite.tif\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points_patch--using-a-list-of-geographic-coordinates","title":"Using a list of geographic coordinates","text":"<p>point_coords = [ ...     [-117.599896, 47.655345], ...     [-117.59992, 47.655167], ...     [-117.599928, 47.654974], ...     [-117.599518, 47.655337], ... ] sam.generate_masks_by_points_patch( ...     point_coords_batch=point_coords, ...     point_crs=\"EPSG:4326\", ...     output=\"masks.tif\", ...     dtype=\"uint8\", ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points_patch--using-a-vector-file-geojson-shapefile-etc","title":"Using a vector file (GeoJSON, Shapefile, etc.)","text":"<p>sam.generate_masks_by_points_patch( ...     point_coords_batch=\"building_centroids.geojson\", ...     point_crs=\"EPSG:4326\", ...     output=\"building_masks.tif\", ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_by_points_patch--using-a-geodataframe","title":"Using a GeoDataFrame","text":"<p>import geopandas as gpd gdf = gpd.read_file(\"points.geojson\") sam.generate_masks_by_points_patch( ...     point_coords_batch=gdf, ...     point_crs=\"EPSG:4326\", ...     output=\"masks.tif\", ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.generate_masks_tiled","title":"<code>generate_masks_tiled(source, prompt, output, tile_size=1024, overlap=128, min_size=0, max_size=None, unique=True, dtype='uint32', bands=None, batch_size=1, verbose=True, **kwargs)</code>","text":"<p>Generate masks for large GeoTIFF images using a sliding window approach.</p> <p>This method processes large images tile by tile to avoid GPU memory issues. The tiles are processed with overlap to ensure seamless mask merging at boundaries. Each detected object gets a unique ID that is consistent across the entire image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>prompt</code> <code>str</code> <p>The text prompt describing the objects to segment.</p> required <code>output</code> <code>str</code> <p>Path to the output GeoTIFF file.</p> required <code>tile_size</code> <code>int</code> <p>Size of each tile in pixels. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent tiles in pixels. Defaults to 128. Higher overlap helps with better boundary merging but increases processing time.</p> <code>128</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out. Defaults to None (no maximum).</p> <code>None</code> <code>unique</code> <code>bool</code> <p>If True, each mask gets a unique value. If False, binary mask (0 or 1). Defaults to True.</p> <code>True</code> <code>dtype</code> <code>str</code> <p>Data type for the output array. Use 'uint32' for large numbers of objects, 'uint16' for up to 65535 objects, or 'uint8' for up to 255 objects. Defaults to 'uint32'.</p> <code>'uint32'</code> <code>bands</code> <code>List[int]</code> <p>List of band indices (1-based) to use for RGB when the input has more than 3 bands. If None, uses first 3 bands.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of tiles to process at once (future use). Defaults to 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress information. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the output GeoTIFF file.</p> Example <p>sam = SamGeo3(backend=\"meta\") sam.generate_masks_tiled( ...     source=\"large_satellite_image.tif\", ...     prompt=\"building\", ...     output=\"buildings_mask.tif\", ...     tile_size=1024, ...     overlap=128, ... )</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks_tiled(\n    self,\n    source: str,\n    prompt: str,\n    output: str,\n    tile_size: int = 1024,\n    overlap: int = 128,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    unique: bool = True,\n    dtype: str = \"uint32\",\n    bands: Optional[List[int]] = None,\n    batch_size: int = 1,\n    verbose: bool = True,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Generate masks for large GeoTIFF images using a sliding window approach.\n\n    This method processes large images tile by tile to avoid GPU memory issues.\n    The tiles are processed with overlap to ensure seamless mask merging at\n    boundaries. Each detected object gets a unique ID that is consistent\n    across the entire image.\n\n    Args:\n        source (str): Path to the input GeoTIFF image.\n        prompt (str): The text prompt describing the objects to segment.\n        output (str): Path to the output GeoTIFF file.\n        tile_size (int): Size of each tile in pixels. Defaults to 1024.\n        overlap (int): Overlap between adjacent tiles in pixels. Defaults to 128.\n            Higher overlap helps with better boundary merging but increases\n            processing time.\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out. Defaults to 0.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out. Defaults to None (no maximum).\n        unique (bool): If True, each mask gets a unique value. If False, binary\n            mask (0 or 1). Defaults to True.\n        dtype (str): Data type for the output array. Use 'uint32' for large\n            numbers of objects, 'uint16' for up to 65535 objects, or 'uint8'\n            for up to 255 objects. Defaults to 'uint32'.\n        bands (List[int], optional): List of band indices (1-based) to use for RGB\n            when the input has more than 3 bands. If None, uses first 3 bands.\n        batch_size (int): Number of tiles to process at once (future use).\n            Defaults to 1.\n        verbose (bool): Whether to print progress information. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: Path to the output GeoTIFF file.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n        &gt;&gt;&gt; sam.generate_masks_tiled(\n        ...     source=\"large_satellite_image.tif\",\n        ...     prompt=\"building\",\n        ...     output=\"buildings_mask.tif\",\n        ...     tile_size=1024,\n        ...     overlap=128,\n        ... )\n    \"\"\"\n    import rasterio\n    from rasterio.windows import Window\n\n    if not source.lower().endswith((\".tif\", \".tiff\")):\n        raise ValueError(\"Source must be a GeoTIFF file for tiled processing.\")\n\n    if not os.path.exists(source):\n        raise ValueError(f\"Source file not found: {source}\")\n\n    if tile_size &lt;= overlap:\n        raise ValueError(\"tile_size must be greater than overlap\")\n\n    # Open the source file to get metadata\n    with rasterio.open(source) as src:\n        img_height = src.height\n        img_width = src.width\n        profile = src.profile.copy()\n\n    if verbose:\n        print(f\"Processing image: {img_width} x {img_height} pixels\")\n        print(f\"Tile size: {tile_size}, Overlap: {overlap}\")\n\n    # Calculate the number of tiles\n    step = tile_size - overlap\n    n_tiles_x = max(1, (img_width - overlap + step - 1) // step)\n    n_tiles_y = max(1, (img_height - overlap + step - 1) // step)\n    total_tiles = n_tiles_x * n_tiles_y\n\n    if verbose:\n        print(f\"Total tiles to process: {total_tiles} ({n_tiles_x} x {n_tiles_y})\")\n\n    # Determine output dtype\n    if dtype == \"uint8\":\n        np_dtype = np.uint8\n        max_objects = 255\n    elif dtype == \"uint16\":\n        np_dtype = np.uint16\n        max_objects = 65535\n    elif dtype == \"uint32\":\n        np_dtype = np.uint32\n        max_objects = 4294967295\n    else:\n        np_dtype = np.uint32\n        max_objects = 4294967295\n\n    # Create output array in memory (for smaller images) or use memory-mapped file\n    # For very large images, you might want to use rasterio windowed writing\n    output_mask = np.zeros((img_height, img_width), dtype=np_dtype)\n\n    # Track unique object IDs across all tiles\n    current_max_id = 0\n    total_objects = 0\n\n    # Process each tile\n    tile_iterator = tqdm(\n        range(total_tiles),\n        desc=\"Processing tiles\",\n        disable=not verbose,\n    )\n\n    for tile_idx in tile_iterator:\n        # Calculate tile position\n        tile_y = tile_idx // n_tiles_x\n        tile_x = tile_idx % n_tiles_x\n\n        # Calculate window coordinates\n        x_start = tile_x * step\n        y_start = tile_y * step\n\n        # Ensure we don't go beyond image bounds\n        x_end = min(x_start + tile_size, img_width)\n        y_end = min(y_start + tile_size, img_height)\n\n        # Adjust start if we're at the edge\n        if x_end - x_start &lt; tile_size and x_start &gt; 0:\n            x_start = max(0, x_end - tile_size)\n        if y_end - y_start &lt; tile_size and y_start &gt; 0:\n            y_start = max(0, y_end - tile_size)\n\n        window_width = x_end - x_start\n        window_height = y_end - y_start\n\n        # Read tile from source\n        with rasterio.open(source) as src:\n            window = Window(x_start, y_start, window_width, window_height)\n            if bands is not None:\n                tile_data = np.stack(\n                    [src.read(b, window=window) for b in bands], axis=0\n                )\n            else:\n                tile_data = src.read(window=window)\n                if tile_data.shape[0] &gt;= 3:\n                    tile_data = tile_data[:3, :, :]\n                elif tile_data.shape[0] == 1:\n                    tile_data = np.repeat(tile_data, 3, axis=0)\n                elif tile_data.shape[0] == 2:\n                    tile_data = np.concatenate(\n                        [tile_data, tile_data[0:1, :, :]], axis=0\n                    )\n\n        # Transpose to (height, width, channels)\n        tile_data = np.transpose(tile_data, (1, 2, 0))\n\n        # Normalize to 8-bit\n        tile_data = tile_data.astype(np.float32)\n        tile_data -= tile_data.min()\n        if tile_data.max() &gt; 0:\n            tile_data /= tile_data.max()\n        tile_data *= 255\n        tile_image = tile_data.astype(np.uint8)\n\n        # Process the tile\n        try:\n            # Set image for the tile\n            self.image = tile_image\n            self.image_height, self.image_width = tile_image.shape[:2]\n            self.source = None  # Don't need georef for individual tiles\n\n            # Initialize inference state for this tile\n            pil_image = Image.fromarray(tile_image)\n            self.pil_image = pil_image\n\n            if self.backend == \"meta\":\n                self.inference_state = self.processor.set_image(pil_image)\n            else:\n                # For transformers backend, process directly\n                pass\n\n            # Generate masks for this tile (quiet=True to avoid per-tile messages)\n            self.generate_masks(\n                prompt, min_size=min_size, max_size=max_size, quiet=True\n            )\n\n            # Get masks for this tile\n            tile_masks = self.masks\n\n            if tile_masks is not None and len(tile_masks) &gt; 0:\n                # Create a mask array for this tile\n                tile_mask_array = np.zeros(\n                    (window_height, window_width), dtype=np_dtype\n                )\n\n                for mask in tile_masks:\n                    # Convert mask to numpy\n                    if hasattr(mask, \"cpu\"):\n                        mask_np = mask.squeeze().cpu().numpy()\n                    elif hasattr(mask, \"numpy\"):\n                        mask_np = mask.squeeze().numpy()\n                    else:\n                        mask_np = (\n                            mask.squeeze() if hasattr(mask, \"squeeze\") else mask\n                        )\n\n                    if mask_np.ndim &gt; 2:\n                        mask_np = mask_np[0]\n\n                    # Resize mask to tile size if needed\n                    if mask_np.shape != (window_height, window_width):\n                        mask_np = cv2.resize(\n                            mask_np.astype(np.float32),\n                            (window_width, window_height),\n                            interpolation=cv2.INTER_NEAREST,\n                        )\n\n                    mask_bool = mask_np &gt; 0\n                    mask_size = np.sum(mask_bool)\n\n                    # Filter by size\n                    if mask_size &lt; min_size:\n                        continue\n                    if max_size is not None and mask_size &gt; max_size:\n                        continue\n\n                    if unique:\n                        current_max_id += 1\n                        if current_max_id &gt; max_objects:\n                            raise ValueError(\n                                f\"Maximum number of objects ({max_objects}) exceeded. \"\n                                \"Consider using a larger dtype or reducing the number of objects.\"\n                            )\n                        tile_mask_array[mask_bool] = current_max_id\n                    else:\n                        tile_mask_array[mask_bool] = 1\n\n                    total_objects += 1\n\n                # Merge tile mask into output mask\n                # For overlapping regions, use the tile's values if they are non-zero\n                # This simple approach works well for most cases\n                self._merge_tile_mask(\n                    output_mask,\n                    tile_mask_array,\n                    x_start,\n                    y_start,\n                    x_end,\n                    y_end,\n                    overlap,\n                    tile_x,\n                    tile_y,\n                    n_tiles_x,\n                    n_tiles_y,\n                )\n\n        except Exception as e:\n            if verbose:\n                print(f\"Warning: Failed to process tile ({tile_x}, {tile_y}): {e}\")\n            continue\n\n        # Clear GPU memory\n        self.masks = None\n        self.boxes = None\n        self.scores = None\n        if hasattr(self, \"inference_state\"):\n            self.inference_state = None\n        # Additionally clear PyTorch CUDA cache, if available, to free GPU memory\n        try:\n            import torch\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except ImportError:\n            # If torch is not installed, skip CUDA cache clearing\n            pass\n    # Update output profile\n    profile.update(\n        {\n            \"count\": 1,\n            \"dtype\": dtype,\n            \"compress\": \"deflate\",\n        }\n    )\n\n    # Save the output\n    with rasterio.open(output, \"w\", **profile) as dst:\n        dst.write(output_mask, 1)\n\n    if verbose:\n        print(f\"Saved mask to {output}\")\n        print(f\"Total objects found: {total_objects}\")\n\n    # Store result for potential visualization\n    self.objects = output_mask\n    self.source = source\n\n    return output\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst","title":"<code>predict_inst(point_coords=None, point_labels=None, box=None, mask_input=None, multimask_output=True, return_logits=False, normalize_coords=True, point_crs=None, box_crs=None)</code>","text":"<p>Predict masks for the given input prompts using SAM3's interactive instance segmentation (SAM1-style task). This enables point and box prompts for precise object segmentation.</p> <p>Note: This method requires the model to be initialized with <code>enable_inst_interactivity=True</code> (Meta backend only).</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>ndarray or List</code> <p>A Nx2 array or list of point prompts to the model. Each point is in (X, Y) pixel coordinates. Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].</p> <code>None</code> <code>point_labels</code> <code>ndarray or List</code> <p>A length N array or list of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>box</code> <code>ndarray or List</code> <p>A length 4 array/list or Bx4 array/list of box prompt(s) to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW (or BxHxW for batched), where H=W=256 for SAM.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If True, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results. Defaults to True.</p> <code>True</code> <code>return_logits</code> <code>bool</code> <p>If True, returns un-thresholded mask logits instead of binary masks. Defaults to False.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>If True, the point coordinates will be normalized to the range [0, 1] and point_coords is expected to be w.r.t. image dimensions. Defaults to True.</p> <code>True</code> <code>point_crs</code> <code>str</code> <p>Coordinate reference system for point coordinates (e.g., \"EPSG:4326\"). Only used if the source image is a GeoTIFF. If None, points are in pixel coordinates.</p> <code>None</code> <code>box_crs</code> <code>str</code> <p>Coordinate reference system for box coordinates (e.g., \"EPSG:4326\"). Only used if the source image is a GeoTIFF. If None, box is in pixel coordinates.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: - masks: The output masks in CxHxW format (or BxCxHxW for batched   box input), where C is the number of masks, and (H, W) is the   original image size. - scores: An array of length C (or BxC) containing the model's   predictions for the quality of each mask. - logits: An array of shape CxHxW (or BxCxHxW), where C is the   number of masks and H=W=256. These low resolution logits can   be passed to a subsequent iteration as mask input.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def predict_inst(\n    self,\n    point_coords: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    point_labels: Optional[Union[np.ndarray, List[int]]] = None,\n    box: Optional[Union[np.ndarray, List[float], List[List[float]]]] = None,\n    mask_input: Optional[np.ndarray] = None,\n    multimask_output: bool = True,\n    return_logits: bool = False,\n    normalize_coords: bool = True,\n    point_crs: Optional[str] = None,\n    box_crs: Optional[str] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Predict masks for the given input prompts using SAM3's interactive instance\n    segmentation (SAM1-style task). This enables point and box prompts for\n    precise object segmentation.\n\n    Note: This method requires the model to be initialized with\n    `enable_inst_interactivity=True` (Meta backend only).\n\n    Args:\n        point_coords (np.ndarray or List, optional): A Nx2 array or list of point\n            prompts to the model. Each point is in (X, Y) pixel coordinates.\n            Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].\n        point_labels (np.ndarray or List, optional): A length N array or list of\n            labels for the point prompts. 1 indicates a foreground point and\n            0 indicates a background point.\n        box (np.ndarray or List, optional): A length 4 array/list or Bx4 array/list\n            of box prompt(s) to the model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the\n            model, typically coming from a previous prediction iteration.\n            Has form 1xHxW (or BxHxW for batched), where H=W=256 for SAM.\n        multimask_output (bool): If True, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will\n            often produce better masks than a single prediction. If only a\n            single mask is needed, the model's predicted quality score can\n            be used to select the best mask. For non-ambiguous prompts, such\n            as multiple input prompts, multimask_output=False can give\n            better results. Defaults to True.\n        return_logits (bool): If True, returns un-thresholded mask logits\n            instead of binary masks. Defaults to False.\n        normalize_coords (bool): If True, the point coordinates will be\n            normalized to the range [0, 1] and point_coords is expected to\n            be w.r.t. image dimensions. Defaults to True.\n        point_crs (str, optional): Coordinate reference system for point\n            coordinates (e.g., \"EPSG:4326\"). Only used if the source image\n            is a GeoTIFF. If None, points are in pixel coordinates.\n        box_crs (str, optional): Coordinate reference system for box\n            coordinates (e.g., \"EPSG:4326\"). Only used if the source image\n            is a GeoTIFF. If None, box is in pixel coordinates.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n            - masks: The output masks in CxHxW format (or BxCxHxW for batched\n              box input), where C is the number of masks, and (H, W) is the\n              original image size.\n            - scores: An array of length C (or BxC) containing the model's\n              predictions for the quality of each mask.\n            - logits: An array of shape CxHxW (or BxCxHxW), where C is the\n              number of masks and H=W=256. These low resolution logits can\n              be passed to a subsequent iteration as mask input.\n\n    Example:\n        &gt;&gt;&gt; # Initialize with instance interactivity enabled\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Single point prompt\n        &gt;&gt;&gt; point_coords = np.array([[520, 375]])\n        &gt;&gt;&gt; point_labels = np.array([1])\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n        ...     point_coords=point_coords,\n        ...     point_labels=point_labels,\n        ...     multimask_output=True,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Select best mask based on score\n        &gt;&gt;&gt; best_mask_idx = np.argmax(scores)\n        &gt;&gt;&gt; best_mask = masks[best_mask_idx]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Refine with additional points\n        &gt;&gt;&gt; point_coords = np.array([[500, 375], [1125, 625]])\n        &gt;&gt;&gt; point_labels = np.array([1, 0])  # foreground and background\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n        ...     point_coords=point_coords,\n        ...     point_labels=point_labels,\n        ...     mask_input=logits[best_mask_idx:best_mask_idx+1],  # Use previous best\n        ...     multimask_output=False,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Box prompt\n        &gt;&gt;&gt; box = np.array([425, 600, 700, 875])\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(box=box, multimask_output=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Combined box and point prompt\n        &gt;&gt;&gt; box = np.array([425, 600, 700, 875])\n        &gt;&gt;&gt; point_coords = np.array([[575, 750]])\n        &gt;&gt;&gt; point_labels = np.array([0])  # Exclude this region\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n        ...     point_coords=point_coords,\n        ...     point_labels=point_labels,\n        ...     box=box,\n        ...     multimask_output=False,\n        ... )\n    \"\"\"\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"predict_inst is only available for the Meta backend. \"\n            \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n        )\n\n    if self.inference_state is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if (\n        not hasattr(self.model, \"inst_interactive_predictor\")\n        or self.model.inst_interactive_predictor is None\n    ):\n        raise ValueError(\n            \"Instance interactivity not enabled. Please initialize with \"\n            \"enable_inst_interactivity=True.\"\n        )\n\n    # Convert lists to numpy arrays\n    if point_coords is not None and not isinstance(point_coords, np.ndarray):\n        point_coords = np.array(point_coords)\n    if point_labels is not None and not isinstance(point_labels, np.ndarray):\n        point_labels = np.array(point_labels)\n    if box is not None and not isinstance(box, np.ndarray):\n        box = np.array(box)\n\n    # Transform point coordinates from CRS to pixel coordinates if needed\n    if (\n        point_coords is not None\n        and point_crs is not None\n        and self.source is not None\n    ):\n        point_coords = np.array(point_coords)\n        point_coords, _ = common.coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    # Transform box coordinates from CRS to pixel coordinates if needed\n    if box is not None and box_crs is not None and self.source is not None:\n        box = np.array(box)\n        if box.ndim == 1:\n            # Single box [xmin, ymin, xmax, ymax]\n            xmin, ymin, xmax, ymax = box\n            min_coords = np.array([[xmin, ymin]])\n            max_coords = np.array([[xmax, ymax]])\n            min_xy, _ = common.coords_to_xy(\n                self.source, min_coords, box_crs, return_out_of_bounds=True\n            )\n            max_xy, _ = common.coords_to_xy(\n                self.source, max_coords, box_crs, return_out_of_bounds=True\n            )\n            x1_px, y1_px = min_xy[0]\n            x2_px, y2_px = max_xy[0]\n            box = np.array(\n                [\n                    min(x1_px, x2_px),\n                    min(y1_px, y2_px),\n                    max(x1_px, x2_px),\n                    max(y1_px, y2_px),\n                ]\n            )\n        else:\n            # Multiple boxes [B, 4]\n            transformed_boxes = []\n            for b in box:\n                xmin, ymin, xmax, ymax = b\n                min_coords = np.array([[xmin, ymin]])\n                max_coords = np.array([[xmax, ymax]])\n                min_xy, _ = common.coords_to_xy(\n                    self.source, min_coords, box_crs, return_out_of_bounds=True\n                )\n                max_xy, _ = common.coords_to_xy(\n                    self.source, max_coords, box_crs, return_out_of_bounds=True\n                )\n                x1_px, y1_px = min_xy[0]\n                x2_px, y2_px = max_xy[0]\n                transformed_boxes.append(\n                    [\n                        min(x1_px, x2_px),\n                        min(y1_px, y2_px),\n                        max(x1_px, x2_px),\n                        max(y1_px, y2_px),\n                    ]\n                )\n            box = np.array(transformed_boxes)\n\n    # Call the model's predict_inst method\n    masks, scores, logits = self.model.predict_inst(\n        self.inference_state,\n        point_coords=point_coords,\n        point_labels=point_labels,\n        box=box,\n        mask_input=mask_input,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    # Store results\n    self.masks = (\n        [masks[i] for i in range(len(masks))] if masks.ndim &gt; 2 else [masks]\n    )\n    self.scores = list(scores) if isinstance(scores, np.ndarray) else [scores]\n    self.logits = logits\n\n    return masks, scores, logits\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--initialize-with-instance-interactivity-enabled","title":"Initialize with instance interactivity enabled","text":"<p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"image.jpg\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--single-point-prompt","title":"Single point prompt","text":"<p>point_coords = np.array([[520, 375]]) point_labels = np.array([1]) masks, scores, logits = sam.predict_inst( ...     point_coords=point_coords, ...     point_labels=point_labels, ...     multimask_output=True, ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--select-best-mask-based-on-score","title":"Select best mask based on score","text":"<p>best_mask_idx = np.argmax(scores) best_mask = masks[best_mask_idx]</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--refine-with-additional-points","title":"Refine with additional points","text":"<p>point_coords = np.array([[500, 375], [1125, 625]]) point_labels = np.array([1, 0])  # foreground and background masks, scores, logits = sam.predict_inst( ...     point_coords=point_coords, ...     point_labels=point_labels, ...     mask_input=logits[best_mask_idx:best_mask_idx+1],  # Use previous best ...     multimask_output=False, ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--box-prompt","title":"Box prompt","text":"<p>box = np.array([425, 600, 700, 875]) masks, scores, logits = sam.predict_inst(box=box, multimask_output=False)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst--combined-box-and-point-prompt","title":"Combined box and point prompt","text":"<p>box = np.array([425, 600, 700, 875]) point_coords = np.array([[575, 750]]) point_labels = np.array([0])  # Exclude this region masks, scores, logits = sam.predict_inst( ...     point_coords=point_coords, ...     point_labels=point_labels, ...     box=box, ...     multimask_output=False, ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst_batch","title":"<code>predict_inst_batch(point_coords_batch=None, point_labels_batch=None, box_batch=None, mask_input_batch=None, multimask_output=True, return_logits=False, normalize_coords=True)</code>","text":"<p>Predict masks for batched input prompts using SAM3's interactive instance segmentation. This is used when the model is set with multiple images via <code>set_image_batch()</code>.</p> <p>Note: This method requires the model to be initialized with <code>enable_inst_interactivity=True</code> (Meta backend only).</p> <p>Parameters:</p> Name Type Description Default <code>point_coords_batch</code> <code>List[ndarray]</code> <p>List of Nx2 arrays of point prompts for each image in the batch. Each point is in (X, Y) pixel coordinates.</p> <code>None</code> <code>point_labels_batch</code> <code>List[ndarray]</code> <p>List of length N arrays of labels for the point prompts for each image. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>box_batch</code> <code>List[ndarray]</code> <p>List of Bx4 arrays of box prompts for each image in the batch, in XYXY format.</p> <code>None</code> <code>mask_input_batch</code> <code>List[ndarray]</code> <p>List of low resolution mask inputs for each image, typically from previous iterations.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If True, the model will return three masks per prompt. Defaults to True.</p> <code>True</code> <code>return_logits</code> <code>bool</code> <p>If True, returns un-thresholded mask logits instead of binary masks. Defaults to False.</p> <code>False</code> <code>normalize_coords</code> <code>bool</code> <p>If True, coordinates are normalized to [0, 1]. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[List[ndarray], List[ndarray], List[ndarray]]</code> <p>Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]: - masks_batch: List of mask arrays for each image - scores_batch: List of score arrays for each image - logits_batch: List of logit arrays for each image</p> Example <p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def predict_inst_batch(\n    self,\n    point_coords_batch: Optional[List[np.ndarray]] = None,\n    point_labels_batch: Optional[List[np.ndarray]] = None,\n    box_batch: Optional[List[np.ndarray]] = None,\n    mask_input_batch: Optional[List[np.ndarray]] = None,\n    multimask_output: bool = True,\n    return_logits: bool = False,\n    normalize_coords: bool = True,\n) -&gt; Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"\n    Predict masks for batched input prompts using SAM3's interactive instance\n    segmentation. This is used when the model is set with multiple images via\n    `set_image_batch()`.\n\n    Note: This method requires the model to be initialized with\n    `enable_inst_interactivity=True` (Meta backend only).\n\n    Args:\n        point_coords_batch (List[np.ndarray], optional): List of Nx2 arrays of\n            point prompts for each image in the batch. Each point is in (X, Y)\n            pixel coordinates.\n        point_labels_batch (List[np.ndarray], optional): List of length N arrays\n            of labels for the point prompts for each image. 1 indicates a\n            foreground point and 0 indicates a background point.\n        box_batch (List[np.ndarray], optional): List of Bx4 arrays of box\n            prompts for each image in the batch, in XYXY format.\n        mask_input_batch (List[np.ndarray], optional): List of low resolution\n            mask inputs for each image, typically from previous iterations.\n        multimask_output (bool): If True, the model will return three masks\n            per prompt. Defaults to True.\n        return_logits (bool): If True, returns un-thresholded mask logits\n            instead of binary masks. Defaults to False.\n        normalize_coords (bool): If True, coordinates are normalized to [0, 1].\n            Defaults to True.\n\n    Returns:\n        Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n            - masks_batch: List of mask arrays for each image\n            - scores_batch: List of score arrays for each image\n            - logits_batch: List of logit arrays for each image\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load batch of images\n        &gt;&gt;&gt; image1 = Image.open(\"truck.jpg\")\n        &gt;&gt;&gt; image2 = Image.open(\"groceries.jpg\")\n        &gt;&gt;&gt; sam.set_image_batch([image1, image2])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define box prompts for each image\n        &gt;&gt;&gt; image1_boxes = np.array([\n        ...     [75, 275, 1725, 850],\n        ...     [425, 600, 700, 875],\n        ... ])\n        &gt;&gt;&gt; image2_boxes = np.array([\n        ...     [450, 170, 520, 350],\n        ...     [350, 190, 450, 350],\n        ... ])\n        &gt;&gt;&gt; boxes_batch = [image1_boxes, image2_boxes]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Predict masks for all images\n        &gt;&gt;&gt; masks_batch, scores_batch, logits_batch = sam.predict_inst_batch(\n        ...     box_batch=boxes_batch,\n        ...     multimask_output=False,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Or use point prompts\n        &gt;&gt;&gt; image1_pts = np.array([[[500, 375]], [[650, 750]]])  # Bx1x2\n        &gt;&gt;&gt; image1_labels = np.array([[1], [1]])\n        &gt;&gt;&gt; image2_pts = np.array([[[400, 300]], [[630, 300]]])\n        &gt;&gt;&gt; image2_labels = np.array([[1], [1]])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; masks_batch, scores_batch, logits_batch = sam.predict_inst_batch(\n        ...     point_coords_batch=[image1_pts, image2_pts],\n        ...     point_labels_batch=[image1_labels, image2_labels],\n        ...     multimask_output=True,\n        ... )\n    \"\"\"\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"predict_inst_batch is only available for the Meta backend. \"\n            \"Please initialize with backend='meta' and enable_inst_interactivity=True.\"\n        )\n\n    if self.batch_state is None:\n        raise ValueError(\n            \"No images set for batch processing. \"\n            \"Please call set_image_batch() first.\"\n        )\n\n    if (\n        not hasattr(self.model, \"inst_interactive_predictor\")\n        or self.model.inst_interactive_predictor is None\n    ):\n        raise ValueError(\n            \"Instance interactivity not enabled. Please initialize with \"\n            \"enable_inst_interactivity=True.\"\n        )\n\n    # Call the model's predict_inst_batch method\n    masks_batch, scores_batch, logits_batch = self.model.predict_inst_batch(\n        self.batch_state,\n        point_coords_batch,\n        point_labels_batch,\n        box_batch=box_batch,\n        mask_input_batch=mask_input_batch,\n        multimask_output=multimask_output,\n        return_logits=return_logits,\n        normalize_coords=normalize_coords,\n    )\n\n    return masks_batch, scores_batch, logits_batch\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst_batch--load-batch-of-images","title":"Load batch of images","text":"<p>image1 = Image.open(\"truck.jpg\") image2 = Image.open(\"groceries.jpg\") sam.set_image_batch([image1, image2])</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst_batch--define-box-prompts-for-each-image","title":"Define box prompts for each image","text":"<p>image1_boxes = np.array([ ...     [75, 275, 1725, 850], ...     [425, 600, 700, 875], ... ]) image2_boxes = np.array([ ...     [450, 170, 520, 350], ...     [350, 190, 450, 350], ... ]) boxes_batch = [image1_boxes, image2_boxes]</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst_batch--predict-masks-for-all-images","title":"Predict masks for all images","text":"<p>masks_batch, scores_batch, logits_batch = sam.predict_inst_batch( ...     box_batch=boxes_batch, ...     multimask_output=False, ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.predict_inst_batch--or-use-point-prompts","title":"Or use point prompts","text":"<p>image1_pts = np.array([[[500, 375]], [[650, 750]]])  # Bx1x2 image1_labels = np.array([[1], [1]]) image2_pts = np.array([[[400, 300]], [[630, 300]]]) image2_labels = np.array([[1], [1]])</p> <p>masks_batch, scores_batch, logits_batch = sam.predict_inst_batch( ...     point_coords_batch=[image1_pts, image2_pts], ...     point_labels_batch=[image1_labels, image2_labels], ...     multimask_output=True, ... )</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.raster_to_vector","title":"<code>raster_to_vector(raster, vector, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a raster image file to a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>The path to the raster image.</p> required <code>vector</code> <code>str</code> <p>The path to the output vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement.</p> <code>None</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def raster_to_vector(\n    self,\n    raster: str,\n    vector: str,\n    simplify_tolerance: Optional[float] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Convert a raster image file to a vector dataset.\n\n    Args:\n        raster (str): The path to the raster image.\n        vector (str): The path to the output vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n    \"\"\"\n    common.raster_to_vector(\n        raster, vector, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.save_masks","title":"<code>save_masks(output=None, unique=True, min_size=0, max_size=None, dtype='uint8', save_scores=None, **kwargs)</code>","text":"<p>Save the generated masks to a file or generate mask array for visualization.</p> <p>If the input image is a GeoTIFF, the output will be saved as a GeoTIFF with the same georeferencing information. Otherwise, it will be saved as PNG.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output file. If None, only generates the mask array in memory (self.objects) without saving to disk.</p> <code>None</code> <code>unique</code> <code>bool</code> <p>If True, each mask gets a unique value (1, 2, 3, ...). If False, all masks are combined into a binary mask (0 or 255).</p> <code>True</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels. Masks smaller than this will be filtered out.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels. Masks larger than this will be filtered out.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type for the output array.</p> <code>'uint8'</code> <code>save_scores</code> <code>str</code> <p>If provided, saves a confidence score map to this path. Each pixel will have the confidence score of its mask. The output format (GeoTIFF or PNG) follows the same logic as the mask output.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to common.array_to_image().</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def save_masks(\n    self,\n    output: Optional[str] = None,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    dtype: str = \"uint8\",\n    save_scores: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the generated masks to a file or generate mask array for visualization.\n\n    If the input image is a GeoTIFF, the output will be saved as a GeoTIFF\n    with the same georeferencing information. Otherwise, it will be saved as PNG.\n\n    Args:\n        output (str, optional): The path to the output file. If None, only generates\n            the mask array in memory (self.objects) without saving to disk.\n        unique (bool): If True, each mask gets a unique value (1, 2, 3, ...).\n            If False, all masks are combined into a binary mask (0 or 255).\n        min_size (int): Minimum mask size in pixels. Masks smaller than this\n            will be filtered out.\n        max_size (int, optional): Maximum mask size in pixels. Masks larger than\n            this will be filtered out.\n        dtype (str): Data type for the output array.\n        save_scores (str, optional): If provided, saves a confidence score map\n            to this path. Each pixel will have the confidence score of its mask.\n            The output format (GeoTIFF or PNG) follows the same logic as the mask output.\n        **kwargs: Additional keyword arguments passed to common.array_to_image().\n    \"\"\"\n    if self.masks is None or len(self.masks) == 0:\n        raise ValueError(\"No masks found. Please run generate_masks() first.\")\n\n    if save_scores is not None and self.scores is None:\n        raise ValueError(\"No scores found. Cannot save scores.\")\n\n    # Create empty array for combined masks\n    mask_array = np.zeros(\n        (self.image_height, self.image_width),\n        dtype=np.uint32 if unique else np.uint8,\n    )\n\n    # Create empty array for scores if requested\n    if save_scores is not None:\n        scores_array = np.zeros(\n            (self.image_height, self.image_width), dtype=np.float32\n        )\n\n    # Process each mask\n    valid_mask_count = 0\n    mask_index = 0\n    for mask in self.masks:\n        # Convert mask to numpy array if it's a tensor\n        if hasattr(mask, \"cpu\"):\n            mask_np = mask.squeeze().cpu().numpy()\n        elif hasattr(mask, \"numpy\"):\n            mask_np = mask.squeeze().numpy()\n        else:\n            mask_np = mask.squeeze() if hasattr(mask, \"squeeze\") else mask\n\n        # Ensure mask is 2D\n        if mask_np.ndim &gt; 2:\n            mask_np = mask_np[0]\n\n        # Convert to boolean\n        mask_bool = mask_np &gt; 0\n\n        # Calculate mask size\n        mask_size = np.sum(mask_bool)\n\n        # Filter by size\n        if mask_size &lt; min_size:\n            mask_index += 1\n            continue\n        if max_size is not None and mask_size &gt; max_size:\n            mask_index += 1\n            continue\n\n        # Get confidence score for this mask\n        if save_scores is not None:\n            if hasattr(self.scores[mask_index], \"item\"):\n                score = self.scores[mask_index].item()\n            else:\n                score = float(self.scores[mask_index])\n\n        # Add mask to array\n        if unique:\n            # Assign unique value to each mask (starting from 1)\n            mask_value = valid_mask_count + 1\n            mask_array[mask_bool] = mask_value\n        else:\n            # Binary mask: all foreground pixels are 255\n            mask_array[mask_bool] = 255\n\n        # Add score to scores array\n        if save_scores is not None:\n            scores_array[mask_bool] = score\n\n        valid_mask_count += 1\n        mask_index += 1\n\n    if valid_mask_count == 0:\n        print(\"No masks met the size criteria.\")\n        return\n\n    # Convert to requested dtype\n    if dtype == \"uint8\":\n        if unique and valid_mask_count &gt; 255:\n            print(\n                f\"Warning: {valid_mask_count} masks found, but uint8 can only represent 255 unique values. Consider using dtype='uint16'.\"\n            )\n        mask_array = mask_array.astype(np.uint8)\n    elif dtype == \"uint16\":\n        mask_array = mask_array.astype(np.uint16)\n    elif dtype == \"int32\":\n        mask_array = mask_array.astype(np.int32)\n    else:\n        mask_array = mask_array.astype(dtype)\n\n    # Store the mask array for visualization\n    self.objects = mask_array\n\n    # Only save to file if output path is provided\n    if output is not None:\n        # Save using common utility which handles GeoTIFF georeferencing\n        common.array_to_image(\n            mask_array, output, self.source, dtype=dtype, **kwargs\n        )\n        print(f\"Saved {valid_mask_count} mask(s) to {output}\")\n\n        # Save scores if requested\n        if save_scores is not None:\n            common.array_to_image(\n                scores_array, save_scores, self.source, dtype=\"float32\", **kwargs\n            )\n            print(f\"Saved confidence scores to {save_scores}\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.save_masks_batch","title":"<code>save_masks_batch(output_dir, prefix='mask', unique=True, min_size=0, max_size=None, dtype='uint8', **kwargs)</code>","text":"<p>Save masks from batch processing to files.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save the mask files.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output filenames. Files will be named \"{prefix}{index}.tif\" or \"{prefix}.png\".</p> <code>'mask'</code> <code>unique</code> <code>bool</code> <p>If True, each mask gets a unique value (1, 2, 3, ...). If False, all masks are combined into a binary mask.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>Minimum mask size in pixels.</p> <code>0</code> <code>max_size</code> <code>int</code> <p>Maximum mask size in pixels.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type for the output array.</p> <code>'uint8'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to common.array_to_image().</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to saved mask files.</p> Example <p>sam = SamGeo3(backend=\"meta\") sam.set_image_batch([\"img1.tif\", \"img2.tif\"]) sam.generate_masks_batch(\"building\") saved_files = sam.save_masks_batch(\"output/\", prefix=\"building_mask\")</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def save_masks_batch(\n    self,\n    output_dir: str,\n    prefix: str = \"mask\",\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    dtype: str = \"uint8\",\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Save masks from batch processing to files.\n\n    Args:\n        output_dir (str): Directory to save the mask files.\n        prefix (str): Prefix for output filenames. Files will be named\n            \"{prefix}_{index}.tif\" or \"{prefix}_{index}.png\".\n        unique (bool): If True, each mask gets a unique value (1, 2, 3, ...).\n            If False, all masks are combined into a binary mask.\n        min_size (int): Minimum mask size in pixels.\n        max_size (int, optional): Maximum mask size in pixels.\n        dtype (str): Data type for the output array.\n        **kwargs: Additional arguments passed to common.array_to_image().\n\n    Returns:\n        List[str]: List of paths to saved mask files.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n        &gt;&gt;&gt; sam.set_image_batch([\"img1.tif\", \"img2.tif\"])\n        &gt;&gt;&gt; sam.generate_masks_batch(\"building\")\n        &gt;&gt;&gt; saved_files = sam.save_masks_batch(\"output/\", prefix=\"building_mask\")\n    \"\"\"\n    if self.batch_results is None:\n        raise ValueError(\n            \"No batch results found. Please run generate_masks_batch() first.\"\n        )\n\n    os.makedirs(output_dir, exist_ok=True)\n    saved_files = []\n\n    for i, result in enumerate(self.batch_results):\n        masks = result.get(\"masks\", [])\n        source = result.get(\"source\")\n        image = result.get(\"image\")\n\n        if not masks:\n            print(f\"No masks for image {i + 1}, skipping.\")\n            continue\n\n        # Get image dimensions\n        if image is not None:\n            height, width = image.shape[:2]\n        else:\n            # Try to get from first mask\n            mask = masks[0]\n            if hasattr(mask, \"shape\"):\n                if mask.ndim &gt; 2:\n                    height, width = mask.shape[-2:]\n                else:\n                    height, width = mask.shape\n            else:\n                raise ValueError(f\"Cannot determine dimensions for image {i}\")\n\n        # Create combined mask array\n        mask_array = np.zeros(\n            (height, width), dtype=np.uint32 if unique else np.uint8\n        )\n\n        valid_mask_count = 0\n        for j, mask in enumerate(masks):\n            # Convert to numpy\n            if hasattr(mask, \"cpu\"):\n                mask_np = mask.squeeze().cpu().numpy()\n            elif hasattr(mask, \"numpy\"):\n                mask_np = mask.squeeze().numpy()\n            else:\n                mask_np = np.squeeze(mask) if hasattr(mask, \"squeeze\") else mask\n\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np[0]\n\n            mask_bool = mask_np &gt; 0\n            mask_size = np.sum(mask_bool)\n\n            if mask_size &lt; min_size:\n                continue\n            if max_size is not None and mask_size &gt; max_size:\n                continue\n\n            if unique:\n                mask_array[mask_bool] = valid_mask_count + 1\n            else:\n                mask_array[mask_bool] = 255\n\n            valid_mask_count += 1\n\n        if valid_mask_count == 0:\n            print(f\"No valid masks for image {i + 1} after filtering.\")\n            continue\n\n        # Convert dtype\n        if unique and valid_mask_count &gt; np.iinfo(np.dtype(dtype)).max:\n            print(\n                f\"Warning: {valid_mask_count} masks exceed {dtype} range. Consider using uint16 or uint32.\"\n            )\n        mask_array = mask_array.astype(dtype)\n\n        # Determine output path and extension\n        if source is not None and source.lower().endswith((\".tif\", \".tiff\")):\n            ext = \".tif\"\n        else:\n            ext = \".png\"\n\n        output_path = os.path.join(output_dir, f\"{prefix}_{i + 1}{ext}\")\n\n        # Save\n        common.array_to_image(\n            mask_array, output_path, source, dtype=dtype, **kwargs\n        )\n        saved_files.append(output_path)\n        print(\n            f\"Saved {valid_mask_count} mask(s) for image {i + 1} to {output_path}\"\n        )\n\n    return saved_files\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype='float32', vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask.</p> <code>255</code> <code>dtype</code> <code>str</code> <p>The data type of the output image.</p> <code>'float32'</code> <code>vector</code> <code>Optional[str]</code> <p>The path to the output vector file.</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>The maximum allowed geometry displacement.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def save_prediction(\n    self,\n    output: str,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: str = \"float32\",\n    vector: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (Optional[int]): The index of the mask to save.\n        mask_multiplier (int): The mask multiplier for the output mask.\n        dtype (str): The data type of the output image.\n        vector (Optional[str]): The path to the output vector file.\n        simplify_tolerance (Optional[float]): The maximum allowed geometry displacement.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    common.array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        common.raster_to_vector(\n            output, vector, simplify_tolerance=simplify_tolerance\n        )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.set_confidence_threshold","title":"<code>set_confidence_threshold(threshold, state=None)</code>","text":"<p>Sets the confidence threshold for the masks. Args:     threshold (float): The confidence threshold.     state (optional): An optional state object to pass to the processor's set_confidence_threshold method (Meta backend only).</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def set_confidence_threshold(self, threshold: float, state=None):\n    \"\"\"Sets the confidence threshold for the masks.\n    Args:\n        threshold (float): The confidence threshold.\n        state (optional): An optional state object to pass to the processor's set_confidence_threshold method (Meta backend only).\n    \"\"\"\n    self.confidence_threshold = threshold\n    if self.backend == \"meta\":\n        self.inference_state = self.processor.set_confidence_threshold(\n            threshold, state\n        )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.set_image","title":"<code>set_image(image, state=None, bands=None)</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray, Image]</code> <p>The input image as a path, a numpy array, or an Image.</p> required <code>state</code> <code>optional</code> <p>An optional state object to pass to the processor's set_image method (Meta backend only).</p> <code>None</code> <code>bands</code> <code>List[int]</code> <p>List of band indices (1-based) to use for RGB when the input is a GeoTIFF with more than 3 bands. For example, [4, 3, 2] for NIR-R-G false color composite. If None, uses the first 3 bands for multi-band images. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def set_image(\n    self,\n    image: Union[str, np.ndarray],\n    state=None,\n    bands: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (Union[str, np.ndarray, Image]): The input image as a path,\n            a numpy array, or an Image.\n        state (optional): An optional state object to pass to the processor's set_image method (Meta backend only).\n        bands (List[int], optional): List of band indices (1-based) to use for RGB\n            when the input is a GeoTIFF with more than 3 bands. For example,\n            [4, 3, 2] for NIR-R-G false color composite. If None, uses the\n            first 3 bands for multi-band images. Defaults to None.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = common.download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        # Check if image is a GeoTIFF and handle band selection\n        if image.lower().endswith((\".tif\", \".tiff\")):\n            import rasterio\n\n            with rasterio.open(image) as src:\n                if bands is not None:\n                    # Validate band indices (1-based)\n                    if len(bands) != 3:\n                        raise ValueError(\n                            \"bands must contain exactly 3 band indices for RGB.\"\n                        )\n                    for band in bands:\n                        if band &lt; 1 or band &gt; src.count:\n                            raise ValueError(\n                                f\"Band index {band} is out of range. \"\n                                f\"Image has {src.count} bands (1-indexed).\"\n                            )\n                    # Read specified bands (rasterio uses 1-based indexing)\n                    array = np.stack([src.read(b) for b in bands], axis=0)\n                else:\n                    # Read all bands\n                    array = src.read()\n                    # If more than 3 bands, use first 3\n                    if array.shape[0] &gt;= 3:\n                        array = array[:3, :, :]\n                    elif array.shape[0] == 1:\n                        array = np.repeat(array, 3, axis=0)\n                    elif array.shape[0] == 2:\n                        # Repeat the first band to make 3 bands: [band1, band2, band1]\n                        array = np.concatenate([array, array[0:1, :, :]], axis=0)\n                # Transpose from (bands, height, width) to (height, width, bands)\n                array = np.transpose(array, (1, 2, 0))\n\n                # Normalize to 8-bit (0-255) range\n                array = array.astype(np.float32)\n                array -= array.min()\n                if array.max() &gt; 0:\n                    array /= array.max()\n                array *= 255\n                image = array.astype(np.uint8)\n        else:\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        self.image = image\n        self.source = None\n    elif isinstance(image, Image.Image):\n        self.image = np.array(image)\n        self.source = None\n    else:\n        raise ValueError(\n            \"Input image must be either a path, numpy array, or PIL Image.\"\n        )\n\n    self.image_height, self.image_width = self.image.shape[:2]\n\n    # Convert to PIL Image for processing\n    image_for_processor = Image.fromarray(self.image)\n\n    # Set image based on backend\n    if self.backend == \"meta\":\n        # SAM3's processor expects PIL Image or tensor with (C, H, W) format\n        # Numpy arrays from cv2 have (H, W, C) format which causes incorrect dimension extraction\n        self.inference_state = self.processor.set_image(\n            image_for_processor, state=state\n        )\n    else:  # transformers\n        # For Transformers backend, we just store the PIL image\n        # Processing will happen during generate_masks\n        self.pil_image = image_for_processor\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.set_image_batch","title":"<code>set_image_batch(images, state=None)</code>","text":"<p>Set multiple images for batch processing.</p> <p>Note: This method is only available for the Meta backend.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Union[str, ndarray, Image]]</code> <p>A list of input images. Each image can be a file path, a numpy array, or a PIL Image.</p> required <code>state</code> <code>dict</code> <p>An optional state object to pass to the processor's set_image_batch method.</p> <code>None</code> Example <p>sam = SamGeo3(backend=\"meta\") sam.set_image_batch([\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]) results = sam.generate_masks_batch(\"tree\")</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def set_image_batch(\n    self,\n    images: List[Union[str, np.ndarray, Image.Image]],\n    state: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Set multiple images for batch processing.\n\n    Note: This method is only available for the Meta backend.\n\n    Args:\n        images (List[Union[str, np.ndarray, Image]]): A list of input images.\n            Each image can be a file path, a numpy array, or a PIL Image.\n        state (dict, optional): An optional state object to pass to the\n            processor's set_image_batch method.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\")\n        &gt;&gt;&gt; sam.set_image_batch([\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"])\n        &gt;&gt;&gt; results = sam.generate_masks_batch(\"tree\")\n    \"\"\"\n    if self.backend != \"meta\":\n        raise NotImplementedError(\n            \"Batch image processing is only available for the Meta backend. \"\n            \"Use set_image() for the Transformers backend.\"\n        )\n\n    if not isinstance(images, list) or len(images) == 0:\n        raise ValueError(\"images must be a non-empty list\")\n\n    # Process each image to PIL format\n    pil_images = []\n    sources = []\n    numpy_images = []\n\n    for image in images:\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = common.download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            sources.append(image)\n            img = cv2.imread(image)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            numpy_images.append(img)\n            pil_images.append(Image.fromarray(img))\n\n        elif isinstance(image, np.ndarray):\n            sources.append(None)\n            numpy_images.append(image)\n            pil_images.append(Image.fromarray(image))\n\n        elif isinstance(image, Image.Image):\n            sources.append(None)\n            numpy_images.append(np.array(image))\n            pil_images.append(image)\n\n        else:\n            raise ValueError(\n                \"Each image must be either a path, numpy array, or PIL Image.\"\n            )\n\n    # Store batch information\n    self.images_batch = numpy_images\n    self.sources_batch = sources\n\n    # Call the processor's set_image_batch method\n    self.batch_state = self.processor.set_image_batch(pil_images, state=state)\n\n    print(f\"Set {len(pil_images)} images for batch processing.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', show_bbox=True, show_score=True, output=None, blend=True, alpha=0.5, font_scale=0.8, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>This method uses OpenCV for fast rendering, which is significantly faster than matplotlib when there are many objects to plot.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size (used for display).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis.</p> <code>'off'</code> <code>show_bbox</code> <code>bool</code> <p>Whether to show the bounding box.</p> <code>True</code> <code>show_score</code> <code>bool</code> <p>Whether to show the score.</p> <code>True</code> <code>output</code> <code>str</code> <p>The path to the output image. If provided, the figure will be saved instead of displayed.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image as background. If False, only annotations will be shown on a white background.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations.</p> <code>0.5</code> <code>font_scale</code> <code>float</code> <p>The font scale for labels. Defaults to 0.8.</p> <code>0.8</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (kept for backward compatibility).</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_anns(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    show_bbox: bool = True,\n    show_score: bool = True,\n    output: Optional[str] = None,\n    blend: bool = True,\n    alpha: float = 0.5,\n    font_scale: float = 0.8,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    This method uses OpenCV for fast rendering, which is significantly faster\n    than matplotlib when there are many objects to plot.\n\n    Args:\n        figsize (tuple): The figure size (used for display).\n        axis (str): Whether to show the axis.\n        show_bbox (bool): Whether to show the bounding box.\n        show_score (bool): Whether to show the score.\n        output (str, optional): The path to the output image. If provided, the\n            figure will be saved instead of displayed.\n        blend (bool): Whether to show the input image as background. If False,\n            only annotations will be shown on a white background.\n        alpha (float): The alpha value for the annotations.\n        font_scale (float): The font scale for labels. Defaults to 0.8.\n        **kwargs: Additional keyword arguments (kept for backward compatibility).\n    \"\"\"\n\n    if self.image is None:\n        print(\"Please run set_image() first.\")\n        return\n\n    if self.masks is None or len(self.masks) == 0:\n        return\n\n    # Create the blended image using OpenCV (much faster than matplotlib)\n    blended = self._render_anns_opencv(\n        show_bbox=show_bbox,\n        show_score=show_score,\n        blend=blend,\n        alpha=alpha,\n        font_scale=font_scale,\n    )\n\n    if output is not None:\n        # Save directly using OpenCV\n        cv2.imwrite(output, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n        print(f\"Saved annotations to {output}\")\n    else:\n        # Display the image\n        self._display_image(blended, figsize=figsize, axis=axis)\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_anns_batch","title":"<code>show_anns_batch(figsize=(12, 10), axis='off', show_bbox=True, show_score=True, output_dir=None, prefix='anns', blend=True, alpha=0.5, ncols=2, **kwargs)</code>","text":"<p>Show annotations for all images in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>Figure size for each subplot.</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show axis.</p> <code>'off'</code> <code>show_bbox</code> <code>bool</code> <p>Whether to show bounding boxes.</p> <code>True</code> <code>show_score</code> <code>bool</code> <p>Whether to show confidence scores.</p> <code>True</code> <code>output_dir</code> <code>str</code> <p>Directory to save annotation images. If None, displays the figure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for output filenames.</p> <code>'anns'</code> <code>blend</code> <code>bool</code> <p>Whether to show image as background.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Alpha value for mask overlay.</p> <code>0.5</code> <code>ncols</code> <code>int</code> <p>Number of columns in the grid display.</p> <code>2</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for saving.</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_anns_batch(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    show_bbox: bool = True,\n    show_score: bool = True,\n    output_dir: Optional[str] = None,\n    prefix: str = \"anns\",\n    blend: bool = True,\n    alpha: float = 0.5,\n    ncols: int = 2,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show annotations for all images in the batch.\n\n    Args:\n        figsize (tuple): Figure size for each subplot.\n        axis (str): Whether to show axis.\n        show_bbox (bool): Whether to show bounding boxes.\n        show_score (bool): Whether to show confidence scores.\n        output_dir (str, optional): Directory to save annotation images.\n            If None, displays the figure.\n        prefix (str): Prefix for output filenames.\n        blend (bool): Whether to show image as background.\n        alpha (float): Alpha value for mask overlay.\n        ncols (int): Number of columns in the grid display.\n        **kwargs: Additional arguments for saving.\n    \"\"\"\n    if self.batch_results is None:\n        raise ValueError(\n            \"No batch results found. Please run generate_masks_batch() first.\"\n        )\n\n    num_images = len(self.batch_results)\n\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n        # Save each image separately\n        for i, result in enumerate(self.batch_results):\n            self._show_single_ann(\n                result,\n                figsize=figsize,\n                axis=axis,\n                show_bbox=show_bbox,\n                show_score=show_score,\n                blend=blend,\n                alpha=alpha,\n                output=os.path.join(output_dir, f\"{prefix}_{i + 1}.png\"),\n                **kwargs,\n            )\n    else:\n        # Display in grid\n        nrows = (num_images + ncols - 1) // ncols\n        fig, axes = plt.subplots(\n            nrows, ncols, figsize=(figsize[0] * ncols, figsize[1] * nrows)\n        )\n\n        if num_images == 1 or ncols == 1 or nrows == 1:\n            axes = np.array([axes]).flatten()\n        else:\n            axes = axes.flatten()\n        for i, result in enumerate(self.batch_results):\n            ax = axes[i]\n            self._show_single_ann(\n                result,\n                figsize=figsize,\n                axis=axis,\n                show_bbox=show_bbox,\n                show_score=show_score,\n                blend=blend,\n                alpha=alpha,\n                ax=ax,\n            )\n            ax.set_title(f\"Image {i + 1}\")\n\n        # Hide unused subplots\n        for j in range(num_images, len(axes)):\n            axes[j].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_boxes","title":"<code>show_boxes(boxes, box_labels=None, box_crs=None, figsize=(12, 10), axis='off', positive_color=(0, 255, 0), negative_color=(255, 0, 0), thickness=3)</code>","text":"<p>Visualize bounding boxes on the image.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>List[List[float]]</code> <p>List of bounding boxes in XYXY format [[xmin, ymin, xmax, ymax], ...]. If box_crs is None: pixel coordinates. If box_crs is specified: coordinates in the given CRS.</p> required <code>box_labels</code> <code>List[bool]</code> <p>List of boolean labels for each box. True (positive) shown in green, False (negative) shown in red. If None, all boxes shown in green.</p> <code>None</code> <code>box_crs</code> <code>str</code> <p>Coordinate reference system for box coordinates (e.g., \"EPSG:4326\"). If None, boxes are in pixel coordinates.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for display.</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show axis (\"on\" or \"off\").</p> <code>'off'</code> <code>positive_color</code> <code>Tuple[int, int, int]</code> <p>RGB color for positive boxes.</p> <code>(0, 255, 0)</code> <code>negative_color</code> <code>Tuple[int, int, int]</code> <p>RGB color for negative boxes.</p> <code>(255, 0, 0)</code> <code>thickness</code> <code>int</code> <p>Line thickness for box borders.</p> <code>3</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_boxes(\n    self,\n    boxes: List[List[float]],\n    box_labels: Optional[List[bool]] = None,\n    box_crs: Optional[str] = None,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    positive_color: Tuple[int, int, int] = (0, 255, 0),\n    negative_color: Tuple[int, int, int] = (255, 0, 0),\n    thickness: int = 3,\n) -&gt; None:\n    \"\"\"\n    Visualize bounding boxes on the image.\n\n    Args:\n        boxes (List[List[float]]): List of bounding boxes in XYXY format\n            [[xmin, ymin, xmax, ymax], ...].\n            If box_crs is None: pixel coordinates.\n            If box_crs is specified: coordinates in the given CRS.\n        box_labels (List[bool], optional): List of boolean labels for each box.\n            True (positive) shown in green, False (negative) shown in red.\n            If None, all boxes shown in green.\n        box_crs (str, optional): Coordinate reference system for box coordinates\n            (e.g., \"EPSG:4326\"). If None, boxes are in pixel coordinates.\n        figsize (Tuple[int, int]): Figure size for display.\n        axis (str): Whether to show axis (\"on\" or \"off\").\n        positive_color (Tuple[int, int, int]): RGB color for positive boxes.\n        negative_color (Tuple[int, int, int]): RGB color for negative boxes.\n        thickness (int): Line thickness for box borders.\n    \"\"\"\n    if self.image is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if box_labels is None:\n        box_labels = [True] * len(boxes)\n\n    # Transform boxes from CRS to pixel coordinates if needed\n    if box_crs is not None and self.source is not None:\n        pixel_boxes = []\n        for box in boxes:\n            xmin, ymin, xmax, ymax = box\n\n            # Transform min corner\n            min_coords = np.array([[xmin, ymin]])\n            min_xy, _ = common.coords_to_xy(\n                self.source, min_coords, box_crs, return_out_of_bounds=True\n            )\n\n            # Transform max corner\n            max_coords = np.array([[xmax, ymax]])\n            max_xy, _ = common.coords_to_xy(\n                self.source, max_coords, box_crs, return_out_of_bounds=True\n            )\n\n            # Convert to pixel coordinates and ensure correct min/max order\n            # (geographic y increases north, pixel y increases down)\n            x1_px = min_xy[0][0]\n            y1_px = min_xy[0][1]\n            x2_px = max_xy[0][0]\n            y2_px = max_xy[0][1]\n\n            # Ensure we have correct min/max values\n            x_min_px = min(x1_px, x2_px)\n            y_min_px = min(y1_px, y2_px)\n            x_max_px = max(x1_px, x2_px)\n            y_max_px = max(y1_px, y2_px)\n\n            pixel_boxes.append([x_min_px, y_min_px, x_max_px, y_max_px])\n\n        boxes = pixel_boxes\n\n    # Convert image to PIL if needed\n    if isinstance(self.image, np.ndarray):\n        img = Image.fromarray(self.image)\n    else:\n        img = self.image\n\n    # Draw each box\n    for box, label in zip(boxes, box_labels):\n        # Convert XYXY to XYWH for drawing\n        xmin, ymin, xmax, ymax = box\n        box_xywh = [xmin, ymin, xmax - xmin, ymax - ymin]\n\n        # Choose color based on label\n        color = positive_color if label else negative_color\n\n        # Draw box\n        img = draw_box_on_image(img, box_xywh, color=color, thickness=thickness)\n\n    # Display\n    plt.figure(figsize=figsize)\n    plt.imshow(img)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_canvas","title":"<code>show_canvas(fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>fg_color</code> <code>Tuple[int, int, int]</code> <p>The color for foreground points.</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>Tuple[int, int, int]</code> <p>The color for background points.</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[list, list]</code> <p>Tuple of foreground and background points.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_canvas(\n    self,\n    fg_color: Tuple[int, int, int] = (0, 255, 0),\n    bg_color: Tuple[int, int, int] = (0, 0, 255),\n    radius: int = 5,\n) -&gt; Tuple[list, list]:\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        fg_color (Tuple[int, int, int]): The color for foreground points.\n        bg_color (Tuple[int, int, int]): The color for background points.\n        radius (int): The radius of the points.\n\n    Returns:\n        Tuple of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = common.show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n\n    return fg_points, bg_points\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_inst_masks","title":"<code>show_inst_masks(masks, scores, point_coords=None, point_labels=None, box_coords=None, figsize=(10, 10), borders=True)</code>","text":"<p>Display masks from predict_inst results with optional point and box overlays.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>Masks from predict_inst, shape CxHxW.</p> required <code>scores</code> <code>ndarray</code> <p>Scores from predict_inst, shape C.</p> required <code>point_coords</code> <code>ndarray or List</code> <p>Point coordinates used for prompts. Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].</p> <code>None</code> <code>point_labels</code> <code>ndarray or List</code> <p>Point labels (1=foreground, 0=background).</p> <code>None</code> <code>box_coords</code> <code>ndarray or List</code> <p>Box coordinates used for prompt. Can be a numpy array or a Python list like [x1, y1, x2, y2].</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for each mask display.</p> <code>(10, 10)</code> <code>borders</code> <code>bool</code> <p>Whether to draw contour borders on masks.</p> <code>True</code> Example <p>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) sam.set_image(\"image.jpg\") masks, scores, logits = sam.predict_inst( ...     point_coords=[[520, 375]], ...     point_labels=[1], ... ) sam.show_inst_masks( ...     masks, scores, ...     point_coords=[[520, 375]], ...     point_labels=[1], ... )</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_inst_masks(\n    self,\n    masks: np.ndarray,\n    scores: np.ndarray,\n    point_coords: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    point_labels: Optional[Union[np.ndarray, List[int]]] = None,\n    box_coords: Optional[Union[np.ndarray, List[float]]] = None,\n    figsize: Tuple[int, int] = (10, 10),\n    borders: bool = True,\n) -&gt; None:\n    \"\"\"\n    Display masks from predict_inst results with optional point and box overlays.\n\n    Args:\n        masks (np.ndarray): Masks from predict_inst, shape CxHxW.\n        scores (np.ndarray): Scores from predict_inst, shape C.\n        point_coords (np.ndarray or List, optional): Point coordinates used for prompts.\n            Can be a numpy array or a Python list like [[x1, y1], [x2, y2]].\n        point_labels (np.ndarray or List, optional): Point labels (1=foreground, 0=background).\n        box_coords (np.ndarray or List, optional): Box coordinates used for prompt.\n            Can be a numpy array or a Python list like [x1, y1, x2, y2].\n        figsize (Tuple[int, int]): Figure size for each mask display.\n        borders (bool): Whether to draw contour borders on masks.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n        &gt;&gt;&gt; sam.set_image(\"image.jpg\")\n        &gt;&gt;&gt; masks, scores, logits = sam.predict_inst(\n        ...     point_coords=[[520, 375]],\n        ...     point_labels=[1],\n        ... )\n        &gt;&gt;&gt; sam.show_inst_masks(\n        ...     masks, scores,\n        ...     point_coords=[[520, 375]],\n        ...     point_labels=[1],\n        ... )\n    \"\"\"\n    if self.image is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    # Convert lists to numpy arrays\n    if point_coords is not None and not isinstance(point_coords, np.ndarray):\n        point_coords = np.array(point_coords)\n    if point_labels is not None and not isinstance(point_labels, np.ndarray):\n        point_labels = np.array(point_labels)\n    if box_coords is not None and not isinstance(box_coords, np.ndarray):\n        box_coords = np.array(box_coords)\n\n    # Sort by score (descending)\n    sorted_ind = np.argsort(scores)[::-1]\n    masks = masks[sorted_ind]\n    scores = scores[sorted_ind]\n\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        fig = plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        # Show mask\n        h, w = mask.shape[-2:]\n        mask_uint8 = mask.astype(np.uint8)\n        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n        mask_image = mask_uint8.reshape(h, w, 1) * color.reshape(1, 1, -1)\n\n        if borders:\n            contours, _ = cv2.findContours(\n                mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n            )\n            contours = [\n                cv2.approxPolyDP(contour, epsilon=0.01, closed=True)\n                for contour in contours\n            ]\n            mask_image = cv2.drawContours(\n                mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2\n            )\n\n        plt.gca().imshow(mask_image)\n\n        # Show points if provided\n        if point_coords is not None and point_labels is not None:\n            pos_points = point_coords[point_labels == 1]\n            neg_points = point_coords[point_labels == 0]\n            plt.scatter(\n                pos_points[:, 0],\n                pos_points[:, 1],\n                color=\"green\",\n                marker=\"*\",\n                s=375,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n            plt.scatter(\n                neg_points[:, 0],\n                neg_points[:, 1],\n                color=\"red\",\n                marker=\"*\",\n                s=375,\n                edgecolor=\"white\",\n                linewidth=1.25,\n            )\n\n        # Show box if provided\n        if box_coords is not None:\n            x0, y0 = box_coords[0], box_coords[1]\n            box_w, box_h = (\n                box_coords[2] - box_coords[0],\n                box_coords[3] - box_coords[1],\n            )\n            plt.gca().add_patch(\n                plt.Rectangle(\n                    (x0, y0),\n                    box_w,\n                    box_h,\n                    edgecolor=\"green\",\n                    facecolor=(0, 0, 0, 0),\n                    lw=2,\n                )\n            )\n\n        if len(scores) &gt; 1:\n            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n\n        plt.axis(\"off\")\n        plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_map","title":"<code>show_map(basemap='Esri.WorldImagery', out_dir=None, min_size=10, max_size=None, prompt='text', **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. Valid options include \"Esri.WorldImagery\", \"OpenStreetMap\", \"HYBRID\", \"ROADMAP\", \"TERRAIN\", etc. See the leafmap documentation for a full list of supported basemaps.</p> <code>'Esri.WorldImagery'</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>The minimum size of the object. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>int</code> <p>The maximum size of the object. Defaults to None.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The prompt type. Defaults to \"text\". Valid options include \"text\" and \"point\".</p> <code>'text'</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_map(\n    self,\n    basemap=\"Esri.WorldImagery\",\n    out_dir=None,\n    min_size=10,\n    max_size=None,\n    prompt=\"text\",\n    **kwargs,\n):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. Valid options include \"Esri.WorldImagery\", \"OpenStreetMap\", \"HYBRID\", \"ROADMAP\", \"TERRAIN\", etc. See the leafmap documentation for a full list of supported basemaps.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n        min_size (int, optional): The minimum size of the object. Defaults to 10.\n        max_size (int, optional): The maximum size of the object. Defaults to None.\n        prompt (str, optional): The prompt type. Defaults to \"text\".\n            Valid options include \"text\" and \"point\".\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    if prompt.lower() == \"text\":\n        return common.text_sam_gui(\n            self,\n            basemap=basemap,\n            out_dir=out_dir,\n            box_threshold=self.confidence_threshold,\n            text_threshold=self.mask_threshold,\n            min_size=min_size,\n            max_size=max_size,\n            **kwargs,\n        )\n    elif prompt.lower() == \"point\":\n        return common.sam_map_gui(\n            self,\n            basemap=basemap,\n            out_dir=out_dir,\n            min_size=min_size,\n            max_size=max_size,\n            **kwargs,\n        )\n    else:\n        raise ValueError(f\"Invalid prompt: {prompt}. Please use 'text' or 'point'.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='tab20', axis='off', unique=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size.</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Default is \"tab20\" for showing unique objects. Use \"binary_r\" for binary masks when unique=False. Other good options: \"viridis\", \"nipy_spectral\", \"rainbow\".</p> <code>'tab20'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis.</p> <code>'off'</code> <code>unique</code> <code>bool</code> <p>If True, each mask gets a unique color value. If False, binary mask.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_masks() for filtering (e.g., min_size, max_size, dtype).</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_masks(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    cmap: str = \"tab20\",\n    axis: str = \"off\",\n    unique: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple): The figure size.\n        cmap (str): The colormap. Default is \"tab20\" for showing unique objects.\n            Use \"binary_r\" for binary masks when unique=False.\n            Other good options: \"viridis\", \"nipy_spectral\", \"rainbow\".\n        axis (str): Whether to show the axis.\n        unique (bool): If True, each mask gets a unique color value. If False, binary mask.\n        **kwargs: Additional keyword arguments passed to save_masks() for filtering\n            (e.g., min_size, max_size, dtype).\n    \"\"\"\n\n    # Always regenerate mask array to ensure it matches the unique parameter\n    # This prevents showing stale cached binary masks when unique=True is requested\n    self.save_masks(output=None, unique=unique, **kwargs)\n\n    if self.objects is None:\n        # save_masks would have printed a message if no masks met criteria\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap, interpolation=\"nearest\")\n    plt.axis(axis)\n\n    plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3.show_points","title":"<code>show_points(point_coords, point_labels=None, point_crs=None, figsize=(12, 10), axis='off', foreground_color='green', background_color='red', marker='*', marker_size=375)</code>","text":"<p>Visualize point prompts on the image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>List[List[float]]</code> <p>List of point coordinates [[x, y], ...]. If point_crs is None: pixel coordinates. If point_crs is specified: coordinates in the given CRS.</p> required <code>point_labels</code> <code>List[int]</code> <p>List of labels for each point. 1 = foreground (shown in green), 0 = background (shown in red). If None, all points shown as foreground.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>Coordinate reference system for point coordinates (e.g., \"EPSG:4326\"). If None, points are in pixel coordinates.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for display.</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show axis (\"on\" or \"off\").</p> <code>'off'</code> <code>foreground_color</code> <code>str</code> <p>Color for foreground points (label=1).</p> <code>'green'</code> <code>background_color</code> <code>str</code> <p>Color for background points (label=0).</p> <code>'red'</code> <code>marker</code> <code>str</code> <p>Marker style for points.</p> <code>'*'</code> <code>marker_size</code> <code>int</code> <p>Size of the markers.</p> <code>375</code> Example <p>sam.show_points([[520, 375]], [1])  # Single foreground point sam.show_points([[500, 375], [600, 400]], [1, 0])  # Mixed points</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_points(\n    self,\n    point_coords: List[List[float]],\n    point_labels: Optional[List[int]] = None,\n    point_crs: Optional[str] = None,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    foreground_color: str = \"green\",\n    background_color: str = \"red\",\n    marker: str = \"*\",\n    marker_size: int = 375,\n) -&gt; None:\n    \"\"\"\n    Visualize point prompts on the image.\n\n    Args:\n        point_coords (List[List[float]]): List of point coordinates [[x, y], ...].\n            If point_crs is None: pixel coordinates.\n            If point_crs is specified: coordinates in the given CRS.\n        point_labels (List[int], optional): List of labels for each point.\n            1 = foreground (shown in green), 0 = background (shown in red).\n            If None, all points shown as foreground.\n        point_crs (str, optional): Coordinate reference system for point coordinates\n            (e.g., \"EPSG:4326\"). If None, points are in pixel coordinates.\n        figsize (Tuple[int, int]): Figure size for display.\n        axis (str): Whether to show axis (\"on\" or \"off\").\n        foreground_color (str): Color for foreground points (label=1).\n        background_color (str): Color for background points (label=0).\n        marker (str): Marker style for points.\n        marker_size (int): Size of the markers.\n\n    Example:\n        sam.show_points([[520, 375]], [1])  # Single foreground point\n        sam.show_points([[500, 375], [600, 400]], [1, 0])  # Mixed points\n    \"\"\"\n    if self.image is None:\n        raise ValueError(\"No image set. Please call set_image() first.\")\n\n    if point_labels is None:\n        point_labels = [1] * len(point_coords)\n\n    # Convert to numpy arrays\n    point_coords = np.array(point_coords)\n    point_labels = np.array(point_labels)\n\n    # Transform points from CRS to pixel coordinates if needed\n    if point_crs is not None and self.source is not None:\n        point_coords, _ = common.coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    # Display image\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    # Plot foreground points\n    fg_mask = point_labels == 1\n    if np.any(fg_mask):\n        plt.scatter(\n            point_coords[fg_mask, 0],\n            point_coords[fg_mask, 1],\n            color=foreground_color,\n            marker=marker,\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n\n    # Plot background points\n    bg_mask = point_labels == 0\n    if np.any(bg_mask):\n        plt.scatter(\n            point_coords[bg_mask, 0],\n            point_coords[bg_mask, 1],\n            color=background_color,\n            marker=marker,\n            s=marker_size,\n            edgecolor=\"white\",\n            linewidth=1.25,\n        )\n\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video","title":"<code>SamGeo3Video</code>","text":"<p>Video segmentation and tracking with SAM3 for geospatial data.</p> <p>This class provides a simplified API for segmenting and tracking objects in videos or time series remote sensing images using SAM3.</p> Example <p>from samgeo.samgeo3 import SamGeo3Video sam = SamGeo3Video() sam.set_video(\"path/to/video.mp4\")  # or path to image sequence sam.generate_masks(\"person\")  # text prompt sam.save_masks(\"output/\") sam.save_video(\"output.mp4\") sam.close()</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>class SamGeo3Video:\n    \"\"\"Video segmentation and tracking with SAM3 for geospatial data.\n\n    This class provides a simplified API for segmenting and tracking objects\n    in videos or time series remote sensing images using SAM3.\n\n    Example:\n        &gt;&gt;&gt; from samgeo.samgeo3 import SamGeo3Video\n        &gt;&gt;&gt; sam = SamGeo3Video()\n        &gt;&gt;&gt; sam.set_video(\"path/to/video.mp4\")  # or path to image sequence\n        &gt;&gt;&gt; sam.generate_masks(\"person\")  # text prompt\n        &gt;&gt;&gt; sam.save_masks(\"output/\")\n        &gt;&gt;&gt; sam.save_video(\"output.mp4\")\n        &gt;&gt;&gt; sam.close()\n    \"\"\"\n\n    def __init__(\n        self,\n        gpus_to_use: Optional[List[int]] = None,\n        bpe_path: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the SamGeo3Video class.\n\n        Args:\n            gpus_to_use (List[int], optional): List of GPU indices to use.\n                If None, uses all available GPUs. Defaults to None.\n            bpe_path (str, optional): Path to the BPE tokenizer vocabulary.\n                If None, uses the default path. Defaults to None.\n            **kwargs: Additional keyword arguments passed to build_sam3_video_predictor.\n        \"\"\"\n        if not SAM3_META_AVAILABLE:\n            error_msg = (\n                \"SAM3 is not available. Please install it as:\\n\\t\"\n                \"pip install segment-geospatial[samgeo3]\"\n            )\n            if SAM3_META_IMPORT_ERROR is not None:\n                error_msg += f\"\\n\\nUnderlying import error:\\n\\t{SAM3_META_IMPORT_ERROR}\"\n            raise ImportError(error_msg)\n\n        import torch\n\n        # Set up GPU configuration\n        if gpus_to_use is None:\n            gpus_to_use = list(range(torch.cuda.device_count()))\n            if len(gpus_to_use) == 0:\n                gpus_to_use = [torch.cuda.current_device()]\n\n        # Set up BPE path\n        if bpe_path is None:\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            bpe_path = os.path.abspath(\n                os.path.join(current_dir, \"assets\", \"bpe_simple_vocab_16e6.txt.gz\")\n            )\n            if not os.path.exists(bpe_path):\n                bpe_dir = os.path.dirname(bpe_path)\n                os.makedirs(bpe_dir, exist_ok=True)\n                url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/bpe_simple_vocab_16e6.txt.gz\"\n                bpe_path = common.download_file(url, bpe_path, quiet=True)\n\n        print(f\"Using GPUs: {gpus_to_use}\")\n\n        self.predictor = build_sam3_video_predictor(\n            gpus_to_use=gpus_to_use, bpe_path=bpe_path, **kwargs\n        )\n        self.gpus_to_use = gpus_to_use\n        self.session_id = None\n        self.video_path = None\n        self.video_frames = None\n        self.outputs_per_frame = None\n        self.frame_width = None\n        self.frame_height = None\n        self._tif_source = None\n        self._tif_dir = None\n        self._tif_names = None\n\n    def set_video(\n        self,\n        video_path: str,\n        output_dir: Optional[str] = None,\n        frame_rate: Optional[int] = None,\n        prefix: str = \"\",\n        bands: Optional[List[int]] = None,\n    ) -&gt; None:\n        \"\"\"Load a video or time series images for segmentation.\n\n        The video can be:\n        - An MP4 video file\n        - A directory of JPEG frames\n        - A directory of GeoTIFF images (for time series remote sensing data)\n\n        Args:\n            video_path (str): Path to the video file or image directory.\n            output_dir (str, optional): Directory to save extracted frames.\n                Only used when video_path is an MP4 file. Defaults to None.\n            frame_rate (int, optional): Frame rate for extracting frames from video.\n                Only used when video_path is an MP4 file. Defaults to None.\n            prefix (str): Prefix for extracted frame filenames. Defaults to \"\".\n            bands (List[int], optional): List of band indices (1-based) to use for RGB\n                when the input is a GeoTIFF directory with multi-band images. For example,\n                [4, 3, 2] for NIR-R-G false color composite. If None, uses the\n                first 3 bands for multi-band images. Defaults to None.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3Video()\n            &gt;&gt;&gt; sam.set_video(\"video.mp4\")  # Load MP4 video\n            &gt;&gt;&gt; sam.set_video(\"frames/\")  # Load from JPEG frames directory\n            &gt;&gt;&gt; sam.set_video(\"landsat_ts/\")  # Load GeoTIFF time series\n            &gt;&gt;&gt; sam.set_video(\"landsat_ts/\", bands=[4, 3, 2])  # NIR-R-G composite\n        \"\"\"\n        if isinstance(video_path, str):\n            if video_path.startswith(\"http\"):\n                video_path = common.download_file(video_path)\n\n            if os.path.isfile(video_path):\n                # MP4 video file - extract frames\n                if output_dir is None:\n                    output_dir = common.make_temp_dir()\n                if not os.path.exists(output_dir):\n                    os.makedirs(output_dir)\n                print(f\"Extracting frames to: {output_dir}\")\n                common.video_to_images(\n                    video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n                )\n                video_path = output_dir\n\n            elif os.path.isdir(video_path):\n                files = sorted(os.listdir(video_path))\n                if len(files) == 0:\n                    raise ValueError(f\"No files found in {video_path}.\")\n\n                # Check if it's a GeoTIFF directory\n                if files[0].lower().endswith((\".tif\", \".tiff\")):\n                    self._tif_source = os.path.join(video_path, files[0])\n                    self._tif_dir = video_path\n                    self._tif_names = files\n                    # Convert GeoTIFFs to JPEGs for SAM3\n                    video_path = common.geotiff_to_jpg_batch(video_path, bands=bands)\n                    print(f\"Converted GeoTIFFs to JPEGs: {video_path}\")\n\n            if not os.path.exists(video_path):\n                raise ValueError(f\"Input path {video_path} does not exist.\")\n        else:\n            raise ValueError(\"video_path must be a string.\")\n\n        self.video_path = video_path\n\n        # Load frames for visualization\n        self._load_video_frames(video_path)\n\n        # Start a session\n        response = self.predictor.handle_request(\n            request=dict(\n                type=\"start_session\",\n                resource_path=video_path,\n            )\n        )\n        self.session_id = response[\"session_id\"]\n        print(f\"Loaded {len(self.video_frames)} frames. Session started.\")\n\n    def show_video(self, video_path: str, embed: bool = True, **kwargs: Any) -&gt; None:\n        \"\"\"Show the video.\n\n        Args:\n            video_path (str): Path to video file.\n            embed (bool, optional): Whether to embed the video. Defaults to True.\n            **kwargs: Additional keyword arguments passed to Video.\n\n        Returns:\n            IPython.display.Video: The video object.\n        \"\"\"\n        from IPython.display import Video\n\n        return Video(video_path, embed=embed, **kwargs)\n\n    def _load_video_frames(self, video_path: str) -&gt; None:\n        \"\"\"Load video frames for visualization.\n\n        Args:\n            video_path (str): Path to video file or frame directory.\n        \"\"\"\n        if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n            cap = cv2.VideoCapture(video_path)\n            self.video_frames = []\n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                self.video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            cap.release()\n            if self.video_frames:\n                self.frame_height, self.frame_width = self.video_frames[0].shape[:2]\n            else:\n                raise ValueError(f\"Failed to load any frames from video: {video_path}\")\n        else:\n            self.video_frames = glob.glob(os.path.join(video_path, \"*.jpg\"))\n            try:\n                self.video_frames.sort(\n                    key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n                )\n            except ValueError:\n                self.video_frames.sort()\n\n            if self.video_frames:\n                first_frame = load_frame(self.video_frames[0])\n                self.frame_height, self.frame_width = first_frame.shape[:2]\n            else:\n                raise ValueError(f\"No JPEG frames found in directory: {video_path}\")\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the current session, clearing all prompts and masks.\n\n        Use this when you want to start fresh with new prompts on the same video.\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        self.predictor.handle_request(\n            request=dict(\n                type=\"reset_session\",\n                session_id=self.session_id,\n            )\n        )\n        self.outputs_per_frame = None\n        print(\"Session reset.\")\n\n    def init_tracker(self, frame_idx: int = 0) -&gt; None:\n        \"\"\"Initialize the tracker without detecting any objects.\n\n        This method initializes SAM3's video tracking state, which is required\n        before using point prompts (add_point_prompts), box prompts (add_box_prompt),\n        or mask prompts (add_mask_prompt).\n\n        Call this method when you want to:\n        - Use only point or box prompts without text-based detection\n        - Add external masks from other segmentation models\n        - Have full control over which objects to track\n\n        Args:\n            frame_idx (int): Frame index to initialize the tracker on. Defaults to 0.\n\n        Example:\n            &gt;&gt;&gt; sam = SamGeo3Video()\n            &gt;&gt;&gt; sam.set_video(\"video.mp4\")\n            &gt;&gt;&gt; sam.init_tracker()  # Initialize without detecting anything\n            &gt;&gt;&gt; sam.add_point_prompts([[500, 300]], [1], obj_id=1)  # Add object via point\n            &gt;&gt;&gt; sam.add_box_prompt([100, 100, 200, 150], obj_id=2)  # Add object via box\n            &gt;&gt;&gt; sam.propagate()  # Track through video\n            &gt;&gt;&gt; sam.save_video(\"output.mp4\")\n\n        Note:\n            - This is equivalent to calling generate_masks() with a prompt that\n              matches no objects, but provides a cleaner API.\n            - The tracker must be initialized before adding point/box/mask prompts.\n            - After init_tracker(), you can add objects using:\n              - add_point_prompts() for point-based segmentation\n              - add_box_prompt() for box-based segmentation\n              - add_mask_prompt() for external mask tracking\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        # Reset prompts before initializing\n        self.reset()\n\n        # Use a nonsense text prompt that won't match any real objects\n        # This initializes the tracker state without detecting anything\n        _init_prompt = \"__samgeo_init_tracker_placeholder_xyzzy_12345__\"\n\n        response = self.predictor.handle_request(\n            request=dict(\n                type=\"add_prompt\",\n                session_id=self.session_id,\n                frame_index=frame_idx,\n                text=_init_prompt,\n            )\n        )\n\n        # Run propagation to fully initialize the tracker state\n        # This populates cached_frame_outputs which is required for point prompts\n        self.propagate()\n\n        print(\"Tracker initialized. You can now add point, box, or mask prompts.\")\n\n    def generate_masks(\n        self,\n        prompt: str,\n        frame_idx: int = 0,\n        propagate: bool = True,\n    ) -&gt; Dict[int, Any]:\n        \"\"\"Generate masks using a text prompt.\n\n        This will segment all instances of the described object in the video\n        and optionally track them through all frames.\n\n        Args:\n            prompt (str): Text description of objects to segment (e.g., \"person\", \"car\").\n            frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n            propagate (bool): Whether to propagate masks to all frames. Defaults to True.\n\n        Returns:\n            Dict[int, Any]: Dictionary mapping frame index to mask outputs.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"building\")\n            &gt;&gt;&gt; sam.generate_masks(\"tree\", frame_idx=10)\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        # Reset prompts before adding new text prompt\n        self.reset()\n\n        # Add text prompt\n        response = self.predictor.handle_request(\n            request=dict(\n                type=\"add_prompt\",\n                session_id=self.session_id,\n                frame_index=frame_idx,\n                text=prompt,\n            )\n        )\n\n        out = response[\"outputs\"]\n        # Get object IDs - key is 'out_obj_ids' from SAM3 video predictor\n        obj_ids = out.get(\"out_obj_ids\", [])\n        if hasattr(obj_ids, \"tolist\"):\n            obj_ids = obj_ids.tolist()\n        num_objects = len(obj_ids)\n        print(\n            f\"Found {num_objects} object(s) matching '{prompt}' on frame {frame_idx}.\"\n        )\n\n        if propagate:\n            self.propagate()\n\n    def add_point_prompts(\n        self,\n        points: List[List[float]],\n        labels: List[int],\n        obj_id: int,\n        frame_idx: int = 0,\n        point_crs: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Add point prompts to segment or refine an object.\n\n        Args:\n            points (List[List[float]]): List of [x, y] point coordinates.\n                In pixel coordinates by default, or in the specified CRS.\n            labels (List[int]): List of labels for each point.\n                1 for positive (include), 0 for negative (exclude).\n            obj_id (int): Object ID to associate with this prompt.\n            frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n            point_crs (str, optional): Coordinate reference system for points\n                (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.\n\n        Returns:\n            Dict[str, Any]: Response containing the mask output.\n\n        Example:\n            &gt;&gt;&gt; # Add positive point\n            &gt;&gt;&gt; sam.add_point_prompts([[500, 300]], [1], obj_id=1)\n            &gt;&gt;&gt; # Add positive and negative points\n            &gt;&gt;&gt; sam.add_point_prompts([[500, 300], [600, 400]], [1, 0], obj_id=1)\n        \"\"\"\n        import torch\n\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        points = np.array(points)\n\n        # Transform coordinates if CRS is provided\n        if point_crs is not None and self._tif_source is not None:\n            points = common.coords_to_xy(self._tif_source, points, point_crs)\n\n        # Convert to relative coordinates (0-1 range)\n        rel_points = [[x / self.frame_width, y / self.frame_height] for x, y in points]\n\n        points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n        labels_tensor = torch.tensor(labels, dtype=torch.int32)\n\n        response = self.predictor.handle_request(\n            request=dict(\n                type=\"add_prompt\",\n                session_id=self.session_id,\n                frame_index=frame_idx,\n                points=points_tensor,\n                point_labels=labels_tensor,\n                obj_id=obj_id,\n            )\n        )\n\n        return response\n\n    def add_box_prompt(\n        self,\n        box: List[float],\n        obj_id: int,\n        frame_idx: int = 0,\n        box_crs: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Add a bounding box prompt to segment an object.\n\n        Args:\n            box (List[float]): Bounding box in [x, y, width, height] format.\n                In pixel coordinates by default, or in the specified CRS.\n            obj_id (int): Object ID to associate with this prompt.\n            frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n            box_crs (str, optional): Coordinate reference system for box\n                (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.\n\n        Returns:\n            Dict[str, Any]: Response containing the mask output.\n\n        Example:\n            &gt;&gt;&gt; sam.add_box_prompt([100, 100, 200, 150], obj_id=1)\n        \"\"\"\n        import torch\n\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        x, y, w, h = box\n\n        # Transform coordinates if CRS is provided\n        if box_crs is not None and self._tif_source is not None:\n            # Convert box corners to pixel coordinates\n            corners = np.array([[x, y], [x + w, y + h]])\n            corners = common.coords_to_xy(self._tif_source, corners, box_crs)\n            x, y = corners[0]\n            x2, y2 = corners[1]\n            w = x2 - x\n            h = y2 - y\n\n        # Convert to relative coordinates [cx, cy, w, h]\n        cx = (x + w / 2) / self.frame_width\n        cy = (y + h / 2) / self.frame_height\n        rel_w = w / self.frame_width\n        rel_h = h / self.frame_height\n\n        box_tensor = torch.tensor([cx, cy, rel_w, rel_h], dtype=torch.float32)\n\n        response = self.predictor.handle_request(\n            request=dict(\n                type=\"add_prompt\",\n                session_id=self.session_id,\n                frame_index=frame_idx,\n                box=box_tensor,\n                obj_id=obj_id,\n            )\n        )\n\n        return response\n\n    def add_mask_prompt(\n        self,\n        mask: np.ndarray,\n        obj_id: int,\n        frame_idx: int = 0,\n        num_points: int = 5,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Add a mask prompt from an external segmentation model.\n\n        This allows using masks generated by other segmentation models\n        (e.g., YOLO, Detectron2, GroundingDINO, etc.) and leveraging SAM3's\n        tracking capability to propagate them through the video.\n\n        This is useful when SAM3's text prompts don't segment the exact objects\n        you need. You can:\n        1. Use another model to get accurate segmentation on the first frame\n        2. Pass those masks to SAM3 using this method\n        3. Use SAM3's tracking to propagate the masks through the video\n\n        The method works by sampling positive points from the mask region and\n        using SAM3's point prompt capability to segment and track the object.\n\n        **Important**: The tracker must be initialized first by calling\n        `init_tracker()` (recommended) or `generate_masks()` (alternative) with any text prompt before using `add_mask_prompt()`.\n        This is a requirement of SAM3's video tracking architecture.\n\n        Args:\n            mask (np.ndarray): Binary mask array of shape (H, W) where\n                True/1 indicates the object region. Can be bool or numeric.\n                The mask should match the video frame dimensions.\n            obj_id (int): Object ID to associate with this mask. Use different\n                IDs for different objects you want to track.\n            frame_idx (int): Frame index to add the mask on. Defaults to 0.\n            num_points (int): Number of points to sample from the mask region.\n                More points can improve accuracy. Defaults to 5.\n\n        Returns:\n            Dict[str, Any]: Response containing the mask output with keys:\n                - \"frame_index\": The frame index where mask was added\n                - \"outputs\": Dict with \"out_obj_ids\" and mask data\n\n        Example:\n            &gt;&gt;&gt; # Workflow: Use external model + SAM3 tracking\n            &gt;&gt;&gt; from your_segmentation_model import segment_objects\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Step 1: Get segmentation from another model on first frame\n            &gt;&gt;&gt; first_frame = load_image(\"frames/0.jpg\")\n            &gt;&gt;&gt; external_masks = segment_objects(first_frame)  # Your model\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Step 2: Initialize SAM3 video tracker\n            &gt;&gt;&gt; sam = SamGeo3Video()\n            &gt;&gt;&gt; sam.set_video(\"frames/\")  # Load video frames\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Step 3: Initialize tracker for external masks\n            &gt;&gt;&gt; sam.init_tracker()  # Initialize tracker for external masks\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Step 4: Add external masks for tracking (use new obj_ids)\n            &gt;&gt;&gt; for i, mask in enumerate(external_masks):\n            ...     sam.add_mask_prompt(mask, obj_id=100+i, frame_idx=0)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Step 5: Propagate to track through video\n            &gt;&gt;&gt; sam.propagate()\n            &gt;&gt;&gt; sam.save_video(\"tracked_output.mp4\")\n\n        Note:\n            - The tracker MUST be initialized first by calling either `init_tracker()` or `generate_masks()`\n            - Each call to add_mask_prompt adds one object for tracking\n            - Use different obj_id values for different objects\n            - Use obj_ids different from those detected by generate_masks()\n            - The mask should be binary (0/False for background, 1/True for object)\n            - For best results, ensure mask dimensions match the video frame size\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        # Check if tracker has been initialized (has propagated at least once)\n        session = self.predictor._ALL_INFERENCE_STATES.get(self.session_id, None)\n        if session is None:\n            raise RuntimeError(\n                f\"Cannot find session {self.session_id}; it might have expired\"\n            )\n        inference_state = session[\"state\"]\n\n        # Check if tracker is initialized by looking for cached outputs\n        has_cached_outputs = bool(inference_state.get(\"cached_frame_outputs\", {}))\n        tracker_states = inference_state.get(\"tracker_inference_states\", [])\n\n        if not has_cached_outputs and len(tracker_states) == 0:\n            raise RuntimeError(\n                \"Tracker not initialized. Please call init_tracker() or \"\n                \"generate_masks() first to initialize the tracker before adding \"\n                \"mask prompts. Example: sam.init_tracker() or sam.generate_masks('object')\"\n            )\n\n        # Convert mask to numpy array and ensure 2D\n        mask_np = np.array(mask)\n        if mask_np.ndim &gt; 2:\n            mask_np = mask_np.squeeze()\n\n        # Convert to binary\n        mask_np = (mask_np &gt; 0).astype(np.uint8)\n\n        # Resize mask if it doesn't match frame dimensions\n        if mask_np.shape != (self.frame_height, self.frame_width):\n            mask_np = cv2.resize(\n                mask_np,\n                (self.frame_width, self.frame_height),\n                interpolation=cv2.INTER_NEAREST,\n            )\n\n        # Find coordinates where mask is True\n        ys, xs = np.where(mask_np &gt; 0)\n\n        if len(xs) == 0:\n            print(f\"Warning: Empty mask for object {obj_id}. Skipping.\")\n            return {\n                \"frame_index\": frame_idx,\n                \"outputs\": {\"out_obj_ids\": [], \"out_binary_masks\": []},\n            }\n\n        # Sample points from the mask region\n        # Use stratified sampling for better coverage\n        n_points = min(num_points, len(xs))\n\n        if n_points == len(xs):\n            # Use all points if fewer than requested\n            indices = np.arange(len(xs))\n        else:\n            # Sample points with good spatial coverage\n            # Use k-means-like approach: divide mask into regions and sample from each\n            indices = np.linspace(0, len(xs) - 1, n_points, dtype=int)\n\n        points = [[int(xs[i]), int(ys[i])] for i in indices]\n        labels = [1] * len(points)  # All positive points\n\n        # Use add_point_prompts to add the object\n        response = self.add_point_prompts(\n            points=points,\n            labels=labels,\n            obj_id=obj_id,\n            frame_idx=frame_idx,\n        )\n\n        print(\n            f\"Added mask for object {obj_id} on frame {frame_idx} \"\n            f\"using {len(points)} points.\"\n        )\n\n        return response\n\n    def add_masks_prompt(\n        self,\n        masks: List[np.ndarray],\n        obj_ids: Optional[List[int]] = None,\n        frame_idx: int = 0,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Add multiple mask prompts from an external segmentation model.\n\n        Convenience method to add multiple masks at once. This is equivalent\n        to calling add_mask_prompt() for each mask.\n\n        **Important**: The tracker must be initialized first by calling\n        `generate_masks()` with any text prompt before using `add_masks_prompt()`.\n        This is a requirement of SAM3's video tracking architecture.\n\n        Args:\n            masks (List[np.ndarray]): List of binary mask arrays, each of shape\n                (H, W) where True/1 indicates the object region.\n            obj_ids (List[int], optional): List of object IDs for each mask.\n                If None, IDs will be assigned starting from 100 to avoid\n                conflicts with objects detected by generate_masks().\n            frame_idx (int): Frame index to add the masks on. Defaults to 0.\n\n        Returns:\n            List[Dict[str, Any]]: List of responses, one for each mask added.\n\n        Example:\n            &gt;&gt;&gt; # Get multiple object masks from external model\n            &gt;&gt;&gt; masks = external_model.segment_all_objects(first_frame)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Initialize SAM3 and tracker\n            &gt;&gt;&gt; sam = SamGeo3Video()\n            &gt;&gt;&gt; sam.set_video(\"video.mp4\")\n            &gt;&gt;&gt; sam.init_tracker()  # Initialize tracker first!\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Add all masks for tracking\n            &gt;&gt;&gt; sam.add_masks_prompt(masks)  # Adds masks with IDs 100, 101, ...\n            &gt;&gt;&gt; sam.propagate()\n        \"\"\"\n        if obj_ids is None:\n            # Start from 100 to avoid conflicts with text-prompt detected objects\n            obj_ids = list(range(100, 100 + len(masks)))\n\n        if len(masks) != len(obj_ids):\n            raise ValueError(\n                f\"Number of masks ({len(masks)}) must match \"\n                f\"number of obj_ids ({len(obj_ids)})\"\n            )\n\n        responses = []\n        for mask, oid in zip(masks, obj_ids):\n            response = self.add_mask_prompt(mask, obj_id=oid, frame_idx=frame_idx)\n            responses.append(response)\n\n        print(f\"Added {len(masks)} mask(s) for tracking.\")\n        return responses\n\n    def remove_object(self, obj_id: Union[int, List[int]]) -&gt; None:\n        \"\"\"Remove one or more objects from tracking.\n\n        Args:\n            obj_id (Union[int, List[int]]): Object ID(s) to remove.\n                Can be a single int or a list of ints.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"person\")  # Finds 3 people\n            &gt;&gt;&gt; sam.remove_object(2)  # Remove person with ID 2\n            &gt;&gt;&gt; sam.remove_object([1, 3])  # Remove multiple objects at once\n            &gt;&gt;&gt; sam.propagate()  # Re-propagate without removed objects\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        # Convert single int to list for uniform processing\n        obj_ids = [obj_id] if isinstance(obj_id, int) else obj_id\n\n        for oid in obj_ids:\n            self.predictor.handle_request(\n                request=dict(\n                    type=\"remove_object\",\n                    session_id=self.session_id,\n                    obj_id=oid,\n                )\n            )\n\n        if len(obj_ids) == 1:\n            print(f\"Removed object {obj_ids[0]}.\")\n        else:\n            print(f\"Removed objects {obj_ids}.\")\n\n    def propagate(self) -&gt; Dict[int, Any]:\n        \"\"\"Propagate masks through all frames of the video.\n\n        This tracks the segmented objects from the prompt frame through\n        the entire video.\n\n        Returns:\n            Dict[int, Any]: Dictionary mapping frame index to mask outputs.\n        \"\"\"\n        if self.session_id is None:\n            raise ValueError(\"No session active. Please call set_video() first.\")\n\n        outputs_per_frame = {}\n        for response in self.predictor.handle_stream_request(\n            request=dict(\n                type=\"propagate_in_video\",\n                session_id=self.session_id,\n            )\n        ):\n            outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n\n        self.outputs_per_frame = outputs_per_frame\n        print(f\"Propagated masks to {len(outputs_per_frame)} frames.\")\n        return outputs_per_frame\n\n    def _format_outputs(self) -&gt; Dict[int, Dict[int, np.ndarray]]:\n        \"\"\"Format the outputs_per_frame into a simpler structure.\n\n        Returns:\n            Dict mapping frame_idx to Dict mapping obj_id to mask array.\n        \"\"\"\n        if self.outputs_per_frame is None:\n            return {}\n\n        formatted = {}\n        for frame_idx, outputs in self.outputs_per_frame.items():\n            formatted[frame_idx] = {}\n\n            # Handle different output formats\n            if \"out_obj_ids\" in outputs:\n                # Format from propagate_in_video or add_prompt response\n                obj_ids = outputs[\"out_obj_ids\"]\n                # Try multiple possible mask keys\n                masks = outputs.get(\n                    \"out_binary_masks\",\n                    outputs.get(\"out_mask_logits\", outputs.get(\"masks\", [])),\n                )\n\n                if hasattr(obj_ids, \"tolist\"):\n                    obj_ids = obj_ids.tolist()\n\n                for i, obj_id in enumerate(obj_ids):\n                    if i &lt; len(masks):\n                        mask = masks[i]\n                        if hasattr(mask, \"cpu\"):\n                            mask = (mask &gt; 0.0).cpu().numpy()\n                        elif hasattr(mask, \"numpy\"):\n                            mask = (mask &gt; 0.0).numpy()\n                        else:\n                            mask = np.array(mask) &gt; 0.0\n                        formatted[frame_idx][obj_id] = mask.squeeze()\n\n            elif \"object_ids\" in outputs:\n                # Format from add_prompt response\n                obj_ids = outputs[\"object_ids\"]\n                masks = outputs.get(\"masks\", [])\n\n                if hasattr(obj_ids, \"tolist\"):\n                    obj_ids = obj_ids.tolist()\n\n                for i, obj_id in enumerate(obj_ids):\n                    if i &lt; len(masks):\n                        mask = masks[i]\n                        if hasattr(mask, \"cpu\"):\n                            mask = (mask &gt; 0.0).cpu().numpy()\n                        elif hasattr(mask, \"numpy\"):\n                            mask = (mask &gt; 0.0).numpy()\n                        else:\n                            mask = np.array(mask) &gt; 0.0\n                        formatted[frame_idx][obj_id] = mask.squeeze()\n\n            elif isinstance(outputs, dict):\n                # Already in {obj_id: mask} format\n                for obj_id, mask in outputs.items():\n                    if isinstance(obj_id, int):\n                        if hasattr(mask, \"cpu\"):\n                            mask = (mask &gt; 0.0).cpu().numpy()\n                        elif hasattr(mask, \"numpy\"):\n                            mask = (mask &gt; 0.0).numpy()\n                        else:\n                            mask = np.array(mask) &gt; 0.0\n                        formatted[frame_idx][obj_id] = mask.squeeze()\n\n        return formatted\n\n    def save_masks(\n        self,\n        output_dir: str,\n        img_ext: str = \"png\",\n        dtype: str = \"uint8\",\n    ) -&gt; List[str]:\n        \"\"\"Save segmentation masks to files.\n\n        For GeoTIFF time series, masks are saved with georeferencing information.\n\n        Args:\n            output_dir (str): Directory to save mask files.\n            img_ext (str): Image extension for output files. Defaults to \"png\".\n                For GeoTIFF time series, this is overridden to \"tif\".\n            dtype (str): Data type for mask values. Defaults to \"uint8\".\n\n        Returns:\n            List[str]: List of saved file paths.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"building\")\n            &gt;&gt;&gt; sam.save_masks(\"output/masks/\")\n        \"\"\"\n        if self.outputs_per_frame is None:\n            raise ValueError(\"No masks to save. Please run generate_masks() first.\")\n\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Prepare mask data using our custom formatter\n        formatted_outputs = self._format_outputs()\n\n        if not formatted_outputs:\n            print(\"No masks to save.\")\n            return []\n\n        num_digits = len(str(len(self.video_frames)))\n        saved_files = []\n\n        # Check if we have GeoTIFF source\n        is_geotiff = self._tif_source is not None and self._tif_source.lower().endswith(\n            (\".tif\", \".tiff\")\n        )\n        if is_geotiff:\n            img_ext = \"tif\"\n\n        # Determine frame dimensions once\n        if isinstance(self.video_frames[0], str):\n            first_frame = load_frame(self.video_frames[0])\n            h, w = first_frame.shape[:2]\n        else:\n            h, w = self.video_frames[0].shape[:2]\n\n        for frame_idx in tqdm(sorted(formatted_outputs.keys()), desc=\"Saving masks\"):\n            frame_data = formatted_outputs[frame_idx]\n            mask_array = np.zeros((h, w), dtype=np.uint8)\n\n            # Combine all object masks with unique IDs\n            for obj_id, mask in frame_data.items():\n                mask_np = np.array(mask)\n                if mask_np.ndim &gt; 2:\n                    mask_np = mask_np.squeeze()\n                # Resize mask if needed\n                if mask_np.shape != (h, w):\n                    mask_np = cv2.resize(\n                        mask_np.astype(np.uint8),\n                        (w, h),\n                        interpolation=cv2.INTER_NEAREST,\n                    )\n                mask_array[mask_np &gt; 0] = obj_id\n\n            # Determine output path\n            if is_geotiff and self._tif_names is not None:\n                base_name = os.path.splitext(self._tif_names[frame_idx])[0]\n                filename = f\"{base_name}_mask.{img_ext}\"\n                crs_source = os.path.join(self._tif_dir, self._tif_names[frame_idx])\n            else:\n                filename = f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n                crs_source = None\n\n            output_path = os.path.join(output_dir, filename)\n\n            if is_geotiff:\n                common.array_to_image(mask_array, output_path, crs_source, dtype=dtype)\n            else:\n                img = Image.fromarray(mask_array)\n                img.save(output_path)\n\n            saved_files.append(output_path)\n\n        print(f\"Saved {len(saved_files)} mask files to {output_dir}\")\n\n    def save_video(\n        self,\n        output_path: str,\n        fps: int = 30,\n        alpha: float = 0.6,\n        dpi: int = 200,\n        frame_stride: int = 1,\n        show_ids: Union[bool, Dict[int, str]] = True,\n    ) -&gt; str:\n        \"\"\"Save segmentation results as a video with blended masks.\n\n        Args:\n            output_path (str): Path to save the output video (MP4).\n            fps (int): Frames per second for the output video. Defaults to 30.\n            alpha (float): Opacity for mask overlay. Defaults to 0.6.\n            dpi (int): DPI for rendering. Defaults to 200.\n            frame_stride (int): Process every nth frame. Defaults to 1.\n            show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs\n                on the video. If True, shows numeric IDs. If False, hides IDs.\n                If a dict, maps object IDs to custom labels (e.g., player names).\n                Defaults to True.\n\n        Returns:\n            str: Path to the saved video.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"car\")\n            &gt;&gt;&gt; sam.save_video(\"output.mp4\")\n            &gt;&gt;&gt; # With custom labels\n            &gt;&gt;&gt; sam.save_video(\"output.mp4\", show_ids={1: \"Player A\", 2: \"Player B\"})\n        \"\"\"\n        if self.outputs_per_frame is None:\n            raise ValueError(\"No masks to save. Please run generate_masks() first.\")\n\n        # Create temporary directory for frames\n        temp_dir = common.make_temp_dir()\n        os.makedirs(temp_dir, exist_ok=True)\n\n        # Save blended frames\n        self._save_blended_frames(\n            temp_dir,\n            alpha=alpha,\n            dpi=dpi,\n            frame_stride=frame_stride,\n            show_ids=show_ids,\n        )\n\n        # Create video from frames\n        common.images_to_video(temp_dir, output_path, fps=fps)\n        print(f\"Saved video to {output_path}\")\n\n    def _save_blended_frames(\n        self,\n        output_dir: str,\n        alpha: float = 0.6,\n        dpi: int = 200,\n        frame_stride: int = 1,\n        show_ids: Union[bool, Dict[int, str]] = True,\n    ) -&gt; None:\n        \"\"\"Save frames with blended mask overlays.\n\n        Args:\n            output_dir (str): Directory to save blended frames.\n            alpha (float): Opacity for mask overlay.\n            dpi (int): DPI for rendering (not used in optimized version).\n            frame_stride (int): Process every nth frame.\n            show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs.\n                If True, shows numeric IDs. If False, hides IDs.\n                If a dict, maps object IDs to custom labels.\n        \"\"\"\n        formatted_outputs = self._format_outputs()\n        num_frames = len(self.video_frames)\n        num_digits = len(str(num_frames))\n\n        # Pre-compute colors (tab10 colormap)\n        tab10_colors = [\n            (31, 119, 180),  # blue\n            (255, 127, 14),  # orange\n            (44, 160, 44),  # green\n            (214, 39, 40),  # red\n            (148, 103, 189),  # purple\n            (140, 86, 75),  # brown\n            (227, 119, 194),  # pink\n            (127, 127, 127),  # gray\n            (188, 189, 34),  # olive\n            (23, 190, 207),  # cyan\n        ]\n\n        for frame_idx in tqdm(\n            range(0, num_frames, frame_stride), desc=\"Rendering frames\"\n        ):\n            if frame_idx not in formatted_outputs:\n                continue\n\n            # Load frame\n            if isinstance(self.video_frames[frame_idx], str):\n                frame = Image.open(self.video_frames[frame_idx]).convert(\"RGB\")\n            else:\n                frame = Image.fromarray(self.video_frames[frame_idx]).convert(\"RGB\")\n\n            frame_np = np.array(frame, dtype=np.float32)\n            h, w = frame_np.shape[:2]\n\n            # Create overlay for all masks\n            overlay = np.zeros((h, w, 3), dtype=np.float32)\n            mask_combined = np.zeros((h, w), dtype=np.float32)\n\n            frame_data = formatted_outputs[frame_idx]\n            labels_to_draw = []\n\n            for obj_id, mask in frame_data.items():\n                if isinstance(obj_id, str) and obj_id == \"image\":\n                    continue\n\n                color = tab10_colors[obj_id % 10]\n                mask_np = np.array(mask)\n                if mask_np.ndim &gt; 2:\n                    mask_np = mask_np.squeeze()\n\n                # Resize mask if it doesn't match frame dimensions\n                if mask_np.shape != (h, w):\n                    mask_np = cv2.resize(\n                        mask_np.astype(np.float32),\n                        (w, h),\n                        interpolation=cv2.INTER_NEAREST,\n                    )\n\n                # Add color to overlay where mask is present\n                mask_bool = mask_np &gt; 0\n                for c in range(3):\n                    overlay[:, :, c] = np.where(mask_bool, color[c], overlay[:, :, c])\n                mask_combined = np.maximum(mask_combined, mask_np)\n\n                # Collect label info\n                if show_ids:\n                    ys, xs = np.where(mask_bool)\n                    if len(xs) &gt; 0 and len(ys) &gt; 0:\n                        cx, cy = int(np.mean(xs)), int(np.mean(ys))\n                        if isinstance(show_ids, dict):\n                            label = show_ids.get(obj_id, str(obj_id))\n                        else:\n                            label = str(obj_id)\n                        labels_to_draw.append((cx, cy, label, color))\n\n            # Blend overlay with frame\n            mask_3d = mask_combined[:, :, np.newaxis]\n            blended = frame_np * (1 - mask_3d * alpha) + overlay * (mask_3d * alpha)\n            blended = np.clip(blended, 0, 255).astype(np.uint8)\n\n            # Draw labels using OpenCV\n            for cx, cy, label, color in labels_to_draw:\n                # Get text size for background rectangle\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                font_scale = 0.7\n                thickness = 2\n                (text_w, text_h), baseline = cv2.getTextSize(\n                    label, font, font_scale, thickness\n                )\n\n                # Draw background rectangle\n                pad = 4\n                x1 = cx - text_w // 2 - pad\n                y1 = cy - text_h // 2 - pad\n                x2 = cx + text_w // 2 + pad\n                y2 = cy + text_h // 2 + pad + baseline\n\n                # Semi-transparent background\n                sub_img = blended[max(0, y1) : min(h, y2), max(0, x1) : min(w, x2)]\n                if sub_img.size &gt; 0:\n                    bg_color = np.array(color, dtype=np.float32)\n                    blend_rect = (sub_img * 0.3 + bg_color * 0.7).astype(np.uint8)\n                    blended[max(0, y1) : min(h, y2), max(0, x1) : min(w, x2)] = (\n                        blend_rect\n                    )\n\n                # Draw text\n                text_x = cx - text_w // 2\n                text_y = cy + text_h // 2\n                cv2.putText(\n                    blended,\n                    label,\n                    (text_x, text_y),\n                    font,\n                    font_scale,\n                    (255, 255, 255),\n                    thickness,\n                    cv2.LINE_AA,\n                )\n\n            # Save frame\n            filename = f\"{str(frame_idx).zfill(num_digits)}.png\"\n            filepath = os.path.join(output_dir, filename)\n            cv2.imwrite(filepath, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n\n    def show_frame(\n        self,\n        frame_idx: int = 0,\n        figsize: Tuple[int, int] = (12, 8),\n        alpha: float = 0.6,\n        show_ids: Union[bool, Dict[int, str]] = True,\n        axis: str = \"off\",\n        output: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Display a single frame with mask overlay.\n\n        Args:\n            frame_idx (int): Frame index to display. Defaults to 0.\n            figsize (Tuple[int, int]): Figure size. Defaults to (12, 8).\n            alpha (float): Opacity for mask overlay. Defaults to 0.6.\n            show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs.\n                If True, shows numeric IDs. If False, hides IDs.\n                If a dict, maps object IDs to custom labels (e.g., player names).\n                Defaults to True.\n            axis (str): Axis visibility setting. Defaults to \"off\".\n            output (str, optional): Path to save the figure. Defaults to None.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"tree\")\n            &gt;&gt;&gt; sam.show_frame(0)  # Show first frame\n            &gt;&gt;&gt; sam.show_frame(50, output=\"frame_50.png\")  # Save frame 50\n            &gt;&gt;&gt; # With custom labels\n            &gt;&gt;&gt; sam.show_frame(0, show_ids={1: \"Player A\", 2: \"Player B\"})\n        \"\"\"\n        if self.outputs_per_frame is None:\n            raise ValueError(\"No masks to show. Please run generate_masks() first.\")\n\n        formatted_outputs = self._format_outputs()\n\n        if frame_idx not in formatted_outputs:\n            print(f\"Frame {frame_idx} not in outputs.\")\n            return\n\n        # Load frame\n        if isinstance(self.video_frames[frame_idx], str):\n            frame = Image.open(self.video_frames[frame_idx])\n        else:\n            frame = Image.fromarray(self.video_frames[frame_idx])\n\n        w_frame, h_frame = frame.size\n\n        fig = plt.figure(figsize=figsize)\n        plt.axis(axis)\n        plt.title(f\"Frame {frame_idx}\")\n        plt.imshow(frame)\n\n        # Overlay masks\n        frame_data = formatted_outputs[frame_idx]\n        cmap = plt.get_cmap(\"tab10\")\n\n        for obj_id, mask in frame_data.items():\n            if isinstance(obj_id, str) and obj_id == \"image\":\n                continue\n\n            color = np.array([*cmap(obj_id % 10)[:3], alpha])\n            mask_np = np.array(mask)\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np.squeeze()\n\n            # Resize mask if it doesn't match frame dimensions\n            if mask_np.shape != (h_frame, w_frame):\n                mask_np = cv2.resize(\n                    mask_np.astype(np.float32),\n                    (w_frame, h_frame),\n                    interpolation=cv2.INTER_NEAREST,\n                )\n\n            mask_image = mask_np.reshape(h_frame, w_frame, 1) * color.reshape(1, 1, -1)\n            plt.gca().imshow(mask_image)\n\n            # Add object ID label\n            if show_ids:\n                ys, xs = np.where(mask_np &gt; 0)\n                if len(xs) &gt; 0 and len(ys) &gt; 0:\n                    cx, cy = np.mean(xs), np.mean(ys)\n                    # Determine label text\n                    if isinstance(show_ids, dict):\n                        label = show_ids.get(obj_id, str(obj_id))\n                    else:\n                        label = str(obj_id)\n                    plt.text(\n                        cx,\n                        cy,\n                        label,\n                        color=\"white\",\n                        fontsize=12,\n                        fontweight=\"bold\",\n                        ha=\"center\",\n                        va=\"center\",\n                        bbox=dict(\n                            facecolor=cmap(obj_id % 10)[:3],\n                            alpha=0.7,\n                            edgecolor=\"none\",\n                            pad=2,\n                        ),\n                    )\n\n        if output is not None:\n            plt.savefig(output, dpi=150, bbox_inches=\"tight\", pad_inches=0.1)\n            print(f\"Saved frame to {output}\")\n            plt.close(fig)\n        else:\n            plt.show()\n\n    def show_frames(\n        self,\n        frame_stride: int = 10,\n        ncols: int = 3,\n        figsize_per_frame: Tuple[int, int] = (6, 4),\n        alpha: float = 0.6,\n        show_ids: Union[bool, Dict[int, str]] = False,\n    ) -&gt; None:\n        \"\"\"Display multiple frames with mask overlays in a grid.\n\n        Args:\n            frame_stride (int): Show every nth frame. Defaults to 10.\n            ncols (int): Number of columns in the grid. Defaults to 3.\n            figsize_per_frame (Tuple[int, int]): Size per subplot. Defaults to (6, 4).\n            alpha (float): Opacity for mask overlay. Defaults to 0.6.\n            show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs.\n                If True, shows numeric IDs. If False, hides IDs.\n                If a dict, maps object IDs to custom labels (e.g., player names).\n                Defaults to False.\n\n        Example:\n            &gt;&gt;&gt; sam.generate_masks(\"person\")\n            &gt;&gt;&gt; sam.show_frames(frame_stride=30, ncols=4)\n            &gt;&gt;&gt; # With custom labels\n            &gt;&gt;&gt; sam.show_frames(show_ids={1: \"Player A\", 2: \"Player B\"})\n        \"\"\"\n        if self.outputs_per_frame is None:\n            raise ValueError(\"No masks to show. Please run generate_masks() first.\")\n\n        formatted_outputs = self._format_outputs()\n        frame_indices = list(range(0, len(self.video_frames), frame_stride))\n        nrows = (len(frame_indices) + ncols - 1) // ncols\n\n        fig, axes = plt.subplots(\n            nrows,\n            ncols,\n            figsize=(figsize_per_frame[0] * ncols, figsize_per_frame[1] * nrows),\n        )\n        if nrows == 1 and ncols == 1:\n            axes = np.array([[axes]])\n        elif nrows == 1 or ncols == 1:\n            axes = axes.reshape(-1)\n\n        axes = np.array(axes).flatten()\n\n        cmap = plt.get_cmap(\"tab10\")\n\n        for i, frame_idx in enumerate(frame_indices):\n            ax = axes[i]\n            ax.axis(\"off\")\n            ax.set_title(f\"Frame {frame_idx}\")\n\n            # Load frame\n            if isinstance(self.video_frames[frame_idx], str):\n                frame = Image.open(self.video_frames[frame_idx])\n            else:\n                frame = Image.fromarray(self.video_frames[frame_idx])\n\n            w_frame, h_frame = frame.size\n            ax.imshow(frame)\n\n            if frame_idx in formatted_outputs:\n                frame_data = formatted_outputs[frame_idx]\n\n                for obj_id, mask in frame_data.items():\n                    if isinstance(obj_id, str) and obj_id == \"image\":\n                        continue\n\n                    color = np.array([*cmap(obj_id % 10)[:3], alpha])\n                    mask_np = np.array(mask)\n                    if mask_np.ndim &gt; 2:\n                        mask_np = mask_np.squeeze()\n\n                    # Resize mask if it doesn't match frame dimensions\n                    if mask_np.shape != (h_frame, w_frame):\n                        mask_np = cv2.resize(\n                            mask_np.astype(np.float32),\n                            (w_frame, h_frame),\n                            interpolation=cv2.INTER_NEAREST,\n                        )\n\n                    mask_image = mask_np.reshape(h_frame, w_frame, 1) * color.reshape(\n                        1, 1, -1\n                    )\n                    ax.imshow(mask_image)\n\n                    if show_ids:\n                        ys, xs = np.where(mask_np &gt; 0)\n                        if len(xs) &gt; 0 and len(ys) &gt; 0:\n                            cx, cy = np.mean(xs), np.mean(ys)\n                            # Determine label text\n                            if isinstance(show_ids, dict):\n                                label = show_ids.get(obj_id, str(obj_id))\n                            else:\n                                label = str(obj_id)\n                            ax.text(\n                                cx,\n                                cy,\n                                label,\n                                color=\"white\",\n                                fontsize=10,\n                                fontweight=\"bold\",\n                                ha=\"center\",\n                                va=\"center\",\n                            )\n\n        # Hide unused subplots\n        for j in range(len(frame_indices), len(axes)):\n            axes[j].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def close(self) -&gt; None:\n        \"\"\"Close the current session and free GPU resources.\n\n        Call this when you're done with the current video and want to\n        process a new one, or when you want to free up memory.\n        \"\"\"\n        if self.session_id is not None:\n            self.predictor.handle_request(\n                request=dict(\n                    type=\"close_session\",\n                    session_id=self.session_id,\n                )\n            )\n            self.session_id = None\n            print(\"Session closed.\")\n\n    def shutdown(self) -&gt; None:\n        \"\"\"Shutdown the predictor and free all GPU resources.\n\n        Call this when you're completely done with video segmentation.\n        After calling this, you cannot use this instance anymore.\n        \"\"\"\n        self.close()\n        self.predictor.shutdown()\n        print(\"Predictor shutdown complete.\")\n\n    def __del__(self):\n        \"\"\"Destructor to clean up resources.\"\"\"\n        try:\n            if hasattr(self, \"session_id\") and self.session_id is not None:\n                self.close()\n        except Exception:\n            pass\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to clean up resources.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to clean up resources.\"\"\"\n    try:\n        if hasattr(self, \"session_id\") and self.session_id is not None:\n            self.close()\n    except Exception:\n        pass\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.__init__","title":"<code>__init__(gpus_to_use=None, bpe_path=None, **kwargs)</code>","text":"<p>Initialize the SamGeo3Video class.</p> <p>Parameters:</p> Name Type Description Default <code>gpus_to_use</code> <code>List[int]</code> <p>List of GPU indices to use. If None, uses all available GPUs. Defaults to None.</p> <code>None</code> <code>bpe_path</code> <code>str</code> <p>Path to the BPE tokenizer vocabulary. If None, uses the default path. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to build_sam3_video_predictor.</p> <code>{}</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def __init__(\n    self,\n    gpus_to_use: Optional[List[int]] = None,\n    bpe_path: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the SamGeo3Video class.\n\n    Args:\n        gpus_to_use (List[int], optional): List of GPU indices to use.\n            If None, uses all available GPUs. Defaults to None.\n        bpe_path (str, optional): Path to the BPE tokenizer vocabulary.\n            If None, uses the default path. Defaults to None.\n        **kwargs: Additional keyword arguments passed to build_sam3_video_predictor.\n    \"\"\"\n    if not SAM3_META_AVAILABLE:\n        error_msg = (\n            \"SAM3 is not available. Please install it as:\\n\\t\"\n            \"pip install segment-geospatial[samgeo3]\"\n        )\n        if SAM3_META_IMPORT_ERROR is not None:\n            error_msg += f\"\\n\\nUnderlying import error:\\n\\t{SAM3_META_IMPORT_ERROR}\"\n        raise ImportError(error_msg)\n\n    import torch\n\n    # Set up GPU configuration\n    if gpus_to_use is None:\n        gpus_to_use = list(range(torch.cuda.device_count()))\n        if len(gpus_to_use) == 0:\n            gpus_to_use = [torch.cuda.current_device()]\n\n    # Set up BPE path\n    if bpe_path is None:\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        bpe_path = os.path.abspath(\n            os.path.join(current_dir, \"assets\", \"bpe_simple_vocab_16e6.txt.gz\")\n        )\n        if not os.path.exists(bpe_path):\n            bpe_dir = os.path.dirname(bpe_path)\n            os.makedirs(bpe_dir, exist_ok=True)\n            url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/bpe_simple_vocab_16e6.txt.gz\"\n            bpe_path = common.download_file(url, bpe_path, quiet=True)\n\n    print(f\"Using GPUs: {gpus_to_use}\")\n\n    self.predictor = build_sam3_video_predictor(\n        gpus_to_use=gpus_to_use, bpe_path=bpe_path, **kwargs\n    )\n    self.gpus_to_use = gpus_to_use\n    self.session_id = None\n    self.video_path = None\n    self.video_frames = None\n    self.outputs_per_frame = None\n    self.frame_width = None\n    self.frame_height = None\n    self._tif_source = None\n    self._tif_dir = None\n    self._tif_names = None\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_box_prompt","title":"<code>add_box_prompt(box, obj_id, frame_idx=0, box_crs=None)</code>","text":"<p>Add a bounding box prompt to segment an object.</p> <p>Parameters:</p> Name Type Description Default <code>box</code> <code>List[float]</code> <p>Bounding box in [x, y, width, height] format. In pixel coordinates by default, or in the specified CRS.</p> required <code>obj_id</code> <code>int</code> <p>Object ID to associate with this prompt.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index to add the prompt on. Defaults to 0.</p> <code>0</code> <code>box_crs</code> <code>str</code> <p>Coordinate reference system for box (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Response containing the mask output.</p> Example <p>sam.add_box_prompt([100, 100, 200, 150], obj_id=1)</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def add_box_prompt(\n    self,\n    box: List[float],\n    obj_id: int,\n    frame_idx: int = 0,\n    box_crs: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Add a bounding box prompt to segment an object.\n\n    Args:\n        box (List[float]): Bounding box in [x, y, width, height] format.\n            In pixel coordinates by default, or in the specified CRS.\n        obj_id (int): Object ID to associate with this prompt.\n        frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n        box_crs (str, optional): Coordinate reference system for box\n            (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.\n\n    Returns:\n        Dict[str, Any]: Response containing the mask output.\n\n    Example:\n        &gt;&gt;&gt; sam.add_box_prompt([100, 100, 200, 150], obj_id=1)\n    \"\"\"\n    import torch\n\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    x, y, w, h = box\n\n    # Transform coordinates if CRS is provided\n    if box_crs is not None and self._tif_source is not None:\n        # Convert box corners to pixel coordinates\n        corners = np.array([[x, y], [x + w, y + h]])\n        corners = common.coords_to_xy(self._tif_source, corners, box_crs)\n        x, y = corners[0]\n        x2, y2 = corners[1]\n        w = x2 - x\n        h = y2 - y\n\n    # Convert to relative coordinates [cx, cy, w, h]\n    cx = (x + w / 2) / self.frame_width\n    cy = (y + h / 2) / self.frame_height\n    rel_w = w / self.frame_width\n    rel_h = h / self.frame_height\n\n    box_tensor = torch.tensor([cx, cy, rel_w, rel_h], dtype=torch.float32)\n\n    response = self.predictor.handle_request(\n        request=dict(\n            type=\"add_prompt\",\n            session_id=self.session_id,\n            frame_index=frame_idx,\n            box=box_tensor,\n            obj_id=obj_id,\n        )\n    )\n\n    return response\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt","title":"<code>add_mask_prompt(mask, obj_id, frame_idx=0, num_points=5)</code>","text":"<p>Add a mask prompt from an external segmentation model.</p> <p>This allows using masks generated by other segmentation models (e.g., YOLO, Detectron2, GroundingDINO, etc.) and leveraging SAM3's tracking capability to propagate them through the video.</p> <p>This is useful when SAM3's text prompts don't segment the exact objects you need. You can: 1. Use another model to get accurate segmentation on the first frame 2. Pass those masks to SAM3 using this method 3. Use SAM3's tracking to propagate the masks through the video</p> <p>The method works by sampling positive points from the mask region and using SAM3's point prompt capability to segment and track the object.</p> <p>Important: The tracker must be initialized first by calling <code>init_tracker()</code> (recommended) or <code>generate_masks()</code> (alternative) with any text prompt before using <code>add_mask_prompt()</code>. This is a requirement of SAM3's video tracking architecture.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Binary mask array of shape (H, W) where True/1 indicates the object region. Can be bool or numeric. The mask should match the video frame dimensions.</p> required <code>obj_id</code> <code>int</code> <p>Object ID to associate with this mask. Use different IDs for different objects you want to track.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index to add the mask on. Defaults to 0.</p> <code>0</code> <code>num_points</code> <code>int</code> <p>Number of points to sample from the mask region. More points can improve accuracy. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Response containing the mask output with keys: - \"frame_index\": The frame index where mask was added - \"outputs\": Dict with \"out_obj_ids\" and mask data</p> Example Note <ul> <li>The tracker MUST be initialized first by calling either <code>init_tracker()</code> or <code>generate_masks()</code></li> <li>Each call to add_mask_prompt adds one object for tracking</li> <li>Use different obj_id values for different objects</li> <li>Use obj_ids different from those detected by generate_masks()</li> <li>The mask should be binary (0/False for background, 1/True for object)</li> <li>For best results, ensure mask dimensions match the video frame size</li> </ul> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def add_mask_prompt(\n    self,\n    mask: np.ndarray,\n    obj_id: int,\n    frame_idx: int = 0,\n    num_points: int = 5,\n) -&gt; Dict[str, Any]:\n    \"\"\"Add a mask prompt from an external segmentation model.\n\n    This allows using masks generated by other segmentation models\n    (e.g., YOLO, Detectron2, GroundingDINO, etc.) and leveraging SAM3's\n    tracking capability to propagate them through the video.\n\n    This is useful when SAM3's text prompts don't segment the exact objects\n    you need. You can:\n    1. Use another model to get accurate segmentation on the first frame\n    2. Pass those masks to SAM3 using this method\n    3. Use SAM3's tracking to propagate the masks through the video\n\n    The method works by sampling positive points from the mask region and\n    using SAM3's point prompt capability to segment and track the object.\n\n    **Important**: The tracker must be initialized first by calling\n    `init_tracker()` (recommended) or `generate_masks()` (alternative) with any text prompt before using `add_mask_prompt()`.\n    This is a requirement of SAM3's video tracking architecture.\n\n    Args:\n        mask (np.ndarray): Binary mask array of shape (H, W) where\n            True/1 indicates the object region. Can be bool or numeric.\n            The mask should match the video frame dimensions.\n        obj_id (int): Object ID to associate with this mask. Use different\n            IDs for different objects you want to track.\n        frame_idx (int): Frame index to add the mask on. Defaults to 0.\n        num_points (int): Number of points to sample from the mask region.\n            More points can improve accuracy. Defaults to 5.\n\n    Returns:\n        Dict[str, Any]: Response containing the mask output with keys:\n            - \"frame_index\": The frame index where mask was added\n            - \"outputs\": Dict with \"out_obj_ids\" and mask data\n\n    Example:\n        &gt;&gt;&gt; # Workflow: Use external model + SAM3 tracking\n        &gt;&gt;&gt; from your_segmentation_model import segment_objects\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Step 1: Get segmentation from another model on first frame\n        &gt;&gt;&gt; first_frame = load_image(\"frames/0.jpg\")\n        &gt;&gt;&gt; external_masks = segment_objects(first_frame)  # Your model\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Step 2: Initialize SAM3 video tracker\n        &gt;&gt;&gt; sam = SamGeo3Video()\n        &gt;&gt;&gt; sam.set_video(\"frames/\")  # Load video frames\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Step 3: Initialize tracker for external masks\n        &gt;&gt;&gt; sam.init_tracker()  # Initialize tracker for external masks\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Step 4: Add external masks for tracking (use new obj_ids)\n        &gt;&gt;&gt; for i, mask in enumerate(external_masks):\n        ...     sam.add_mask_prompt(mask, obj_id=100+i, frame_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Step 5: Propagate to track through video\n        &gt;&gt;&gt; sam.propagate()\n        &gt;&gt;&gt; sam.save_video(\"tracked_output.mp4\")\n\n    Note:\n        - The tracker MUST be initialized first by calling either `init_tracker()` or `generate_masks()`\n        - Each call to add_mask_prompt adds one object for tracking\n        - Use different obj_id values for different objects\n        - Use obj_ids different from those detected by generate_masks()\n        - The mask should be binary (0/False for background, 1/True for object)\n        - For best results, ensure mask dimensions match the video frame size\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    # Check if tracker has been initialized (has propagated at least once)\n    session = self.predictor._ALL_INFERENCE_STATES.get(self.session_id, None)\n    if session is None:\n        raise RuntimeError(\n            f\"Cannot find session {self.session_id}; it might have expired\"\n        )\n    inference_state = session[\"state\"]\n\n    # Check if tracker is initialized by looking for cached outputs\n    has_cached_outputs = bool(inference_state.get(\"cached_frame_outputs\", {}))\n    tracker_states = inference_state.get(\"tracker_inference_states\", [])\n\n    if not has_cached_outputs and len(tracker_states) == 0:\n        raise RuntimeError(\n            \"Tracker not initialized. Please call init_tracker() or \"\n            \"generate_masks() first to initialize the tracker before adding \"\n            \"mask prompts. Example: sam.init_tracker() or sam.generate_masks('object')\"\n        )\n\n    # Convert mask to numpy array and ensure 2D\n    mask_np = np.array(mask)\n    if mask_np.ndim &gt; 2:\n        mask_np = mask_np.squeeze()\n\n    # Convert to binary\n    mask_np = (mask_np &gt; 0).astype(np.uint8)\n\n    # Resize mask if it doesn't match frame dimensions\n    if mask_np.shape != (self.frame_height, self.frame_width):\n        mask_np = cv2.resize(\n            mask_np,\n            (self.frame_width, self.frame_height),\n            interpolation=cv2.INTER_NEAREST,\n        )\n\n    # Find coordinates where mask is True\n    ys, xs = np.where(mask_np &gt; 0)\n\n    if len(xs) == 0:\n        print(f\"Warning: Empty mask for object {obj_id}. Skipping.\")\n        return {\n            \"frame_index\": frame_idx,\n            \"outputs\": {\"out_obj_ids\": [], \"out_binary_masks\": []},\n        }\n\n    # Sample points from the mask region\n    # Use stratified sampling for better coverage\n    n_points = min(num_points, len(xs))\n\n    if n_points == len(xs):\n        # Use all points if fewer than requested\n        indices = np.arange(len(xs))\n    else:\n        # Sample points with good spatial coverage\n        # Use k-means-like approach: divide mask into regions and sample from each\n        indices = np.linspace(0, len(xs) - 1, n_points, dtype=int)\n\n    points = [[int(xs[i]), int(ys[i])] for i in indices]\n    labels = [1] * len(points)  # All positive points\n\n    # Use add_point_prompts to add the object\n    response = self.add_point_prompts(\n        points=points,\n        labels=labels,\n        obj_id=obj_id,\n        frame_idx=frame_idx,\n    )\n\n    print(\n        f\"Added mask for object {obj_id} on frame {frame_idx} \"\n        f\"using {len(points)} points.\"\n    )\n\n    return response\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--workflow-use-external-model-sam3-tracking","title":"Workflow: Use external model + SAM3 tracking","text":"<p>from your_segmentation_model import segment_objects</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--step-1-get-segmentation-from-another-model-on-first-frame","title":"Step 1: Get segmentation from another model on first frame","text":"<p>first_frame = load_image(\"frames/0.jpg\") external_masks = segment_objects(first_frame)  # Your model</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--step-2-initialize-sam3-video-tracker","title":"Step 2: Initialize SAM3 video tracker","text":"<p>sam = SamGeo3Video() sam.set_video(\"frames/\")  # Load video frames</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--step-3-initialize-tracker-for-external-masks","title":"Step 3: Initialize tracker for external masks","text":"<p>sam.init_tracker()  # Initialize tracker for external masks</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--step-4-add-external-masks-for-tracking-use-new-obj_ids","title":"Step 4: Add external masks for tracking (use new obj_ids)","text":"<p>for i, mask in enumerate(external_masks): ...     sam.add_mask_prompt(mask, obj_id=100+i, frame_idx=0)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_mask_prompt--step-5-propagate-to-track-through-video","title":"Step 5: Propagate to track through video","text":"<p>sam.propagate() sam.save_video(\"tracked_output.mp4\")</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_masks_prompt","title":"<code>add_masks_prompt(masks, obj_ids=None, frame_idx=0)</code>","text":"<p>Add multiple mask prompts from an external segmentation model.</p> <p>Convenience method to add multiple masks at once. This is equivalent to calling add_mask_prompt() for each mask.</p> <p>Important: The tracker must be initialized first by calling <code>generate_masks()</code> with any text prompt before using <code>add_masks_prompt()</code>. This is a requirement of SAM3's video tracking architecture.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>List[ndarray]</code> <p>List of binary mask arrays, each of shape (H, W) where True/1 indicates the object region.</p> required <code>obj_ids</code> <code>List[int]</code> <p>List of object IDs for each mask. If None, IDs will be assigned starting from 100 to avoid conflicts with objects detected by generate_masks().</p> <code>None</code> <code>frame_idx</code> <code>int</code> <p>Frame index to add the masks on. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of responses, one for each mask added.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def add_masks_prompt(\n    self,\n    masks: List[np.ndarray],\n    obj_ids: Optional[List[int]] = None,\n    frame_idx: int = 0,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Add multiple mask prompts from an external segmentation model.\n\n    Convenience method to add multiple masks at once. This is equivalent\n    to calling add_mask_prompt() for each mask.\n\n    **Important**: The tracker must be initialized first by calling\n    `generate_masks()` with any text prompt before using `add_masks_prompt()`.\n    This is a requirement of SAM3's video tracking architecture.\n\n    Args:\n        masks (List[np.ndarray]): List of binary mask arrays, each of shape\n            (H, W) where True/1 indicates the object region.\n        obj_ids (List[int], optional): List of object IDs for each mask.\n            If None, IDs will be assigned starting from 100 to avoid\n            conflicts with objects detected by generate_masks().\n        frame_idx (int): Frame index to add the masks on. Defaults to 0.\n\n    Returns:\n        List[Dict[str, Any]]: List of responses, one for each mask added.\n\n    Example:\n        &gt;&gt;&gt; # Get multiple object masks from external model\n        &gt;&gt;&gt; masks = external_model.segment_all_objects(first_frame)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize SAM3 and tracker\n        &gt;&gt;&gt; sam = SamGeo3Video()\n        &gt;&gt;&gt; sam.set_video(\"video.mp4\")\n        &gt;&gt;&gt; sam.init_tracker()  # Initialize tracker first!\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add all masks for tracking\n        &gt;&gt;&gt; sam.add_masks_prompt(masks)  # Adds masks with IDs 100, 101, ...\n        &gt;&gt;&gt; sam.propagate()\n    \"\"\"\n    if obj_ids is None:\n        # Start from 100 to avoid conflicts with text-prompt detected objects\n        obj_ids = list(range(100, 100 + len(masks)))\n\n    if len(masks) != len(obj_ids):\n        raise ValueError(\n            f\"Number of masks ({len(masks)}) must match \"\n            f\"number of obj_ids ({len(obj_ids)})\"\n        )\n\n    responses = []\n    for mask, oid in zip(masks, obj_ids):\n        response = self.add_mask_prompt(mask, obj_id=oid, frame_idx=frame_idx)\n        responses.append(response)\n\n    print(f\"Added {len(masks)} mask(s) for tracking.\")\n    return responses\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_masks_prompt--get-multiple-object-masks-from-external-model","title":"Get multiple object masks from external model","text":"<p>masks = external_model.segment_all_objects(first_frame)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_masks_prompt--initialize-sam3-and-tracker","title":"Initialize SAM3 and tracker","text":"<p>sam = SamGeo3Video() sam.set_video(\"video.mp4\") sam.init_tracker()  # Initialize tracker first!</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_masks_prompt--add-all-masks-for-tracking","title":"Add all masks for tracking","text":"<p>sam.add_masks_prompt(masks)  # Adds masks with IDs 100, 101, ... sam.propagate()</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_point_prompts","title":"<code>add_point_prompts(points, labels, obj_id, frame_idx=0, point_crs=None)</code>","text":"<p>Add point prompts to segment or refine an object.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>List[List[float]]</code> <p>List of [x, y] point coordinates. In pixel coordinates by default, or in the specified CRS.</p> required <code>labels</code> <code>List[int]</code> <p>List of labels for each point. 1 for positive (include), 0 for negative (exclude).</p> required <code>obj_id</code> <code>int</code> <p>Object ID to associate with this prompt.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index to add the prompt on. Defaults to 0.</p> <code>0</code> <code>point_crs</code> <code>str</code> <p>Coordinate reference system for points (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Response containing the mask output.</p> Example Source code in <code>samgeo/samgeo3.py</code> <pre><code>def add_point_prompts(\n    self,\n    points: List[List[float]],\n    labels: List[int],\n    obj_id: int,\n    frame_idx: int = 0,\n    point_crs: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Add point prompts to segment or refine an object.\n\n    Args:\n        points (List[List[float]]): List of [x, y] point coordinates.\n            In pixel coordinates by default, or in the specified CRS.\n        labels (List[int]): List of labels for each point.\n            1 for positive (include), 0 for negative (exclude).\n        obj_id (int): Object ID to associate with this prompt.\n        frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n        point_crs (str, optional): Coordinate reference system for points\n            (e.g., \"EPSG:4326\"). Only used with GeoTIFF time series.\n\n    Returns:\n        Dict[str, Any]: Response containing the mask output.\n\n    Example:\n        &gt;&gt;&gt; # Add positive point\n        &gt;&gt;&gt; sam.add_point_prompts([[500, 300]], [1], obj_id=1)\n        &gt;&gt;&gt; # Add positive and negative points\n        &gt;&gt;&gt; sam.add_point_prompts([[500, 300], [600, 400]], [1, 0], obj_id=1)\n    \"\"\"\n    import torch\n\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    points = np.array(points)\n\n    # Transform coordinates if CRS is provided\n    if point_crs is not None and self._tif_source is not None:\n        points = common.coords_to_xy(self._tif_source, points, point_crs)\n\n    # Convert to relative coordinates (0-1 range)\n    rel_points = [[x / self.frame_width, y / self.frame_height] for x, y in points]\n\n    points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n    labels_tensor = torch.tensor(labels, dtype=torch.int32)\n\n    response = self.predictor.handle_request(\n        request=dict(\n            type=\"add_prompt\",\n            session_id=self.session_id,\n            frame_index=frame_idx,\n            points=points_tensor,\n            point_labels=labels_tensor,\n            obj_id=obj_id,\n        )\n    )\n\n    return response\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_point_prompts--add-positive-point","title":"Add positive point","text":"<p>sam.add_point_prompts([[500, 300]], [1], obj_id=1)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.add_point_prompts--add-positive-and-negative-points","title":"Add positive and negative points","text":"<p>sam.add_point_prompts([[500, 300], [600, 400]], [1, 0], obj_id=1)</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.close","title":"<code>close()</code>","text":"<p>Close the current session and free GPU resources.</p> <p>Call this when you're done with the current video and want to process a new one, or when you want to free up memory.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the current session and free GPU resources.\n\n    Call this when you're done with the current video and want to\n    process a new one, or when you want to free up memory.\n    \"\"\"\n    if self.session_id is not None:\n        self.predictor.handle_request(\n            request=dict(\n                type=\"close_session\",\n                session_id=self.session_id,\n            )\n        )\n        self.session_id = None\n        print(\"Session closed.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.generate_masks","title":"<code>generate_masks(prompt, frame_idx=0, propagate=True)</code>","text":"<p>Generate masks using a text prompt.</p> <p>This will segment all instances of the described object in the video and optionally track them through all frames.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text description of objects to segment (e.g., \"person\", \"car\").</p> required <code>frame_idx</code> <code>int</code> <p>Frame index to add the prompt on. Defaults to 0.</p> <code>0</code> <code>propagate</code> <code>bool</code> <p>Whether to propagate masks to all frames. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[int, Any]</code> <p>Dict[int, Any]: Dictionary mapping frame index to mask outputs.</p> Example <p>sam.generate_masks(\"building\") sam.generate_masks(\"tree\", frame_idx=10)</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_masks(\n    self,\n    prompt: str,\n    frame_idx: int = 0,\n    propagate: bool = True,\n) -&gt; Dict[int, Any]:\n    \"\"\"Generate masks using a text prompt.\n\n    This will segment all instances of the described object in the video\n    and optionally track them through all frames.\n\n    Args:\n        prompt (str): Text description of objects to segment (e.g., \"person\", \"car\").\n        frame_idx (int): Frame index to add the prompt on. Defaults to 0.\n        propagate (bool): Whether to propagate masks to all frames. Defaults to True.\n\n    Returns:\n        Dict[int, Any]: Dictionary mapping frame index to mask outputs.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"building\")\n        &gt;&gt;&gt; sam.generate_masks(\"tree\", frame_idx=10)\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    # Reset prompts before adding new text prompt\n    self.reset()\n\n    # Add text prompt\n    response = self.predictor.handle_request(\n        request=dict(\n            type=\"add_prompt\",\n            session_id=self.session_id,\n            frame_index=frame_idx,\n            text=prompt,\n        )\n    )\n\n    out = response[\"outputs\"]\n    # Get object IDs - key is 'out_obj_ids' from SAM3 video predictor\n    obj_ids = out.get(\"out_obj_ids\", [])\n    if hasattr(obj_ids, \"tolist\"):\n        obj_ids = obj_ids.tolist()\n    num_objects = len(obj_ids)\n    print(\n        f\"Found {num_objects} object(s) matching '{prompt}' on frame {frame_idx}.\"\n    )\n\n    if propagate:\n        self.propagate()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.init_tracker","title":"<code>init_tracker(frame_idx=0)</code>","text":"<p>Initialize the tracker without detecting any objects.</p> <p>This method initializes SAM3's video tracking state, which is required before using point prompts (add_point_prompts), box prompts (add_box_prompt), or mask prompts (add_mask_prompt).</p> <p>Call this method when you want to: - Use only point or box prompts without text-based detection - Add external masks from other segmentation models - Have full control over which objects to track</p> <p>Parameters:</p> Name Type Description Default <code>frame_idx</code> <code>int</code> <p>Frame index to initialize the tracker on. Defaults to 0.</p> <code>0</code> Example <p>sam = SamGeo3Video() sam.set_video(\"video.mp4\") sam.init_tracker()  # Initialize without detecting anything sam.add_point_prompts([[500, 300]], [1], obj_id=1)  # Add object via point sam.add_box_prompt([100, 100, 200, 150], obj_id=2)  # Add object via box sam.propagate()  # Track through video sam.save_video(\"output.mp4\")</p> Note <ul> <li>This is equivalent to calling generate_masks() with a prompt that   matches no objects, but provides a cleaner API.</li> <li>The tracker must be initialized before adding point/box/mask prompts.</li> <li>After init_tracker(), you can add objects using:</li> <li>add_point_prompts() for point-based segmentation</li> <li>add_box_prompt() for box-based segmentation</li> <li>add_mask_prompt() for external mask tracking</li> </ul> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def init_tracker(self, frame_idx: int = 0) -&gt; None:\n    \"\"\"Initialize the tracker without detecting any objects.\n\n    This method initializes SAM3's video tracking state, which is required\n    before using point prompts (add_point_prompts), box prompts (add_box_prompt),\n    or mask prompts (add_mask_prompt).\n\n    Call this method when you want to:\n    - Use only point or box prompts without text-based detection\n    - Add external masks from other segmentation models\n    - Have full control over which objects to track\n\n    Args:\n        frame_idx (int): Frame index to initialize the tracker on. Defaults to 0.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3Video()\n        &gt;&gt;&gt; sam.set_video(\"video.mp4\")\n        &gt;&gt;&gt; sam.init_tracker()  # Initialize without detecting anything\n        &gt;&gt;&gt; sam.add_point_prompts([[500, 300]], [1], obj_id=1)  # Add object via point\n        &gt;&gt;&gt; sam.add_box_prompt([100, 100, 200, 150], obj_id=2)  # Add object via box\n        &gt;&gt;&gt; sam.propagate()  # Track through video\n        &gt;&gt;&gt; sam.save_video(\"output.mp4\")\n\n    Note:\n        - This is equivalent to calling generate_masks() with a prompt that\n          matches no objects, but provides a cleaner API.\n        - The tracker must be initialized before adding point/box/mask prompts.\n        - After init_tracker(), you can add objects using:\n          - add_point_prompts() for point-based segmentation\n          - add_box_prompt() for box-based segmentation\n          - add_mask_prompt() for external mask tracking\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    # Reset prompts before initializing\n    self.reset()\n\n    # Use a nonsense text prompt that won't match any real objects\n    # This initializes the tracker state without detecting anything\n    _init_prompt = \"__samgeo_init_tracker_placeholder_xyzzy_12345__\"\n\n    response = self.predictor.handle_request(\n        request=dict(\n            type=\"add_prompt\",\n            session_id=self.session_id,\n            frame_index=frame_idx,\n            text=_init_prompt,\n        )\n    )\n\n    # Run propagation to fully initialize the tracker state\n    # This populates cached_frame_outputs which is required for point prompts\n    self.propagate()\n\n    print(\"Tracker initialized. You can now add point, box, or mask prompts.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.propagate","title":"<code>propagate()</code>","text":"<p>Propagate masks through all frames of the video.</p> <p>This tracks the segmented objects from the prompt frame through the entire video.</p> <p>Returns:</p> Type Description <code>Dict[int, Any]</code> <p>Dict[int, Any]: Dictionary mapping frame index to mask outputs.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def propagate(self) -&gt; Dict[int, Any]:\n    \"\"\"Propagate masks through all frames of the video.\n\n    This tracks the segmented objects from the prompt frame through\n    the entire video.\n\n    Returns:\n        Dict[int, Any]: Dictionary mapping frame index to mask outputs.\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    outputs_per_frame = {}\n    for response in self.predictor.handle_stream_request(\n        request=dict(\n            type=\"propagate_in_video\",\n            session_id=self.session_id,\n        )\n    ):\n        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n\n    self.outputs_per_frame = outputs_per_frame\n    print(f\"Propagated masks to {len(outputs_per_frame)} frames.\")\n    return outputs_per_frame\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.remove_object","title":"<code>remove_object(obj_id)</code>","text":"<p>Remove one or more objects from tracking.</p> <p>Parameters:</p> Name Type Description Default <code>obj_id</code> <code>Union[int, List[int]]</code> <p>Object ID(s) to remove. Can be a single int or a list of ints.</p> required Example <p>sam.generate_masks(\"person\")  # Finds 3 people sam.remove_object(2)  # Remove person with ID 2 sam.remove_object([1, 3])  # Remove multiple objects at once sam.propagate()  # Re-propagate without removed objects</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def remove_object(self, obj_id: Union[int, List[int]]) -&gt; None:\n    \"\"\"Remove one or more objects from tracking.\n\n    Args:\n        obj_id (Union[int, List[int]]): Object ID(s) to remove.\n            Can be a single int or a list of ints.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"person\")  # Finds 3 people\n        &gt;&gt;&gt; sam.remove_object(2)  # Remove person with ID 2\n        &gt;&gt;&gt; sam.remove_object([1, 3])  # Remove multiple objects at once\n        &gt;&gt;&gt; sam.propagate()  # Re-propagate without removed objects\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    # Convert single int to list for uniform processing\n    obj_ids = [obj_id] if isinstance(obj_id, int) else obj_id\n\n    for oid in obj_ids:\n        self.predictor.handle_request(\n            request=dict(\n                type=\"remove_object\",\n                session_id=self.session_id,\n                obj_id=oid,\n            )\n        )\n\n    if len(obj_ids) == 1:\n        print(f\"Removed object {obj_ids[0]}.\")\n    else:\n        print(f\"Removed objects {obj_ids}.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.reset","title":"<code>reset()</code>","text":"<p>Reset the current session, clearing all prompts and masks.</p> <p>Use this when you want to start fresh with new prompts on the same video.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the current session, clearing all prompts and masks.\n\n    Use this when you want to start fresh with new prompts on the same video.\n    \"\"\"\n    if self.session_id is None:\n        raise ValueError(\"No session active. Please call set_video() first.\")\n\n    self.predictor.handle_request(\n        request=dict(\n            type=\"reset_session\",\n            session_id=self.session_id,\n        )\n    )\n    self.outputs_per_frame = None\n    print(\"Session reset.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.save_masks","title":"<code>save_masks(output_dir, img_ext='png', dtype='uint8')</code>","text":"<p>Save segmentation masks to files.</p> <p>For GeoTIFF time series, masks are saved with georeferencing information.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save mask files.</p> required <code>img_ext</code> <code>str</code> <p>Image extension for output files. Defaults to \"png\". For GeoTIFF time series, this is overridden to \"tif\".</p> <code>'png'</code> <code>dtype</code> <code>str</code> <p>Data type for mask values. Defaults to \"uint8\".</p> <code>'uint8'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of saved file paths.</p> Example <p>sam.generate_masks(\"building\") sam.save_masks(\"output/masks/\")</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def save_masks(\n    self,\n    output_dir: str,\n    img_ext: str = \"png\",\n    dtype: str = \"uint8\",\n) -&gt; List[str]:\n    \"\"\"Save segmentation masks to files.\n\n    For GeoTIFF time series, masks are saved with georeferencing information.\n\n    Args:\n        output_dir (str): Directory to save mask files.\n        img_ext (str): Image extension for output files. Defaults to \"png\".\n            For GeoTIFF time series, this is overridden to \"tif\".\n        dtype (str): Data type for mask values. Defaults to \"uint8\".\n\n    Returns:\n        List[str]: List of saved file paths.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"building\")\n        &gt;&gt;&gt; sam.save_masks(\"output/masks/\")\n    \"\"\"\n    if self.outputs_per_frame is None:\n        raise ValueError(\"No masks to save. Please run generate_masks() first.\")\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Prepare mask data using our custom formatter\n    formatted_outputs = self._format_outputs()\n\n    if not formatted_outputs:\n        print(\"No masks to save.\")\n        return []\n\n    num_digits = len(str(len(self.video_frames)))\n    saved_files = []\n\n    # Check if we have GeoTIFF source\n    is_geotiff = self._tif_source is not None and self._tif_source.lower().endswith(\n        (\".tif\", \".tiff\")\n    )\n    if is_geotiff:\n        img_ext = \"tif\"\n\n    # Determine frame dimensions once\n    if isinstance(self.video_frames[0], str):\n        first_frame = load_frame(self.video_frames[0])\n        h, w = first_frame.shape[:2]\n    else:\n        h, w = self.video_frames[0].shape[:2]\n\n    for frame_idx in tqdm(sorted(formatted_outputs.keys()), desc=\"Saving masks\"):\n        frame_data = formatted_outputs[frame_idx]\n        mask_array = np.zeros((h, w), dtype=np.uint8)\n\n        # Combine all object masks with unique IDs\n        for obj_id, mask in frame_data.items():\n            mask_np = np.array(mask)\n            if mask_np.ndim &gt; 2:\n                mask_np = mask_np.squeeze()\n            # Resize mask if needed\n            if mask_np.shape != (h, w):\n                mask_np = cv2.resize(\n                    mask_np.astype(np.uint8),\n                    (w, h),\n                    interpolation=cv2.INTER_NEAREST,\n                )\n            mask_array[mask_np &gt; 0] = obj_id\n\n        # Determine output path\n        if is_geotiff and self._tif_names is not None:\n            base_name = os.path.splitext(self._tif_names[frame_idx])[0]\n            filename = f\"{base_name}_mask.{img_ext}\"\n            crs_source = os.path.join(self._tif_dir, self._tif_names[frame_idx])\n        else:\n            filename = f\"{str(frame_idx).zfill(num_digits)}.{img_ext}\"\n            crs_source = None\n\n        output_path = os.path.join(output_dir, filename)\n\n        if is_geotiff:\n            common.array_to_image(mask_array, output_path, crs_source, dtype=dtype)\n        else:\n            img = Image.fromarray(mask_array)\n            img.save(output_path)\n\n        saved_files.append(output_path)\n\n    print(f\"Saved {len(saved_files)} mask files to {output_dir}\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.save_video","title":"<code>save_video(output_path, fps=30, alpha=0.6, dpi=200, frame_stride=1, show_ids=True)</code>","text":"<p>Save segmentation results as a video with blended masks.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to save the output video (MP4).</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video. Defaults to 30.</p> <code>30</code> <code>alpha</code> <code>float</code> <p>Opacity for mask overlay. Defaults to 0.6.</p> <code>0.6</code> <code>dpi</code> <code>int</code> <p>DPI for rendering. Defaults to 200.</p> <code>200</code> <code>frame_stride</code> <code>int</code> <p>Process every nth frame. Defaults to 1.</p> <code>1</code> <code>show_ids</code> <code>Union[bool, Dict[int, str]]</code> <p>Whether to show object IDs on the video. If True, shows numeric IDs. If False, hides IDs. If a dict, maps object IDs to custom labels (e.g., player names). Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved video.</p> Example <p>sam.generate_masks(\"car\") sam.save_video(\"output.mp4\")</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def save_video(\n    self,\n    output_path: str,\n    fps: int = 30,\n    alpha: float = 0.6,\n    dpi: int = 200,\n    frame_stride: int = 1,\n    show_ids: Union[bool, Dict[int, str]] = True,\n) -&gt; str:\n    \"\"\"Save segmentation results as a video with blended masks.\n\n    Args:\n        output_path (str): Path to save the output video (MP4).\n        fps (int): Frames per second for the output video. Defaults to 30.\n        alpha (float): Opacity for mask overlay. Defaults to 0.6.\n        dpi (int): DPI for rendering. Defaults to 200.\n        frame_stride (int): Process every nth frame. Defaults to 1.\n        show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs\n            on the video. If True, shows numeric IDs. If False, hides IDs.\n            If a dict, maps object IDs to custom labels (e.g., player names).\n            Defaults to True.\n\n    Returns:\n        str: Path to the saved video.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"car\")\n        &gt;&gt;&gt; sam.save_video(\"output.mp4\")\n        &gt;&gt;&gt; # With custom labels\n        &gt;&gt;&gt; sam.save_video(\"output.mp4\", show_ids={1: \"Player A\", 2: \"Player B\"})\n    \"\"\"\n    if self.outputs_per_frame is None:\n        raise ValueError(\"No masks to save. Please run generate_masks() first.\")\n\n    # Create temporary directory for frames\n    temp_dir = common.make_temp_dir()\n    os.makedirs(temp_dir, exist_ok=True)\n\n    # Save blended frames\n    self._save_blended_frames(\n        temp_dir,\n        alpha=alpha,\n        dpi=dpi,\n        frame_stride=frame_stride,\n        show_ids=show_ids,\n    )\n\n    # Create video from frames\n    common.images_to_video(temp_dir, output_path, fps=fps)\n    print(f\"Saved video to {output_path}\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.save_video--with-custom-labels","title":"With custom labels","text":"<p>sam.save_video(\"output.mp4\", show_ids={1: \"Player A\", 2: \"Player B\"})</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.set_video","title":"<code>set_video(video_path, output_dir=None, frame_rate=None, prefix='', bands=None)</code>","text":"<p>Load a video or time series images for segmentation.</p> <p>The video can be: - An MP4 video file - A directory of JPEG frames - A directory of GeoTIFF images (for time series remote sensing data)</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file or image directory.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save extracted frames. Only used when video_path is an MP4 file. Defaults to None.</p> <code>None</code> <code>frame_rate</code> <code>int</code> <p>Frame rate for extracting frames from video. Only used when video_path is an MP4 file. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for extracted frame filenames. Defaults to \"\".</p> <code>''</code> <code>bands</code> <code>List[int]</code> <p>List of band indices (1-based) to use for RGB when the input is a GeoTIFF directory with multi-band images. For example, [4, 3, 2] for NIR-R-G false color composite. If None, uses the first 3 bands for multi-band images. Defaults to None.</p> <code>None</code> Example <p>sam = SamGeo3Video() sam.set_video(\"video.mp4\")  # Load MP4 video sam.set_video(\"frames/\")  # Load from JPEG frames directory sam.set_video(\"landsat_ts/\")  # Load GeoTIFF time series sam.set_video(\"landsat_ts/\", bands=[4, 3, 2])  # NIR-R-G composite</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def set_video(\n    self,\n    video_path: str,\n    output_dir: Optional[str] = None,\n    frame_rate: Optional[int] = None,\n    prefix: str = \"\",\n    bands: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"Load a video or time series images for segmentation.\n\n    The video can be:\n    - An MP4 video file\n    - A directory of JPEG frames\n    - A directory of GeoTIFF images (for time series remote sensing data)\n\n    Args:\n        video_path (str): Path to the video file or image directory.\n        output_dir (str, optional): Directory to save extracted frames.\n            Only used when video_path is an MP4 file. Defaults to None.\n        frame_rate (int, optional): Frame rate for extracting frames from video.\n            Only used when video_path is an MP4 file. Defaults to None.\n        prefix (str): Prefix for extracted frame filenames. Defaults to \"\".\n        bands (List[int], optional): List of band indices (1-based) to use for RGB\n            when the input is a GeoTIFF directory with multi-band images. For example,\n            [4, 3, 2] for NIR-R-G false color composite. If None, uses the\n            first 3 bands for multi-band images. Defaults to None.\n\n    Example:\n        &gt;&gt;&gt; sam = SamGeo3Video()\n        &gt;&gt;&gt; sam.set_video(\"video.mp4\")  # Load MP4 video\n        &gt;&gt;&gt; sam.set_video(\"frames/\")  # Load from JPEG frames directory\n        &gt;&gt;&gt; sam.set_video(\"landsat_ts/\")  # Load GeoTIFF time series\n        &gt;&gt;&gt; sam.set_video(\"landsat_ts/\", bands=[4, 3, 2])  # NIR-R-G composite\n    \"\"\"\n    if isinstance(video_path, str):\n        if video_path.startswith(\"http\"):\n            video_path = common.download_file(video_path)\n\n        if os.path.isfile(video_path):\n            # MP4 video file - extract frames\n            if output_dir is None:\n                output_dir = common.make_temp_dir()\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            print(f\"Extracting frames to: {output_dir}\")\n            common.video_to_images(\n                video_path, output_dir, frame_rate=frame_rate, prefix=prefix\n            )\n            video_path = output_dir\n\n        elif os.path.isdir(video_path):\n            files = sorted(os.listdir(video_path))\n            if len(files) == 0:\n                raise ValueError(f\"No files found in {video_path}.\")\n\n            # Check if it's a GeoTIFF directory\n            if files[0].lower().endswith((\".tif\", \".tiff\")):\n                self._tif_source = os.path.join(video_path, files[0])\n                self._tif_dir = video_path\n                self._tif_names = files\n                # Convert GeoTIFFs to JPEGs for SAM3\n                video_path = common.geotiff_to_jpg_batch(video_path, bands=bands)\n                print(f\"Converted GeoTIFFs to JPEGs: {video_path}\")\n\n        if not os.path.exists(video_path):\n            raise ValueError(f\"Input path {video_path} does not exist.\")\n    else:\n        raise ValueError(\"video_path must be a string.\")\n\n    self.video_path = video_path\n\n    # Load frames for visualization\n    self._load_video_frames(video_path)\n\n    # Start a session\n    response = self.predictor.handle_request(\n        request=dict(\n            type=\"start_session\",\n            resource_path=video_path,\n        )\n    )\n    self.session_id = response[\"session_id\"]\n    print(f\"Loaded {len(self.video_frames)} frames. Session started.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.show_frame","title":"<code>show_frame(frame_idx=0, figsize=(12, 8), alpha=0.6, show_ids=True, axis='off', output=None)</code>","text":"<p>Display a single frame with mask overlay.</p> <p>Parameters:</p> Name Type Description Default <code>frame_idx</code> <code>int</code> <p>Frame index to display. Defaults to 0.</p> <code>0</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size. Defaults to (12, 8).</p> <code>(12, 8)</code> <code>alpha</code> <code>float</code> <p>Opacity for mask overlay. Defaults to 0.6.</p> <code>0.6</code> <code>show_ids</code> <code>Union[bool, Dict[int, str]]</code> <p>Whether to show object IDs. If True, shows numeric IDs. If False, hides IDs. If a dict, maps object IDs to custom labels (e.g., player names). Defaults to True.</p> <code>True</code> <code>axis</code> <code>str</code> <p>Axis visibility setting. Defaults to \"off\".</p> <code>'off'</code> <code>output</code> <code>str</code> <p>Path to save the figure. Defaults to None.</p> <code>None</code> Example <p>sam.generate_masks(\"tree\") sam.show_frame(0)  # Show first frame sam.show_frame(50, output=\"frame_50.png\")  # Save frame 50</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_frame(\n    self,\n    frame_idx: int = 0,\n    figsize: Tuple[int, int] = (12, 8),\n    alpha: float = 0.6,\n    show_ids: Union[bool, Dict[int, str]] = True,\n    axis: str = \"off\",\n    output: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Display a single frame with mask overlay.\n\n    Args:\n        frame_idx (int): Frame index to display. Defaults to 0.\n        figsize (Tuple[int, int]): Figure size. Defaults to (12, 8).\n        alpha (float): Opacity for mask overlay. Defaults to 0.6.\n        show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs.\n            If True, shows numeric IDs. If False, hides IDs.\n            If a dict, maps object IDs to custom labels (e.g., player names).\n            Defaults to True.\n        axis (str): Axis visibility setting. Defaults to \"off\".\n        output (str, optional): Path to save the figure. Defaults to None.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"tree\")\n        &gt;&gt;&gt; sam.show_frame(0)  # Show first frame\n        &gt;&gt;&gt; sam.show_frame(50, output=\"frame_50.png\")  # Save frame 50\n        &gt;&gt;&gt; # With custom labels\n        &gt;&gt;&gt; sam.show_frame(0, show_ids={1: \"Player A\", 2: \"Player B\"})\n    \"\"\"\n    if self.outputs_per_frame is None:\n        raise ValueError(\"No masks to show. Please run generate_masks() first.\")\n\n    formatted_outputs = self._format_outputs()\n\n    if frame_idx not in formatted_outputs:\n        print(f\"Frame {frame_idx} not in outputs.\")\n        return\n\n    # Load frame\n    if isinstance(self.video_frames[frame_idx], str):\n        frame = Image.open(self.video_frames[frame_idx])\n    else:\n        frame = Image.fromarray(self.video_frames[frame_idx])\n\n    w_frame, h_frame = frame.size\n\n    fig = plt.figure(figsize=figsize)\n    plt.axis(axis)\n    plt.title(f\"Frame {frame_idx}\")\n    plt.imshow(frame)\n\n    # Overlay masks\n    frame_data = formatted_outputs[frame_idx]\n    cmap = plt.get_cmap(\"tab10\")\n\n    for obj_id, mask in frame_data.items():\n        if isinstance(obj_id, str) and obj_id == \"image\":\n            continue\n\n        color = np.array([*cmap(obj_id % 10)[:3], alpha])\n        mask_np = np.array(mask)\n        if mask_np.ndim &gt; 2:\n            mask_np = mask_np.squeeze()\n\n        # Resize mask if it doesn't match frame dimensions\n        if mask_np.shape != (h_frame, w_frame):\n            mask_np = cv2.resize(\n                mask_np.astype(np.float32),\n                (w_frame, h_frame),\n                interpolation=cv2.INTER_NEAREST,\n            )\n\n        mask_image = mask_np.reshape(h_frame, w_frame, 1) * color.reshape(1, 1, -1)\n        plt.gca().imshow(mask_image)\n\n        # Add object ID label\n        if show_ids:\n            ys, xs = np.where(mask_np &gt; 0)\n            if len(xs) &gt; 0 and len(ys) &gt; 0:\n                cx, cy = np.mean(xs), np.mean(ys)\n                # Determine label text\n                if isinstance(show_ids, dict):\n                    label = show_ids.get(obj_id, str(obj_id))\n                else:\n                    label = str(obj_id)\n                plt.text(\n                    cx,\n                    cy,\n                    label,\n                    color=\"white\",\n                    fontsize=12,\n                    fontweight=\"bold\",\n                    ha=\"center\",\n                    va=\"center\",\n                    bbox=dict(\n                        facecolor=cmap(obj_id % 10)[:3],\n                        alpha=0.7,\n                        edgecolor=\"none\",\n                        pad=2,\n                    ),\n                )\n\n    if output is not None:\n        plt.savefig(output, dpi=150, bbox_inches=\"tight\", pad_inches=0.1)\n        print(f\"Saved frame to {output}\")\n        plt.close(fig)\n    else:\n        plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.show_frame--with-custom-labels","title":"With custom labels","text":"<p>sam.show_frame(0, show_ids={1: \"Player A\", 2: \"Player B\"})</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.show_frames","title":"<code>show_frames(frame_stride=10, ncols=3, figsize_per_frame=(6, 4), alpha=0.6, show_ids=False)</code>","text":"<p>Display multiple frames with mask overlays in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>frame_stride</code> <code>int</code> <p>Show every nth frame. Defaults to 10.</p> <code>10</code> <code>ncols</code> <code>int</code> <p>Number of columns in the grid. Defaults to 3.</p> <code>3</code> <code>figsize_per_frame</code> <code>Tuple[int, int]</code> <p>Size per subplot. Defaults to (6, 4).</p> <code>(6, 4)</code> <code>alpha</code> <code>float</code> <p>Opacity for mask overlay. Defaults to 0.6.</p> <code>0.6</code> <code>show_ids</code> <code>Union[bool, Dict[int, str]]</code> <p>Whether to show object IDs. If True, shows numeric IDs. If False, hides IDs. If a dict, maps object IDs to custom labels (e.g., player names). Defaults to False.</p> <code>False</code> Example <p>sam.generate_masks(\"person\") sam.show_frames(frame_stride=30, ncols=4)</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_frames(\n    self,\n    frame_stride: int = 10,\n    ncols: int = 3,\n    figsize_per_frame: Tuple[int, int] = (6, 4),\n    alpha: float = 0.6,\n    show_ids: Union[bool, Dict[int, str]] = False,\n) -&gt; None:\n    \"\"\"Display multiple frames with mask overlays in a grid.\n\n    Args:\n        frame_stride (int): Show every nth frame. Defaults to 10.\n        ncols (int): Number of columns in the grid. Defaults to 3.\n        figsize_per_frame (Tuple[int, int]): Size per subplot. Defaults to (6, 4).\n        alpha (float): Opacity for mask overlay. Defaults to 0.6.\n        show_ids (Union[bool, Dict[int, str]]): Whether to show object IDs.\n            If True, shows numeric IDs. If False, hides IDs.\n            If a dict, maps object IDs to custom labels (e.g., player names).\n            Defaults to False.\n\n    Example:\n        &gt;&gt;&gt; sam.generate_masks(\"person\")\n        &gt;&gt;&gt; sam.show_frames(frame_stride=30, ncols=4)\n        &gt;&gt;&gt; # With custom labels\n        &gt;&gt;&gt; sam.show_frames(show_ids={1: \"Player A\", 2: \"Player B\"})\n    \"\"\"\n    if self.outputs_per_frame is None:\n        raise ValueError(\"No masks to show. Please run generate_masks() first.\")\n\n    formatted_outputs = self._format_outputs()\n    frame_indices = list(range(0, len(self.video_frames), frame_stride))\n    nrows = (len(frame_indices) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(\n        nrows,\n        ncols,\n        figsize=(figsize_per_frame[0] * ncols, figsize_per_frame[1] * nrows),\n    )\n    if nrows == 1 and ncols == 1:\n        axes = np.array([[axes]])\n    elif nrows == 1 or ncols == 1:\n        axes = axes.reshape(-1)\n\n    axes = np.array(axes).flatten()\n\n    cmap = plt.get_cmap(\"tab10\")\n\n    for i, frame_idx in enumerate(frame_indices):\n        ax = axes[i]\n        ax.axis(\"off\")\n        ax.set_title(f\"Frame {frame_idx}\")\n\n        # Load frame\n        if isinstance(self.video_frames[frame_idx], str):\n            frame = Image.open(self.video_frames[frame_idx])\n        else:\n            frame = Image.fromarray(self.video_frames[frame_idx])\n\n        w_frame, h_frame = frame.size\n        ax.imshow(frame)\n\n        if frame_idx in formatted_outputs:\n            frame_data = formatted_outputs[frame_idx]\n\n            for obj_id, mask in frame_data.items():\n                if isinstance(obj_id, str) and obj_id == \"image\":\n                    continue\n\n                color = np.array([*cmap(obj_id % 10)[:3], alpha])\n                mask_np = np.array(mask)\n                if mask_np.ndim &gt; 2:\n                    mask_np = mask_np.squeeze()\n\n                # Resize mask if it doesn't match frame dimensions\n                if mask_np.shape != (h_frame, w_frame):\n                    mask_np = cv2.resize(\n                        mask_np.astype(np.float32),\n                        (w_frame, h_frame),\n                        interpolation=cv2.INTER_NEAREST,\n                    )\n\n                mask_image = mask_np.reshape(h_frame, w_frame, 1) * color.reshape(\n                    1, 1, -1\n                )\n                ax.imshow(mask_image)\n\n                if show_ids:\n                    ys, xs = np.where(mask_np &gt; 0)\n                    if len(xs) &gt; 0 and len(ys) &gt; 0:\n                        cx, cy = np.mean(xs), np.mean(ys)\n                        # Determine label text\n                        if isinstance(show_ids, dict):\n                            label = show_ids.get(obj_id, str(obj_id))\n                        else:\n                            label = str(obj_id)\n                        ax.text(\n                            cx,\n                            cy,\n                            label,\n                            color=\"white\",\n                            fontsize=10,\n                            fontweight=\"bold\",\n                            ha=\"center\",\n                            va=\"center\",\n                        )\n\n    # Hide unused subplots\n    for j in range(len(frame_indices), len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.show_frames--with-custom-labels","title":"With custom labels","text":"<p>sam.show_frames(show_ids={1: \"Player A\", 2: \"Player B\"})</p>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.show_video","title":"<code>show_video(video_path, embed=True, **kwargs)</code>","text":"<p>Show the video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to video file.</p> required <code>embed</code> <code>bool</code> <p>Whether to embed the video. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to Video.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>IPython.display.Video: The video object.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def show_video(self, video_path: str, embed: bool = True, **kwargs: Any) -&gt; None:\n    \"\"\"Show the video.\n\n    Args:\n        video_path (str): Path to video file.\n        embed (bool, optional): Whether to embed the video. Defaults to True.\n        **kwargs: Additional keyword arguments passed to Video.\n\n    Returns:\n        IPython.display.Video: The video object.\n    \"\"\"\n    from IPython.display import Video\n\n    return Video(video_path, embed=embed, **kwargs)\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.SamGeo3Video.shutdown","title":"<code>shutdown()</code>","text":"<p>Shutdown the predictor and free all GPU resources.</p> <p>Call this when you're completely done with video segmentation. After calling this, you cannot use this instance anymore.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the predictor and free all GPU resources.\n\n    Call this when you're completely done with video segmentation.\n    After calling this, you cannot use this instance anymore.\n    \"\"\"\n    self.close()\n    self.predictor.shutdown()\n    print(\"Predictor shutdown complete.\")\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.draw_box_on_image","title":"<code>draw_box_on_image(image, box, color=(0, 255, 0), thickness=2)</code>","text":"<p>Draw a bounding box on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image or ndarray</code> <p>The image to draw on.</p> required <code>box</code> <code>List[float]</code> <p>Bounding box in XYWH format [x, y, width, height].</p> required <code>color</code> <code>Tuple[int, int, int]</code> <p>RGB color for the box. Default is green.</p> <code>(0, 255, 0)</code> <code>thickness</code> <code>int</code> <p>Line thickness in pixels.</p> <code>2</code> <p>Returns:</p> Type Description <p>PIL.Image.Image: Image with box drawn.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def draw_box_on_image(image, box, color=(0, 255, 0), thickness=2):\n    \"\"\"Draw a bounding box on an image.\n\n    Args:\n        image (PIL.Image.Image or np.ndarray): The image to draw on.\n        box (List[float]): Bounding box in XYWH format [x, y, width, height].\n        color (Tuple[int, int, int]): RGB color for the box. Default is green.\n        thickness (int): Line thickness in pixels.\n\n    Returns:\n        PIL.Image.Image: Image with box drawn.\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Convert numpy array to PIL Image if needed\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    # Make a copy to avoid modifying the original\n    image_copy = image.copy()\n    draw = ImageDraw.Draw(image_copy)\n\n    # Extract box coordinates (XYWH format)\n    x, y, w, h = box\n\n    # Draw rectangle\n    draw.rectangle([x, y, x + w, y + h], outline=color, width=thickness)\n\n    return image_copy\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.generate_colors","title":"<code>generate_colors(n_colors=256, n_samples=5000)</code>","text":"<p>Generate colors for the masks.</p> <p>Parameters:</p> Name Type Description Default <code>n_colors</code> <code>int</code> <p>The number of colors to generate. Defaults to 256.</p> <code>256</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate. Defaults to 5000.</p> <code>5000</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The generated colors in RGB format.</p> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def generate_colors(n_colors: int = 256, n_samples: int = 5000) -&gt; np.ndarray:\n    \"\"\"Generate colors for the masks.\n\n    Args:\n        n_colors (int, optional): The number of colors to generate. Defaults to 256.\n        n_samples (int, optional): The number of samples to generate. Defaults to 5000.\n\n    Returns:\n        np.ndarray: The generated colors in RGB format.\n    \"\"\"\n    # Step 1: Random RGB samples\n    np.random.seed(42)\n    rgb = np.random.rand(n_samples, 3)\n    # Step 2: Convert to LAB for perceptual uniformity\n    # print(f\"Converting {n_samples} RGB samples to LAB color space...\")\n    lab = rgb2lab(rgb.reshape(1, -1, 3)).reshape(-1, 3)\n    # print(\"Conversion to LAB complete.\")\n    # Step 3: k-means clustering in LAB\n    kmeans = KMeans(n_clusters=n_colors, n_init=10)\n    # print(f\"Fitting KMeans with {n_colors} clusters on {n_samples} samples...\")\n    kmeans.fit(lab)\n    # print(\"KMeans fitting complete.\")\n    centers_lab = kmeans.cluster_centers_\n    # Step 4: Convert LAB back to RGB\n    colors_rgb = lab2rgb(centers_lab.reshape(1, -1, 3)).reshape(-1, 3)\n    colors_rgb = np.clip(colors_rgb, 0, 1)\n    return colors_rgb\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.plot_bbox","title":"<code>plot_bbox(img_height, img_width, box, box_format='XYXY', relative_coords=True, color='r', linestyle='solid', text=None, ax=None)</code>","text":"<p>Plot the bounding box on the image.</p> <p>Parameters:</p> Name Type Description Default <code>img_height</code> <code>int</code> <p>The height of the image.</p> required <code>img_width</code> <code>int</code> <p>The width of the image.</p> required <code>box</code> <code>ndarray</code> <p>The bounding box.</p> required <code>box_format</code> <code>str</code> <p>The format of the bounding box.</p> <code>'XYXY'</code> <code>relative_coords</code> <code>bool</code> <p>Whether the coordinates are relative to the image.</p> <code>True</code> <code>color</code> <code>str</code> <p>The color of the bounding box.</p> <code>'r'</code> <code>linestyle</code> <code>str</code> <p>The line style of the bounding box.</p> <code>'solid'</code> <code>text</code> <code>str</code> <p>The text to display in the bounding box.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axis to plot the bounding box on.</p> <code>None</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def plot_bbox(\n    img_height,\n    img_width,\n    box,\n    box_format=\"XYXY\",\n    relative_coords=True,\n    color=\"r\",\n    linestyle=\"solid\",\n    text=None,\n    ax=None,\n):\n    \"\"\"Plot the bounding box on the image.\n\n    Args:\n        img_height (int): The height of the image.\n        img_width (int): The width of the image.\n        box (np.ndarray): The bounding box.\n        box_format (str): The format of the bounding box.\n        relative_coords (bool): Whether the coordinates are relative to the image.\n        color (str): The color of the bounding box.\n        linestyle (str): The line style of the bounding box.\n        text (str): The text to display in the bounding box.\n        ax (matplotlib.axes.Axes, optional): The axis to plot the bounding box on.\n    \"\"\"\n    # Convert box to numpy array if it's a tensor\n    if hasattr(box, \"numpy\"):\n        box = box.numpy()\n    elif hasattr(box, \"cpu\"):\n        box = box.cpu().numpy()\n\n    if box_format == \"XYXY\":\n        x, y, x2, y2 = box\n        w = x2 - x\n        h = y2 - y\n    elif box_format == \"XYWH\":\n        x, y, w, h = box\n    elif box_format == \"CxCyWH\":\n        cx, cy, w, h = box\n        x = cx - w / 2\n        y = cy - h / 2\n    else:\n        raise RuntimeError(f\"Invalid box_format {box_format}\")\n\n    if relative_coords:\n        x *= img_width\n        w *= img_width\n        y *= img_height\n        h *= img_height\n\n    if ax is None:\n        ax = plt.gca()\n    rect = patches.Rectangle(\n        (float(x), float(y)),\n        float(w),\n        float(h),\n        linewidth=1.5,\n        edgecolor=color,\n        facecolor=\"none\",\n        linestyle=linestyle,\n    )\n    ax.add_patch(rect)\n    if text is not None:\n        facecolor = \"w\"\n        ax.text(\n            float(x),\n            float(y) - 5,\n            text,\n            color=color,\n            weight=\"bold\",\n            fontsize=8,\n            bbox={\"facecolor\": facecolor, \"alpha\": 0.75, \"pad\": 2},\n        )\n</code></pre>"},{"location":"samgeo3/#samgeo.samgeo3.plot_mask","title":"<code>plot_mask(mask, color='r', alpha=0.5, ax=None)</code>","text":"<p>Plot the mask on the image.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>The mask to plot.</p> required <code>color</code> <code>str</code> <p>The color of the mask.</p> <code>'r'</code> <code>ax</code> <code>Axes</code> <p>The axis to plot the mask on.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The alpha value for the mask.</p> <code>0.5</code> Source code in <code>samgeo/samgeo3.py</code> <pre><code>def plot_mask(mask, color=\"r\", alpha=0.5, ax=None):\n    \"\"\"Plot the mask on the image.\n\n    Args:\n        mask (np.ndarray): The mask to plot.\n        color (str): The color of the mask.\n        ax (matplotlib.axes.Axes, optional): The axis to plot the mask on.\n        alpha (float): The alpha value for the mask.\n    \"\"\"\n    im_h, im_w = mask.shape\n    mask_img = np.zeros((im_h, im_w, 4), dtype=np.float32)\n    mask_img[..., :3] = to_rgb(color)\n    mask_img[..., 3] = mask * alpha\n    # Use the provided ax or the current axis\n    if ax is None:\n        ax = plt.gca()\n    ax.imshow(mask_img)\n</code></pre>"},{"location":"text_sam/","title":"text_sam module","text":"<p>The LangSAM model for segmenting objects from satellite images using text prompts. The source code is adapted from the https://github.com/luca-medeiros/lang-segment-anything repository. Credits to Luca Medeiros for the original implementation.</p>"},{"location":"text_sam/#samgeo.text_sam.LangSAM","title":"<code>LangSAM</code>","text":"<p>A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>class LangSAM:\n    \"\"\"\n    A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.\n    \"\"\"\n\n    def __init__(self, model_type=\"vit_h\", checkpoint=None):\n        \"\"\"Initialize the LangSAM instance.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the SAM 1\n                models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n                sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n        \"\"\"\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.build_groundingdino()\n        self.build_sam(model_type, checkpoint)\n\n        self.source = None\n        self.image = None\n        self.masks = None\n        self.boxes = None\n        self.phrases = None\n        self.logits = None\n        self.prediction = None\n        self.model_version = \"sam2\"\n\n    def build_sam(self, model_type, checkpoint_url=None):\n        \"\"\"Build the SAM model.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the SAM 1\n                models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n                sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n        \"\"\"\n        sam1_models = [\"vit_h\", \"vit_l\", \"vit_b\"]\n        sam2_models = [\n            \"sam2-hiera-tiny\",\n            \"sam2-hiera-small\",\n            \"sam2-hiera-base-plus\",\n            \"sam2-hiera-large\",\n        ]\n        if model_type in sam1_models:\n            if checkpoint_url is not None:\n                sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n            else:\n                checkpoint_url = SAM_MODELS[model_type]\n                sam = sam_model_registry[model_type]()\n                state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n                sam.load_state_dict(state_dict, strict=True)\n            sam.to(device=self.device)\n            self.sam = SamPredictor(sam)\n            self._sam_version = 1\n        elif model_type in sam2_models:\n            self.sam = SamGeo2(model_id=model_type, device=self.device, automatic=False)\n            self._sam_version = 2\n\n    def build_groundingdino(self):\n        \"\"\"Build the GroundingDINO model.\"\"\"\n        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n        ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n        self.groundingdino = load_model_hf(\n            ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n        )\n\n    def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n        \"\"\"\n        Run the GroundingDINO model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n\n        Returns:\n            tuple: Tuple containing boxes, logits, and phrases.\n        \"\"\"\n\n        image_trans = transform_image(image)\n        boxes, logits, phrases = predict(\n            model=self.groundingdino,\n            image=image_trans,\n            caption=text_prompt,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n            device=self.device,\n        )\n        W, H = image.size\n        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n        return boxes, logits, phrases\n\n    def predict_sam(self, image, boxes):\n        \"\"\"\n        Run the SAM model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            boxes (torch.Tensor): Tensor of bounding boxes.\n\n        Returns:\n            Masks tensor.\n        \"\"\"\n        if self._sam_version == 1:\n            image_array = np.asarray(image)\n            self.sam.set_image(image_array)\n            transformed_boxes = self.sam.transform.apply_boxes_torch(\n                boxes, image_array.shape[:2]\n            )\n            masks, _, _ = self.sam.predict_torch(\n                point_coords=None,\n                point_labels=None,\n                boxes=transformed_boxes.to(self.sam.device),\n                multimask_output=False,\n            )\n            return masks.cpu()\n        elif self._sam_version == 2:\n            if isinstance(self.source, str):\n                self.sam.set_image(self.source)\n            # If no source is set provide PIL image\n            if self.source is None:\n                self.sam.set_image(image)\n            self.sam.boxes = boxes.numpy().tolist()\n            masks, _, _ = self.sam.predict(\n                boxes=boxes.numpy().tolist(),\n                multimask_output=False,\n                return_results=True,\n            )\n            self.masks = masks\n            return masks\n\n    def set_image(self, image):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n    def predict(\n        self,\n        image,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        output=None,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        return_results=False,\n        return_coords=False,\n        detection_filter=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction.\n\n        Parameters:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            output (str, optional): Output path for the prediction. Defaults to None.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            return_results (bool, optional): Whether to return the results. Defaults to False.\n            detection_filter (callable, optional):\n                Callable with box, mask, logit, phrase, and index args returns a boolean.\n                If provided, the function will be called for each detected object.\n                Defaults to None.\n\n        Returns:\n            tuple: Tuple containing masks, boxes, phrases, and logits.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            # Load the georeferenced image\n            with rasterio.open(image) as src:\n                image_np = src.read().transpose(\n                    (1, 2, 0)\n                )  # Convert rasterio image to numpy array\n                self.transform = src.transform  # Save georeferencing information\n                self.crs = src.crs  # Save the Coordinate Reference System\n\n                if self.crs is None:\n                    warnings.warn(\n                        \"The CRS (Coordinate Reference System) \"\n                        \"of input image is None. \"\n                        \"Please define a projection on the input image \"\n                        \"before running segment-geospatial, \"\n                        \"or manually set CRS on result object \"\n                        \"like `sam.crs = 'EPSG:3857'`.\",\n                        UserWarning,\n                    )\n\n                image_pil = Image.fromarray(\n                    image_np[:, :, :3]\n                )  # Convert numpy array to PIL image, excluding the alpha channel\n        else:\n            image_pil = image\n            image_np = np.array(image_pil)\n\n        self.image = image_pil\n\n        boxes, logits, phrases = self.predict_dino(\n            image_pil, text_prompt, box_threshold, text_threshold\n        )\n        masks = torch.tensor([])\n        if len(boxes) &gt; 0:\n            masks = self.predict_sam(image_pil, boxes)\n            # If masks have 4 dimensions and the second dimension is 1 (e.g., [boxes, 1, height, width]),\n            # squeeze that dimension to reduce it to 3 dimensions ([boxes, height, width]).\n            # If boxes = 1, the mask's shape will be [1, height, width] after squeezing.\n            if masks.ndim == 4 and masks.shape[1] == 1:\n                masks = masks.squeeze(1)\n\n        if boxes.nelement() == 0:  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Create an empty mask overlay\n\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            # Validate the detection_filter argument\n            if detection_filter is not None:\n                if not callable(detection_filter):\n                    raise ValueError(\"detection_filter must be callable.\")\n\n                if not len(inspect.signature(detection_filter).parameters) == 5:\n                    raise ValueError(\n                        \"detection_filter required args: \"\n                        \"box, mask, logit, phrase, and index.\"\n                    )\n\n            for i, (box, mask, logit, phrase) in enumerate(\n                zip(boxes, masks, logits, phrases)\n            ):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n\n                # Apply the user-supplied filtering logic if provided\n                if detection_filter is not None:\n                    if not detection_filter(box, mask, logit, phrase, i):\n                        continue\n\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n        self.masks = masks\n        self.boxes = boxes\n        self.phrases = phrases\n        self.logits = logits\n        self.prediction = mask_overlay\n\n        if return_results:\n            return masks, boxes, phrases, logits\n\n        if return_coords:\n            boxlist = []\n            for box in self.boxes:\n                box = box.cpu().numpy()\n                boxlist.append((box[0], box[1]))\n            return boxlist\n\n    def predict_batch(\n        self,\n        images,\n        out_dir,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        merge=True,\n        verbose=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction for a batch of images.\n\n        Parameters:\n            images (list): List of input PIL Images.\n            out_dir (str): Output directory for the prediction.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n        \"\"\"\n\n        import glob\n\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n        if isinstance(images, str):\n            images = list(glob.glob(os.path.join(images, \"*.tif\")))\n            images.sort()\n\n        if not isinstance(images, list):\n            raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n        for i, image in enumerate(images):\n            basename = os.path.splitext(os.path.basename(image))[0]\n            if verbose:\n                print(\n                    f\"Processing image {str(i + 1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n                )\n            output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n            self.predict(\n                image,\n                text_prompt,\n                box_threshold,\n                text_threshold,\n                output=output,\n                mask_multiplier=mask_multiplier,\n                dtype=dtype,\n                save_args=save_args,\n                **kwargs,\n            )\n\n        if merge:\n            output = os.path.join(out_dir, \"merged.tif\")\n            merge_rasters(out_dir, output)\n            if verbose:\n                print(f\"Saved the merged prediction to {output}.\")\n\n    def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n        \"\"\"Save the bounding boxes to a vector file.\n\n        Args:\n            output (str): The path to the output vector file.\n            dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n            **kwargs: Additional arguments for boxes_to_vector().\n        \"\"\"\n\n        if self.boxes is None:\n            print(\"Please run predict() first.\")\n            return\n        else:\n            boxes = self.boxes.tolist()\n            coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n            if output is None:\n                return boxes_to_vector(coords, self.crs, dst_crs, output)\n            else:\n                boxes_to_vector(coords, self.crs, dst_crs, output)\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        cmap=\"viridis\",\n        alpha=0.4,\n        add_boxes=True,\n        box_color=\"r\",\n        box_linewidth=1,\n        title=None,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n            add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n            box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n            box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n            title (str, optional): The title for the image. Defaults to None.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n            kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n        \"\"\"\n\n        import warnings\n\n        import matplotlib.patches as patches\n        import matplotlib.pyplot as plt\n\n        warnings.filterwarnings(\"ignore\")\n\n        anns = self.prediction\n\n        if anns is None:\n            print(\"Please run predict() first.\")\n            return\n        elif len(anns) == 0:\n            print(\"No objects found in the image.\")\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        if add_boxes:\n            for box in self.boxes:\n                # Draw bounding box\n                box = box.cpu().numpy()  # Convert the tensor to a numpy array\n                rect = patches.Rectangle(\n                    (box[0], box[1]),\n                    box[2] - box[0],\n                    box[3] - box[1],\n                    linewidth=box_linewidth,\n                    edgecolor=box_color,\n                    facecolor=\"none\",\n                )\n                plt.gca().add_patch(rect)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n        if title is not None:\n            plt.title(title)\n        plt.axis(axis)\n\n        if output is not None:\n            if blend:\n                plt.savefig(output, **kwargs)\n            else:\n                array_to_image(self.prediction, output, self.source)\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n\n    def region_groups(\n        self,\n        image: Union[str, \"xr.DataArray\", np.ndarray],\n        connectivity: int = 1,\n        min_size: int = 10,\n        max_size: Optional[int] = None,\n        threshold: Optional[int] = None,\n        properties: Optional[List[str]] = None,\n        intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n        out_csv: Optional[str] = None,\n        out_vector: Optional[str] = None,\n        out_image: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[\n        Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n    ]:\n        \"\"\"\n        Segment regions in an image and filter them based on size.\n\n        Args:\n            image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n                path, xarray DataArray, or numpy array.\n            connectivity (int, optional): Connectivity for labeling. Defaults to 1\n                for 4-connectivity. Use 2 for 8-connectivity.\n            min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n            max_size (Optional[int], optional): Maximum size of regions to keep.\n                Defaults to None.\n            threshold (Optional[int], optional): Threshold for filling holes.\n                Defaults to None, which is equal to min_size.\n            properties (Optional[List[str]], optional): List of properties to measure.\n                See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n                Defaults to None.\n            intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n                Intensity image to use for properties. Defaults to None.\n            out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n                Defaults to None.\n            out_vector (Optional[str], optional): Path to save the vector file.\n                Defaults to None.\n            out_image (Optional[str], optional): Path to save the output image.\n                Defaults to None.\n\n        Returns:\n            Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n        \"\"\"\n        return self.sam.region_groups(\n            image,\n            connectivity=connectivity,\n            min_size=min_size,\n            max_size=max_size,\n            threshold=threshold,\n            properties=properties,\n            intensity_image=intensity_image,\n            out_csv=out_csv,\n            out_vector=out_vector,\n            out_image=out_image,\n            **kwargs,\n        )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.__init__","title":"<code>__init__(model_type='vit_h', checkpoint=None)</code>","text":"<p>Initialize the LangSAM instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the SAM 1 models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny, sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large) Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_url</code> <code>str</code> <p>The URL to the checkpoint file. Defaults to None</p> required Source code in <code>samgeo/text_sam.py</code> <pre><code>def __init__(self, model_type=\"vit_h\", checkpoint=None):\n    \"\"\"Initialize the LangSAM instance.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the SAM 1\n            models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n            sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n    \"\"\"\n\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.build_groundingdino()\n    self.build_sam(model_type, checkpoint)\n\n    self.source = None\n    self.image = None\n    self.masks = None\n    self.boxes = None\n    self.phrases = None\n    self.logits = None\n    self.prediction = None\n    self.model_version = \"sam2\"\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_groundingdino","title":"<code>build_groundingdino()</code>","text":"<p>Build the GroundingDINO model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_groundingdino(self):\n    \"\"\"Build the GroundingDINO model.\"\"\"\n    ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n    ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n    ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n    self.groundingdino = load_model_hf(\n        ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n    )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_sam","title":"<code>build_sam(model_type, checkpoint_url=None)</code>","text":"<p>Build the SAM model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the SAM 1 models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny, sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large) Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> required <code>checkpoint_url</code> <code>str</code> <p>The URL to the checkpoint file. Defaults to None</p> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_sam(self, model_type, checkpoint_url=None):\n    \"\"\"Build the SAM model.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the SAM 1\n            models () vit_h, vit_l, vit_b) or SAM 2 models (sam2-hiera-tiny,\n            sam2-hiera-small, sam2-hiera-base-plus, sam2-hiera-large)\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_url (str, optional): The URL to the checkpoint file. Defaults to None\n    \"\"\"\n    sam1_models = [\"vit_h\", \"vit_l\", \"vit_b\"]\n    sam2_models = [\n        \"sam2-hiera-tiny\",\n        \"sam2-hiera-small\",\n        \"sam2-hiera-base-plus\",\n        \"sam2-hiera-large\",\n    ]\n    if model_type in sam1_models:\n        if checkpoint_url is not None:\n            sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n        else:\n            checkpoint_url = SAM_MODELS[model_type]\n            sam = sam_model_registry[model_type]()\n            state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n            sam.load_state_dict(state_dict, strict=True)\n        sam.to(device=self.device)\n        self.sam = SamPredictor(sam)\n        self._sam_version = 1\n    elif model_type in sam2_models:\n        self.sam = SamGeo2(model_id=model_type, device=self.device, automatic=False)\n        self._sam_version = 2\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict","title":"<code>predict(image, text_prompt, box_threshold, text_threshold, output=None, mask_multiplier=255, dtype=np.uint8, save_args={}, return_results=False, return_coords=False, detection_filter=None, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>output</code> <code>str</code> <p>Output path for the prediction. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>uint8</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>return_results</code> <code>bool</code> <p>Whether to return the results. Defaults to False.</p> <code>False</code> <code>detection_filter</code> <code>callable</code> <p>Callable with box, mask, logit, phrase, and index args returns a boolean. If provided, the function will be called for each detected object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing masks, boxes, phrases, and logits.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict(\n    self,\n    image,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    output=None,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    return_results=False,\n    return_coords=False,\n    detection_filter=None,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction.\n\n    Parameters:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        output (str, optional): Output path for the prediction. Defaults to None.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        return_results (bool, optional): Whether to return the results. Defaults to False.\n        detection_filter (callable, optional):\n            Callable with box, mask, logit, phrase, and index args returns a boolean.\n            If provided, the function will be called for each detected object.\n            Defaults to None.\n\n    Returns:\n        tuple: Tuple containing masks, boxes, phrases, and logits.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        # Load the georeferenced image\n        with rasterio.open(image) as src:\n            image_np = src.read().transpose(\n                (1, 2, 0)\n            )  # Convert rasterio image to numpy array\n            self.transform = src.transform  # Save georeferencing information\n            self.crs = src.crs  # Save the Coordinate Reference System\n\n            if self.crs is None:\n                warnings.warn(\n                    \"The CRS (Coordinate Reference System) \"\n                    \"of input image is None. \"\n                    \"Please define a projection on the input image \"\n                    \"before running segment-geospatial, \"\n                    \"or manually set CRS on result object \"\n                    \"like `sam.crs = 'EPSG:3857'`.\",\n                    UserWarning,\n                )\n\n            image_pil = Image.fromarray(\n                image_np[:, :, :3]\n            )  # Convert numpy array to PIL image, excluding the alpha channel\n    else:\n        image_pil = image\n        image_np = np.array(image_pil)\n\n    self.image = image_pil\n\n    boxes, logits, phrases = self.predict_dino(\n        image_pil, text_prompt, box_threshold, text_threshold\n    )\n    masks = torch.tensor([])\n    if len(boxes) &gt; 0:\n        masks = self.predict_sam(image_pil, boxes)\n        # If masks have 4 dimensions and the second dimension is 1 (e.g., [boxes, 1, height, width]),\n        # squeeze that dimension to reduce it to 3 dimensions ([boxes, height, width]).\n        # If boxes = 1, the mask's shape will be [1, height, width] after squeezing.\n        if masks.ndim == 4 and masks.shape[1] == 1:\n            masks = masks.squeeze(1)\n\n    if boxes.nelement() == 0:  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Create an empty mask overlay\n\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        # Validate the detection_filter argument\n        if detection_filter is not None:\n            if not callable(detection_filter):\n                raise ValueError(\"detection_filter must be callable.\")\n\n            if not len(inspect.signature(detection_filter).parameters) == 5:\n                raise ValueError(\n                    \"detection_filter required args: \"\n                    \"box, mask, logit, phrase, and index.\"\n                )\n\n        for i, (box, mask, logit, phrase) in enumerate(\n            zip(boxes, masks, logits, phrases)\n        ):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n\n            # Apply the user-supplied filtering logic if provided\n            if detection_filter is not None:\n                if not detection_filter(box, mask, logit, phrase, i):\n                    continue\n\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n    self.masks = masks\n    self.boxes = boxes\n    self.phrases = phrases\n    self.logits = logits\n    self.prediction = mask_overlay\n\n    if return_results:\n        return masks, boxes, phrases, logits\n\n    if return_coords:\n        boxlist = []\n        for box in self.boxes:\n            box = box.cpu().numpy()\n            boxlist.append((box[0], box[1]))\n        return boxlist\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_batch","title":"<code>predict_batch(images, out_dir, text_prompt, box_threshold, text_threshold, mask_multiplier=255, dtype=np.uint8, save_args={}, merge=True, verbose=True, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of input PIL Images.</p> required <code>out_dir</code> <code>str</code> <p>Output directory for the prediction.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>uint8</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>merge</code> <code>bool</code> <p>Whether to merge the predictions into a single GeoTIFF file. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_batch(\n    self,\n    images,\n    out_dir,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    merge=True,\n    verbose=True,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction for a batch of images.\n\n    Parameters:\n        images (list): List of input PIL Images.\n        out_dir (str): Output directory for the prediction.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n    \"\"\"\n\n    import glob\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(images, str):\n        images = list(glob.glob(os.path.join(images, \"*.tif\")))\n        images.sort()\n\n    if not isinstance(images, list):\n        raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n    for i, image in enumerate(images):\n        basename = os.path.splitext(os.path.basename(image))[0]\n        if verbose:\n            print(\n                f\"Processing image {str(i + 1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n            )\n        output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n        self.predict(\n            image,\n            text_prompt,\n            box_threshold,\n            text_threshold,\n            output=output,\n            mask_multiplier=mask_multiplier,\n            dtype=dtype,\n            save_args=save_args,\n            **kwargs,\n        )\n\n    if merge:\n        output = os.path.join(out_dir, \"merged.tif\")\n        merge_rasters(out_dir, output)\n        if verbose:\n            print(f\"Saved the merged prediction to {output}.\")\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_dino","title":"<code>predict_dino(image, text_prompt, box_threshold, text_threshold)</code>","text":"<p>Run the GroundingDINO model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing boxes, logits, and phrases.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n    \"\"\"\n    Run the GroundingDINO model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n\n    Returns:\n        tuple: Tuple containing boxes, logits, and phrases.\n    \"\"\"\n\n    image_trans = transform_image(image)\n    boxes, logits, phrases = predict(\n        model=self.groundingdino,\n        image=image_trans,\n        caption=text_prompt,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n        device=self.device,\n    )\n    W, H = image.size\n    boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n    return boxes, logits, phrases\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_sam","title":"<code>predict_sam(image, boxes)</code>","text":"<p>Run the SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>boxes</code> <code>Tensor</code> <p>Tensor of bounding boxes.</p> required <p>Returns:</p> Type Description <p>Masks tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_sam(self, image, boxes):\n    \"\"\"\n    Run the SAM model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        boxes (torch.Tensor): Tensor of bounding boxes.\n\n    Returns:\n        Masks tensor.\n    \"\"\"\n    if self._sam_version == 1:\n        image_array = np.asarray(image)\n        self.sam.set_image(image_array)\n        transformed_boxes = self.sam.transform.apply_boxes_torch(\n            boxes, image_array.shape[:2]\n        )\n        masks, _, _ = self.sam.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes.to(self.sam.device),\n            multimask_output=False,\n        )\n        return masks.cpu()\n    elif self._sam_version == 2:\n        if isinstance(self.source, str):\n            self.sam.set_image(self.source)\n        # If no source is set provide PIL image\n        if self.source is None:\n            self.sam.set_image(image)\n        self.sam.boxes = boxes.numpy().tolist()\n        masks, _, _ = self.sam.predict(\n            boxes=boxes.numpy().tolist(),\n            multimask_output=False,\n            return_results=True,\n        )\n        self.masks = masks\n        return masks\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.raster_to_vector","title":"<code>raster_to_vector(image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to use for properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def region_groups(\n    self,\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[\n    Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]\n]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to use for properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    return self.sam.region_groups(\n        image,\n        connectivity=connectivity,\n        min_size=min_size,\n        max_size=max_size,\n        threshold=threshold,\n        properties=properties,\n        intensity_image=intensity_image,\n        out_csv=out_csv,\n        out_vector=out_vector,\n        out_image=out_image,\n        **kwargs,\n    )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.save_boxes","title":"<code>save_boxes(output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the bounding boxes to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output vector file.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional arguments for boxes_to_vector().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Save the bounding boxes to a vector file.\n\n    Args:\n        output (str): The path to the output vector file.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional arguments for boxes_to_vector().\n    \"\"\"\n\n    if self.boxes is None:\n        print(\"Please run predict() first.\")\n        return\n    else:\n        boxes = self.boxes.tolist()\n        coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n        if output is None:\n            return boxes_to_vector(coords, self.crs, dst_crs, output)\n        else:\n            boxes_to_vector(coords, self.crs, dst_crs, output)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.set_image","title":"<code>set_image(image)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required Source code in <code>samgeo/text_sam.py</code> <pre><code>def set_image(self, image):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', cmap='viridis', alpha=0.4, add_boxes=True, box_color='r', box_linewidth=1, title=None, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>cmap</code> <code>str</code> <p>The colormap for the annotations. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.4.</p> <code>0.4</code> <code>add_boxes</code> <code>bool</code> <p>Whether to show the bounding boxes. Defaults to True.</p> <code>True</code> <code>box_color</code> <code>str</code> <p>The color for the bounding boxes. Defaults to \"r\".</p> <code>'r'</code> <code>box_linewidth</code> <code>int</code> <p>The line width for the bounding boxes. Defaults to 1.</p> <code>1</code> <code>title</code> <code>str</code> <p>The title for the image. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional arguments for matplotlib.pyplot.savefig().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    cmap=\"viridis\",\n    alpha=0.4,\n    add_boxes=True,\n    box_color=\"r\",\n    box_linewidth=1,\n    title=None,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n        add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n        box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n        box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n        title (str, optional): The title for the image. Defaults to None.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n        kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n    \"\"\"\n\n    import warnings\n\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n\n    warnings.filterwarnings(\"ignore\")\n\n    anns = self.prediction\n\n    if anns is None:\n        print(\"Please run predict() first.\")\n        return\n    elif len(anns) == 0:\n        print(\"No objects found in the image.\")\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    if add_boxes:\n        for box in self.boxes:\n            # Draw bounding box\n            box = box.cpu().numpy()  # Convert the tensor to a numpy array\n            rect = patches.Rectangle(\n                (box[0], box[1]),\n                box[2] - box[0],\n                box[3] - box[1],\n                linewidth=box_linewidth,\n                edgecolor=box_color,\n                facecolor=\"none\",\n            )\n            plt.gca().add_patch(rect)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n    if title is not None:\n        plt.title(title)\n    plt.axis(axis)\n\n    if output is not None:\n        if blend:\n            plt.savefig(output, **kwargs)\n        else:\n            array_to_image(self.prediction, output, self.source)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_map","title":"<code>show_map(basemap='SATELLITE', out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>leafmap.Map: The map object.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.load_model_hf","title":"<code>load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu')</code>","text":"<p>Loads a model from HuggingFace Model Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Repository ID on HuggingFace Model Hub.</p> required <code>filename</code> <code>str</code> <p>Name of the model file in the repository.</p> required <code>ckpt_config_filename</code> <code>str</code> <p>Name of the config file for the model in the repository.</p> required <code>device</code> <code>str</code> <p>Device to load the model onto. Default is 'cpu'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The loaded model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def load_model_hf(\n    repo_id: str, filename: str, ckpt_config_filename: str, device: str = \"cpu\"\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Loads a model from HuggingFace Model Hub.\n\n    Args:\n        repo_id (str): Repository ID on HuggingFace Model Hub.\n        filename (str): Name of the model file in the repository.\n        ckpt_config_filename (str): Name of the config file for the model in the repository.\n        device (str): Device to load the model onto. Default is 'cpu'.\n\n    Returns:\n        torch.nn.Module: The loaded model.\n    \"\"\"\n\n    cache_config_file = hf_hub_download(\n        repo_id=repo_id,\n        filename=ckpt_config_filename,\n        force_filename=ckpt_config_filename,\n    )\n    args = SLConfig.fromfile(cache_config_file)\n    model = build_model(args)\n    model.to(device)\n    cache_file = hf_hub_download(\n        repo_id=repo_id, filename=filename, force_filename=filename\n    )\n    checkpoint = torch.load(cache_file, map_location=\"cpu\", weights_only=False)\n    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n    model.eval()\n    return model\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.transform_image","title":"<code>transform_image(image)</code>","text":"<p>Transforms an image using standard transformations for image-based models.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL Image to be transformed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The transformed image as a tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def transform_image(image: Image) -&gt; torch.Tensor:\n    \"\"\"\n    Transforms an image using standard transformations for image-based models.\n\n    Args:\n        image (Image): The PIL Image to be transformed.\n\n    Returns:\n        torch.Tensor: The transformed image as a tensor.\n    \"\"\"\n    transform = T.Compose(\n        [\n            T.RandomResize([800], max_size=1333),\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n    image_transformed, _ = transform(image, None)\n    return image_transformed\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use segment-geospatial in a project:</p> <pre><code>import samgeo\n</code></pre> <p>Here is a simple example of using segment-geospatial to generate a segmentation mask from a satellite image:</p> <pre><code>import os\nimport torch\nfrom samgeo import SamGeo, tms_to_geotiff\n\nbbox = [-95.3704, 29.6762, -95.368, 29.6775]\nimage = 'satellite.tif'\ntms_to_geotiff(output=image, bbox=bbox, zoom=20, source='Satellite')\n\nout_dir = os.path.join(os.path.expanduser('~'), 'Downloads')\ncheckpoint = os.path.join(out_dir, 'sam_vit_h_4b8939.pth')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nsam = SamGeo(\n    checkpoint=checkpoint,\n    model_type='vit_h',\n    device=device,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    sam_kwargs=None,\n)\n\nmask = 'segment.tif'\nsam.generate(image, mask)\n\nvector = 'segment.gpkg'\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</code></pre> <p></p>"},{"location":"assets/","title":"Credits","text":"<p>Credits to Khalil Misbah for the original design of the leafmap logo. The samgeo logo is a derivative of the leafmap logo.</p> <p></p>"},{"location":"examples/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport arcpy\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import arcpy import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install dependencies.</p> <p><code>conda create esri::python esri::arcpy conda-forge::segment-geospatial --name geo</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap geo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import overlay_images, tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.hq_sam import SamGeo\nfrom samgeo.common import overlay_images, tms_to_geotiff\n</pre> import leafmap from samgeo.hq_sam import SamGeo from samgeo.common import overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from box prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/detectree2/","title":"Detectree2","text":"In\u00a0[\u00a0]: Copied! <pre># Install PyTorch (adjust for your CUDA version)\n# %pip install torch torchvision\n\n# Install Detectron2\n# %pip install 'git+https://github.com/facebookresearch/detectron2.git'\n\n# Install detectree2\n# %pip install git+https://github.com/PatBall1/detectree2.git\n\n# Install segment-geospatial\n# %pip install segment-geospatial\n</pre> # Install PyTorch (adjust for your CUDA version) # %pip install torch torchvision  # Install Detectron2 # %pip install 'git+https://github.com/facebookresearch/detectron2.git'  # Install detectree2 # %pip install git+https://github.com/PatBall1/detectree2.git  # Install segment-geospatial # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.detectree2 import (\n    TreeCrownDelineator,\n    list_pretrained_models,\n)\n</pre> import leafmap from samgeo.detectree2 import (     TreeCrownDelineator,     list_pretrained_models, ) In\u00a0[\u00a0]: Copied! <pre>models = list_pretrained_models()\nfor name, url in models.items():\n    print(f\"{name}: {url}\")\n</pre> models = list_pretrained_models() for name, url in models.items():     print(f\"{name}: {url}\") In\u00a0[\u00a0]: Copied! <pre>image_url = (\n    \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_image.tif\"\n)\nimage_path = leafmap.download_file(image_url)\n</pre> image_url = (     \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_image.tif\" ) image_path = leafmap.download_file(image_url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Tree Image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Tree Image\") m In\u00a0[\u00a0]: Copied! <pre>delineator = TreeCrownDelineator(\n    model_name=\"default\",  # Options: 'paracou', 'sepilok', 'danum', 'default'\n    confidence_threshold=0.5,  # Minimum confidence for predictions\n    nms_threshold=0.3,  # Non-maximum suppression threshold\n)\n</pre> delineator = TreeCrownDelineator(     model_name=\"default\",  # Options: 'paracou', 'sepilok', 'danum', 'default'     confidence_threshold=0.5,  # Minimum confidence for predictions     nms_threshold=0.3,  # Non-maximum suppression threshold ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"tree_crowns.gpkg\"\n\ncrowns = delineator.predict(\n    image_path=image_path,\n    output_path=output_path,\n    tile_width=20,  # Tile width in meters\n    tile_height=20,  # Tile height in meters\n    buffer=30,  # Buffer around tiles in meters\n    simplify_tolerance=0.2,  # Simplify crown geometries\n    min_confidence=0.3,  # Minimum confidence to keep\n    iou_threshold=0.6,  # IoU threshold for removing overlaps\n)\n</pre> output_path = \"tree_crowns.gpkg\"  crowns = delineator.predict(     image_path=image_path,     output_path=output_path,     tile_width=20,  # Tile width in meters     tile_height=20,  # Tile height in meters     buffer=30,  # Buffer around tiles in meters     simplify_tolerance=0.2,  # Simplify crown geometries     min_confidence=0.3,  # Minimum confidence to keep     iou_threshold=0.6,  # IoU threshold for removing overlaps ) In\u00a0[\u00a0]: Copied! <pre>print(f\"Detected {len(crowns)} tree crowns\")\ncrowns.head()\n</pre> print(f\"Detected {len(crowns)} tree crowns\") crowns.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Tree Image\")\nm.add_vector(\n    output_path,\n    layer_name=\"Tree Crowns\",\n    style={\"color\": \"yellow\", \"fillColor\": \"yellow\", \"fillOpacity\": 0.3, \"weight\": 2},\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Tree Image\") m.add_vector(     output_path,     layer_name=\"Tree Crowns\",     style={\"color\": \"yellow\", \"fillColor\": \"yellow\", \"fillOpacity\": 0.3, \"weight\": 2}, ) m In\u00a0[\u00a0]: Copied! <pre># Initialize with a custom model\n# delineator = TreeCrownDelineator(\n#     model_path=\"path/to/your/model.pth\",\n#     confidence_threshold=0.5,\n# )\n</pre> # Initialize with a custom model # delineator = TreeCrownDelineator( #     model_path=\"path/to/your/model.pth\", #     confidence_threshold=0.5, # ) In\u00a0[\u00a0]: Copied! <pre># Tile an orthomosaic for prediction\n# tiles_dir = tile_orthomosaic(\n#     image_path=\"path/to/orthomosaic.tif\",\n#     output_dir=\"./tiles\",\n#     tile_width=40,\n#     tile_height=40,\n#     buffer=30,\n#     mode=\"rgb\",  # Use 'ms' for multispectral imagery\n# )\n</pre> # Tile an orthomosaic for prediction # tiles_dir = tile_orthomosaic( #     image_path=\"path/to/orthomosaic.tif\", #     output_dir=\"./tiles\", #     tile_width=40, #     tile_height=40, #     buffer=30, #     mode=\"rgb\",  # Use 'ms' for multispectral imagery # ) In\u00a0[\u00a0]: Copied! <pre># Prepare training and test data\n# train_dir, test_dir = prepare_training_data(\n#     image_path=\"path/to/orthomosaic.tif\",\n#     crowns_path=\"path/to/manual_crowns.gpkg\",\n#     output_dir=\"./training_data\",\n#     tile_width=40,\n#     tile_height=40,\n#     buffer=30,\n#     threshold=0.6,  # Minimum crown coverage per tile\n#     test_fraction=0.15,  # Fraction for testing\n# )\n</pre> # Prepare training and test data # train_dir, test_dir = prepare_training_data( #     image_path=\"path/to/orthomosaic.tif\", #     crowns_path=\"path/to/manual_crowns.gpkg\", #     output_dir=\"./training_data\", #     tile_width=40, #     tile_height=40, #     buffer=30, #     threshold=0.6,  # Minimum crown coverage per tile #     test_fraction=0.15,  # Fraction for testing # ) In\u00a0[\u00a0]: Copied! <pre># Stitch tile predictions into a single crown map\n# crowns = stitch_predictions(\n#     geo_predictions_dir=\"./predictions_geo\",\n#     output_path=\"final_crowns.gpkg\",\n#     iou_threshold=0.6,\n#     min_confidence=0.5,\n#     simplify_tolerance=0.3,\n# )\n</pre> # Stitch tile predictions into a single crown map # crowns = stitch_predictions( #     geo_predictions_dir=\"./predictions_geo\", #     output_path=\"final_crowns.gpkg\", #     iou_threshold=0.6, #     min_confidence=0.5, #     simplify_tolerance=0.3, # )"},{"location":"examples/detectree2/#tree-crown-delineation-with-detectree2","title":"Tree Crown Delineation with detectree2\u00b6","text":"<p>This notebook demonstrates how to use the detectree2 module for automatic tree crown delineation in aerial RGB imagery using Mask R-CNN.</p>"},{"location":"examples/detectree2/#overview","title":"Overview\u00b6","text":"<p>detectree2 is a Python package for automatic tree crown delineation based on the Detectron2 implementation of Mask R-CNN. It has been designed to delineate trees in challenging dense tropical forests and has been validated across various forest types.</p> <p>Reference: Ball, J.G.C., et al. (2023). Accurate delineation of individual tree crowns in tropical forests from aerial RGB imagery using Mask R-CNN. Remote Sens Ecol Conserv. 9(5):641-655. https://doi.org/10.1002/rse2.332</p>"},{"location":"examples/detectree2/#installation","title":"Installation\u00b6","text":"<p>First, install the required packages. detectree2 requires PyTorch and Detectron2 to be installed first.</p>"},{"location":"examples/detectree2/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/detectree2/#list-available-pre-trained-models","title":"List Available Pre-trained Models\u00b6","text":"<p>detectree2 provides several pre-trained models from different forest types:</p>"},{"location":"examples/detectree2/#download-sample-image","title":"Download Sample Image\u00b6","text":"<p>Download a sample tree image for testing:</p>"},{"location":"examples/detectree2/#visualize-the-sample-image","title":"Visualize the Sample Image\u00b6","text":""},{"location":"examples/detectree2/#initialize-the-tree-crown-delineator","title":"Initialize the Tree Crown Delineator\u00b6","text":"<p>Initialize the <code>TreeCrownDelineator</code> with a pre-trained model. The model will be automatically downloaded on first use.</p>"},{"location":"examples/detectree2/#predict-tree-crowns","title":"Predict Tree Crowns\u00b6","text":"<p>Run the prediction on the sample image:</p>"},{"location":"examples/detectree2/#examine-the-results","title":"Examine the Results\u00b6","text":""},{"location":"examples/detectree2/#visualize-results-on-map","title":"Visualize Results on Map\u00b6","text":"<p>Display the detected tree crowns overlaid on the original image:</p>"},{"location":"examples/detectree2/#using-a-custom-model","title":"Using a Custom Model\u00b6","text":"<p>If you have trained your own model, you can use it instead of the pre-trained models:</p>"},{"location":"examples/detectree2/#tiling-orthomosaics","title":"Tiling Orthomosaics\u00b6","text":"<p>For large orthomosaics, you may want to tile them first before running predictions:</p>"},{"location":"examples/detectree2/#preparing-training-data","title":"Preparing Training Data\u00b6","text":"<p>If you want to train a custom model, you'll need to prepare your training data with manually delineated crown polygons:</p>"},{"location":"examples/detectree2/#stitching-predictions","title":"Stitching Predictions\u00b6","text":"<p>After running predictions on tiles, you can stitch them together:</p>"},{"location":"examples/detectree2/#tips-for-best-results","title":"Tips for Best Results\u00b6","text":"<ol> <li><p>Image Resolution: detectree2 works best with high-resolution imagery (10cm or better).</p> </li> <li><p>Tile Size: The default tile size of 40x40 meters works well for most applications. Adjust based on your tree sizes.</p> </li> <li><p>Buffer Size: A buffer of 30 meters helps handle trees at tile edges.</p> </li> <li><p>Confidence Threshold: Start with 0.5 and adjust based on your precision/recall needs.</p> </li> <li><p>Training Data: If training custom models, ensure your manual crowns are:</p> <ul> <li>Densely clustered (not scattered)</li> <li>Comprehensively labeled (no missing trees in labeled areas)</li> <li>Accurately delineated</li> </ul> </li> <li><p>GPU: For faster predictions, use a CUDA-enabled GPU.</p> </li> </ol>"},{"location":"examples/detectree2/#references","title":"References\u00b6","text":"<ul> <li>detectree2 GitHub Repository</li> <li>detectree2 Documentation</li> <li>Pre-trained Models (Model Garden)</li> <li>Sample Data</li> </ul>"},{"location":"examples/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/image_captioning/","title":"Image captioning","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>from samgeo.caption import ImageCaptioner, blip_analyze_image, show_image\n</pre> from samgeo.caption import ImageCaptioner, blip_analyze_image, show_image In\u00a0[\u00a0]: Copied! <pre>captioner = ImageCaptioner(\n    blip_model_name=\"Salesforce/blip-image-captioning-base\",\n    spacy_model_name=\"en_core_web_sm\",\n)\n</pre> captioner = ImageCaptioner(     blip_model_name=\"Salesforce/blip-image-captioning-base\",     spacy_model_name=\"en_core_web_sm\", ) In\u00a0[\u00a0]: Copied! <pre>url1 = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/caption-building.webp\"\nshow_image(url1)\n</pre> url1 = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/caption-building.webp\" show_image(url1) In\u00a0[\u00a0]: Copied! <pre>caption, features = captioner.analyze(url1)\nprint(f\"Caption: {caption}\")\nprint(f\"Features: {features}\")\n</pre> caption, features = captioner.analyze(url1) print(f\"Caption: {caption}\") print(f\"Features: {features}\") In\u00a0[\u00a0]: Copied! <pre>caption, features = captioner.analyze(url1, include_features=\"default\")\nprint(f\"Caption: {caption}\")\nprint(f\"Aerial Features: {features}\")\n</pre> caption, features = captioner.analyze(url1, include_features=\"default\") print(f\"Caption: {caption}\") print(f\"Aerial Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># Look only for specific features\ncaption, features = captioner.analyze(\n    url1, include_features=[\"building\", \"parking_lot\", \"road\", \"car\", \"tree\"]\n)\nprint(f\"Caption: {caption}\")\nprint(f\"Custom Features: {features}\")\n</pre> # Look only for specific features caption, features = captioner.analyze(     url1, include_features=[\"building\", \"parking_lot\", \"road\", \"car\", \"tree\"] ) print(f\"Caption: {caption}\") print(f\"Custom Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># Exclude certain features from results\ncaption, features = captioner.analyze(url1, exclude_features=[\"view\", \"image\"])\nprint(f\"Caption: {caption}\")\nprint(f\"Features (excluding 'view', 'image'): {features}\")\n</pre> # Exclude certain features from results caption, features = captioner.analyze(url1, exclude_features=[\"view\", \"image\"]) print(f\"Caption: {caption}\") print(f\"Features (excluding 'view', 'image'): {features}\") In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/caption-traffic-sign.webp\"\nshow_image(url2)\n</pre> url2 = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/caption-traffic-sign.webp\" show_image(url2) In\u00a0[\u00a0]: Copied! <pre>caption, features = captioner.analyze(url2)\nprint(f\"Caption: {caption}\")\nprint(f\"Features: {features}\")\n</pre> caption, features = captioner.analyze(url2) print(f\"Caption: {caption}\") print(f\"Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># Using aerial vocabulary\ncaption, features = captioner.analyze(url2, include_features=\"default\")\nprint(f\"Caption: {caption}\")\nprint(f\"Aerial Features: {features}\")\n</pre> # Using aerial vocabulary caption, features = captioner.analyze(url2, include_features=\"default\") print(f\"Caption: {caption}\") print(f\"Aerial Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># Generate caption only\ncaption = captioner.generate_caption(url1)\nprint(f\"Caption: {caption}\")\n</pre> # Generate caption only caption = captioner.generate_caption(url1) print(f\"Caption: {caption}\") In\u00a0[\u00a0]: Copied! <pre># Extract features from an existing caption\nfeatures = captioner.extract_features(caption)\nprint(f\"All Features: {features}\")\n\naerial_features = captioner.extract_features(caption, include_features=\"default\")\nprint(f\"Aerial Features: {aerial_features}\")\n</pre> # Extract features from an existing caption features = captioner.extract_features(caption) print(f\"All Features: {features}\")  aerial_features = captioner.extract_features(caption, include_features=\"default\") print(f\"Aerial Features: {aerial_features}\") In\u00a0[\u00a0]: Copied! <pre># Quick analysis with default models\ncaption, features = blip_analyze_image(url1)\nprint(f\"Caption: {caption}\")\nprint(f\"Features: {features}\")\n</pre> # Quick analysis with default models caption, features = blip_analyze_image(url1) print(f\"Caption: {caption}\") print(f\"Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># Using a larger BLIP model for potentially better captions\ncaption, features = blip_analyze_image(\n    url1,\n    include_features=\"default\",\n    blip_model_name=\"Salesforce/blip-image-captioning-large\",\n)\nprint(f\"Caption (large model): {caption}\")\nprint(f\"Aerial Features: {features}\")\n</pre> # Using a larger BLIP model for potentially better captions caption, features = blip_analyze_image(     url1,     include_features=\"default\",     blip_model_name=\"Salesforce/blip-image-captioning-large\", ) print(f\"Caption (large model): {caption}\") print(f\"Aerial Features: {features}\")"},{"location":"examples/image_captioning/#image-captioning","title":"Image Captioning\u00b6","text":"<p>This notebook demonstrates how to perform image captioning and feature extraction using the BLIP model and spaCy NLP processing. The <code>ImageCaptioner</code> class provides a convenient interface for:</p> <ul> <li>Generating captions for images from local files or URLs</li> <li>Extracting meaningful features (nouns) from captions</li> <li>Filtering features using predefined aerial vocabulary or custom lists</li> </ul>"},{"location":"examples/image_captioning/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/image_captioning/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/image_captioning/#initialize-the-imagecaptioner","title":"Initialize the ImageCaptioner\u00b6","text":"<p>Create an <code>ImageCaptioner</code> instance. You can customize the models used:</p> <ul> <li><code>blip_model_name</code>: The BLIP model for caption generation (default: <code>\"Salesforce/blip-image-captioning-base\"</code>)</li> <li><code>spacy_model_name</code>: The spaCy model for NLP processing (default: <code>\"en_core_web_sm\"</code>)</li> <li><code>device</code>: The device to run inference on (<code>\"cuda\"</code>, <code>\"mps\"</code>, or <code>\"cpu\"</code>). Auto-detected if not specified.</li> </ul> <p>Available BLIP models:</p> <ul> <li><code>Salesforce/blip-image-captioning-base</code> (default, ~990MB)</li> <li><code>Salesforce/blip-image-captioning-large</code> (larger, more accurate, ~1.9GB)</li> </ul>"},{"location":"examples/image_captioning/#example-1-building-image","title":"Example 1: Building Image\u00b6","text":"<p>Let's analyze an aerial image of a building.</p>"},{"location":"examples/image_captioning/#basic-analysis","title":"Basic Analysis\u00b6","text":"<p>Use the <code>analyze()</code> method to generate a caption and extract all noun features.</p>"},{"location":"examples/image_captioning/#using-aerial-features-vocabulary","title":"Using Aerial Features Vocabulary\u00b6","text":"<p>Set <code>include_features=\"default\"</code> to filter features using a predefined aerial/geospatial vocabulary available here. This helps identify features relevant to remote sensing and aerial imagery analysis.</p>"},{"location":"examples/image_captioning/#custom-feature-filtering","title":"Custom Feature Filtering\u00b6","text":"<p>You can also provide a custom list of features to look for, and exclude specific features.</p>"},{"location":"examples/image_captioning/#example-2-traffic-sign-image","title":"Example 2: Traffic Sign Image\u00b6","text":"<p>Let's analyze a different type of image - a traffic sign.</p>"},{"location":"examples/image_captioning/#using-individual-methods","title":"Using Individual Methods\u00b6","text":"<p>The <code>ImageCaptioner</code> class also provides individual methods for more granular control:</p> <ul> <li><code>generate_caption()</code>: Generate only the caption</li> <li><code>extract_features()</code>: Extract features from an existing caption</li> </ul>"},{"location":"examples/image_captioning/#using-the-convenience-function","title":"Using the Convenience Function\u00b6","text":"<p>For quick one-off analyses, you can use the <code>blip_analyze_image()</code> function directly without creating an <code>ImageCaptioner</code> instance. You can also specify custom models.</p>"},{"location":"examples/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.hq_sam import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo.hq_sam import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import raster_to_vector, overlay_images\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/sam2_automatic/","title":"Sam2 automatic","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.6768, -95.3692], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.6768, -95.3692], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True)\n</pre> sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image)\n</pre> sam2.generate(image) In\u00a0[\u00a0]: Copied! <pre>sam2.save_masks(output=\"masks.tif\")\n</pre> sam2.save_masks(output=\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"binary_r\")\n</pre> sam2.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7)\nm\n</pre> m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7) m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.gpkg\", layer_name=\"Objects\")\n</pre> m.add_vector(\"masks.gpkg\", layer_name=\"Objects\") In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    apply_postprocessing=False,\n    points_per_side=32,\n    points_per_batch=64,\n    pred_iou_thresh=0.7,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.7,\n    crop_n_layers=1,\n    box_nms_thresh=0.7,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=25.0,\n    use_m2m=True,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     apply_postprocessing=False,     points_per_side=32,     points_per_batch=64,     pred_iou_thresh=0.7,     stability_score_thresh=0.92,     stability_score_offset=0.7,     crop_n_layers=1,     box_nms_thresh=0.7,     crop_n_points_downscale_factor=2,     min_mask_region_area=25.0,     use_m2m=True, ) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image, output=\"masks2.tif\")\n</pre> sam2.generate(image, output=\"masks2.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations2.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations2.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", )"},{"location":"examples/sam2_automatic/#automatic-mask-generation-with-sam-2","title":"Automatic Mask Generation with SAM 2\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model 2 (SAM2) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_automatic/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_automatic/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_automatic/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/sam2_automatic/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/sam2_automatic/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/sam2_automatic/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/sam2_box_prompts/","title":"Sam2 box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\nfrom samgeo.common import raster_to_vector, regularize\n</pre> import leafmap from samgeo import SamGeo2 from samgeo.common import raster_to_vector, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-117.5995, 47.6518, -117.5988, 47.652],\n        [-117.5987, 47.6518, -117.5979, 47.652],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-117.5995, 47.6518, -117.5988, 47.652],         [-117.5987, 47.6518, -117.5979, 47.652],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\"\ngeojson = \"building_bboxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\" geojson = \"building_bboxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict(     boxes=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_vector = \"building_vector.geojson\"\nraster_to_vector(output_masks, output_vector)\n</pre> output_vector = \"building_vector.geojson\" raster_to_vector(output_masks, output_vector) In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(output_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(output_vector, output_regularized) In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\n</pre> m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) <p></p>"},{"location":"examples/sam2_box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from box prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/sam2_box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/sam2_box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/sam2_box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/sam2_box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/sam2_box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/sam2_box_prompts/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/sam2_box_prompts/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/sam2_point_prompts/","title":"Sam2 point prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\nfrom samgeo.common import regularize\n</pre> import leafmap from samgeo import SamGeo2 from samgeo.common import regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    point_coords_batch = m.user_rois\nelse:\n    point_coords_batch = [\n        [-117.599896, 47.655345],\n        [-117.59992, 47.655167],\n        [-117.599928, 47.654974],\n        [-117.599518, 47.655337],\n    ]\n</pre> if m.user_rois is not None:     point_coords_batch = m.user_rois else:     point_coords_batch = [         [-117.599896, 47.655345],         [-117.59992, 47.655167],         [-117.599928, 47.654974],         [-117.599518, 47.655337],     ] <p>Segment the objects using the point prompts and save the output masks.</p> In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=point_coords_batch,\n    point_crs=\"EPSG:4326\",\n    output=\"mask.tif\",\n    dtype=\"uint8\",\n)\n</pre> sam.predict_by_points(     point_coords_batch=point_coords_batch,     point_crs=\"EPSG:4326\",     output=\"mask.tif\",     dtype=\"uint8\", ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n</pre> geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\" <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.add_circle_markers_from_xy(\n    geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.add_circle_markers_from_xy(     geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8 ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict_by_points(     point_coords_batch=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>out_vector = \"building_vector.geojson\"\nout_image = \"buildings.tif\"\n</pre> out_vector = \"building_vector.geojson\" out_image = \"buildings.tif\" In\u00a0[\u00a0]: Copied! <pre>array, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() <p></p> In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(out_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(out_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\")\nm.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\") m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/sam2_point_prompts/#segmenting-remote-sensing-imagery-with-point-prompts","title":"Segmenting remote sensing imagery with point prompts\u00b6","text":"<p>This notebook shows how to generate object masks from point prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_point_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_point_prompts/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam2_point_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_point_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"examples/sam2_point_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/sam2_point_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict_by_points()</code> method to segment the image with specified point coordinates. You can use the draw tools to add place markers on the map. If no point is added, the default sample points will be used.</p>"},{"location":"examples/sam2_point_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/sam2_point_prompts/#use-an-existing-vector-dataset-as-points-prompts","title":"Use an existing vector dataset as points prompts\u00b6","text":"<p>Alternatively, you can specify a file path or HTTP URL to a vector dataset containing point geometries.</p>"},{"location":"examples/sam2_point_prompts/#segment-image-with-a-vector-dataset","title":"Segment image with a vector dataset\u00b6","text":"<p>Segment the image using the specified file path to the vector dataset.</p>"},{"location":"examples/sam2_point_prompts/#clean-up-the-result","title":"Clean up the result\u00b6","text":"<p>Remove small objects from the segmented masks, fill holes, and compute geometric properties.</p>"},{"location":"examples/sam2_point_prompts/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"examples/sam2_point_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/sam2_predictor/","title":"Sam2 predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15) m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to use the predictor mode rather than the automatic mode.</p> In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.set_image(image)\n</pre> sam2.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam2.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam2.show_map()\nm\n</pre> m = sam2.show_map() m <p></p>"},{"location":"examples/sam2_predictor/#generating-object-masks-from-input-prompts-with-sam-2","title":"Generating object masks from input prompts with SAM 2\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_predictor/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_predictor/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/sam2_predictor/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/sam2_predictor/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/sam2_text_prompts/","title":"Sam2 text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM(model_type=\"sam2-hiera-large\")\n</pre> sam = LangSAM(model_type=\"sam2-hiera-large\") In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam.region_groups(\n    image=\"trees.tif\",\n    min_size=100,\n    out_csv=\"objects.csv\",\n    out_image=\"objects.tif\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam.region_groups(     image=\"trees.tif\",     min_size=100,     out_csv=\"objects.csv\",     out_image=\"objects.tif\",     out_vector=\"objects.gpkg\", ) <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/sam2_text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-2-sam-2","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model 2 (SAM 2)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/sam2_text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/sam2_text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/sam2_text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/sam2_text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/sam2_text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/sam2_text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/sam2_video/","title":"Sam2 video","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U segment-geospatial\n</pre> # %pip install -U segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url)\n</pre> leafmap.download_file(url) In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>video_path = \"landsat_ts\"\npredictor.set_video(video_path)\n</pre> video_path = \"landsat_ts\" predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[1582, 933], [1287, 905], [1473, 998]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[1582, 933], [1287, 905], [1473, 998]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p>Althernatively, prompts can be provided in lon/lat coordinates. The model will automatically convert the lon/lat coordinates to pixel coordinates when the <code>point_crs</code> parameter is set to the coordinate reference system of the lon/lat coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\npredictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\")\n</pre> prompts = {     1: {         \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } predictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video()\n</pre> predictor.predict_video() In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments(\"segments\")\n</pre> predictor.save_video_segments(\"segments\") <p>To save the results as blended images and MP4 video:</p> In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\n    \"blended\", fps=5, output_video=\"segments_blended.mp4\"\n)\n</pre> predictor.save_video_segments_blended(     \"blended\", fps=5, output_video=\"segments_blended.mp4\" ) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" In\u00a0[\u00a0]: Copied! <pre>video_path = url\npredictor.set_video(video_path)\n</pre> video_path = url predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[335, 203]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n    2: {\n        \"points\": [[420, 201]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[335, 203]],         \"labels\": [1],         \"frame_idx\": 0,     },     2: {         \"points\": [[420, 201]],         \"labels\": [1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video(prompts)\n</pre> predictor.predict_video(prompts) In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25)\n</pre> predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25) <p></p>"},{"location":"examples/sam2_video/#segmenting-objects-from-timeseries-images-with-sam-2","title":"Segmenting objects from timeseries images with SAM 2\u00b6","text":"<p>This notebook shows how to segment objects from timeseries with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam2_video/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam2_video/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam2_video/#download-sample-data","title":"Download sample data\u00b6","text":"<p>For now, SamGeo2 supports remote sensing data in the form of RGB images, 8-bit integer. Make sure all images are in the same width and height.</p>"},{"location":"examples/sam2_video/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/sam2_video/#specify-the-input-data","title":"Specify the input data\u00b6","text":"<p>Point to the directory containing the images or the video file.</p>"},{"location":"examples/sam2_video/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":"<p>The prompts can be points and boxes. The points are represented as a list of tuples, where each tuple contains the x and y coordinates of the point. The boxes are represented as a list of tuples, where each tuple contains the x, y, width, and height of the box.</p>"},{"location":"examples/sam2_video/#segment-the-objects","title":"Segment the objects\u00b6","text":""},{"location":"examples/sam2_video/#save-results","title":"Save results\u00b6","text":"<p>To save the results as gray-scale GeoTIFFs with the same georeference as the input images:</p>"},{"location":"examples/sam2_video/#segment-the-objects-from-a-video","title":"Segment the objects from a video\u00b6","text":""},{"location":"examples/sam3_automated_segmentation/","title":"Sam3 automated segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\nfrom samgeo.caption import ImageCaptioner\n</pre> import leafmap from samgeo import SamGeo3, download_file from samgeo.caption import ImageCaptioner In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\"\nimage_path = download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\" image_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Satellite image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Satellite image\") m In\u00a0[\u00a0]: Copied! <pre>captioner = ImageCaptioner(\n    blip_model_name=\"Salesforce/blip-image-captioning-base\",\n    spacy_model_name=\"en_core_web_sm\",\n)\n</pre> captioner = ImageCaptioner(     blip_model_name=\"Salesforce/blip-image-captioning-base\",     spacy_model_name=\"en_core_web_sm\", ) In\u00a0[\u00a0]: Copied! <pre>caption, features = captioner.analyze(image_path)\nprint(f\"Caption: {caption}\")\nprint(f\"Features: {features}\")\n</pre> caption, features = captioner.analyze(image_path) print(f\"Caption: {caption}\") print(f\"Features: {features}\") In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import login\n# login()\n</pre> # from huggingface_hub import login # login() In\u00a0[\u00a0]: Copied! <pre>sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True)\n</pre> sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True) In\u00a0[\u00a0]: Copied! <pre>sam3.set_image(image_path)\n</pre> sam3.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre>feature = features[0]\nsam3.generate_masks(prompt=feature)\n</pre> feature = features[0] sam3.generate_masks(prompt=feature) In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks()\n</pre> sam3.show_masks() In\u00a0[\u00a0]: Copied! <pre># Save masks with unique values for each object\n# Since uc_berkeley.tif is a GeoTIFF, the output will also be a GeoTIFF\nsam3.save_masks(output=f\"{feature}_masks.tif\", unique=True)\n</pre> # Save masks with unique values for each object # Since uc_berkeley.tif is a GeoTIFF, the output will also be a GeoTIFF sam3.save_masks(output=f\"{feature}_masks.tif\", unique=True) In\u00a0[\u00a0]: Copied! <pre># Save as binary mask (all foreground pixels are 255)\nsam3.save_masks(output=f\"{feature}_masks_binary.tif\", unique=False)\n</pre> # Save as binary mask (all foreground pixels are 255) sam3.save_masks(output=f\"{feature}_masks_binary.tif\", unique=False) In\u00a0[\u00a0]: Copied! <pre># Save masks and confidence scores\n# Each pixel in the scores image will have the confidence value of its mask\nsam3.save_masks(\n    output=f\"{feature}_masks_with_scores.tif\",\n    save_scores=f\"{feature}_scores.tif\",\n    unique=True,\n)\n</pre> # Save masks and confidence scores # Each pixel in the scores image will have the confidence value of its mask sam3.save_masks(     output=f\"{feature}_masks_with_scores.tif\",     save_scores=f\"{feature}_scores.tif\",     unique=True, ) In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks(cmap=\"coolwarm\")\n</pre> sam3.show_masks(cmap=\"coolwarm\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(f\"{feature}_masks.tif\", layer_name=f\"{feature} masks\", visible=False)\nm.add_raster(\n    f\"{feature}_scores.tif\",\n    layer_name=f\"{feature} scores\",\n    cmap=\"coolwarm\",\n    opacity=0.8,\n    nodata=0,\n)\nm\n</pre> m.add_raster(f\"{feature}_masks.tif\", layer_name=f\"{feature} masks\", visible=False) m.add_raster(     f\"{feature}_scores.tif\",     layer_name=f\"{feature} scores\",     cmap=\"coolwarm\",     opacity=0.8,     nodata=0, ) m"},{"location":"examples/sam3_automated_segmentation/#automated-segmentation-of-remote-sensing-imagery-with-sam-3","title":"Automated Segmentation of Remote Sensing Imagery with SAM 3\u00b6","text":"<p>In this notebook, we demonstrate the automated segmentation of remote sensing imagery using SAM 3. The process begins with image captioning, which automatically identifies key features within the image. These features can then be utilized as text prompts for SAM 3, enabling precise segmentation.</p>"},{"location":"examples/sam3_automated_segmentation/#installation","title":"Installation\u00b6","text":"<p>First, make sure you have the required dependencies installed:</p>"},{"location":"examples/sam3_automated_segmentation/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_automated_segmentation/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering the University of California, Berkeley, for testing:</p>"},{"location":"examples/sam3_automated_segmentation/#image-captioning","title":"Image captioning\u00b6","text":""},{"location":"examples/sam3_automated_segmentation/#request-access-to-sam3","title":"Request access to SAM3\u00b6","text":"<p>To use SAM3, you need to request access by filling out this form on Hugging Face: https://huggingface.co/facebook/sam3</p> <p>Once you have access, uncomment the following code block and run it.</p>"},{"location":"examples/sam3_automated_segmentation/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>When initializing SAM3, you can choose the backend from \"meta\", or \"transformers\".</p>"},{"location":"examples/sam3_automated_segmentation/#set-the-image","title":"Set the image\u00b6","text":"<p>You can set the image by either passing the image path or the image URL.</p>"},{"location":"examples/sam3_automated_segmentation/#generate-masks-with-text-prompt","title":"Generate masks with text prompt\u00b6","text":""},{"location":"examples/sam3_automated_segmentation/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_automated_segmentation/#save-masks","title":"Save Masks\u00b6","text":"<p>Save the generated masks to a file. If the input is a GeoTIFF, the output will be a GeoTIFF with the same georeferencing. Otherwise, it will be saved as PNG.</p>"},{"location":"examples/sam3_automated_segmentation/#save-masks-with-confidence-scores","title":"Save Masks with Confidence Scores\u00b6","text":"<p>You can also save the confidence scores for each mask. The scores indicate the model's confidence for each predicted mask.</p>"},{"location":"examples/sam3_automated_segmentation/#visualize-confidence-scores","title":"Visualize Confidence Scores\u00b6","text":"<p>Let's visualize the confidence scores to see which masks have higher confidence:</p>"},{"location":"examples/sam3_batch_segmentation/","title":"Sam3 batch segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\n</pre> import leafmap from samgeo import SamGeo3, download_file In\u00a0[\u00a0]: Copied! <pre>image_paths = []\nfor i in range(1, 5):\n    url = f\"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley_{i}.tif\"\n    image_path = download_file(url)\n    image_paths.append(image_path)\n</pre> image_paths = [] for i in range(1, 5):     url = f\"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley_{i}.tif\"     image_path = download_file(url)     image_paths.append(image_path) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nfor i, image_path in enumerate(image_paths):\n    m.add_raster(image_path, layer_name=f\"image_{i + 1}\")\nm\n</pre> m = leafmap.Map() for i, image_path in enumerate(image_paths):     m.add_raster(image_path, layer_name=f\"image_{i + 1}\") m In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import login\n# login()\n</pre> # from huggingface_hub import login # login() In\u00a0[\u00a0]: Copied! <pre>sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True)\n</pre> sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True) In\u00a0[\u00a0]: Copied! <pre>sam3.set_image_batch(image_paths)\n</pre> sam3.set_image_batch(image_paths) In\u00a0[\u00a0]: Copied! <pre>sam3.generate_masks_batch(\"building\", min_size=100)\n</pre> sam3.generate_masks_batch(\"building\", min_size=100) In\u00a0[\u00a0]: Copied! <pre># Access results for each image\nfor i, result in enumerate(sam3.batch_results):\n    print(f\"Image {i + 1}: Found {len(result['masks'])} objects\")\n    # result contains: masks, boxes, scores, image, source\n</pre> # Access results for each image for i, result in enumerate(sam3.batch_results):     print(f\"Image {i + 1}: Found {len(result['masks'])} objects\")     # result contains: masks, boxes, scores, image, source In\u00a0[\u00a0]: Copied! <pre># Visualize all annotations in a grid\nsam3.show_anns_batch(ncols=2, show_bbox=True, show_score=True)\n</pre> # Visualize all annotations in a grid sam3.show_anns_batch(ncols=2, show_bbox=True, show_score=True) In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns_batch(output_dir=\"output/annotations/\", prefix=\"ann\", dpi=300)\n</pre> sam3.show_anns_batch(output_dir=\"output/annotations/\", prefix=\"ann\", dpi=300) In\u00a0[\u00a0]: Copied! <pre># Save all masks to disk\nsaved_files = sam3.save_masks_batch(\n    output_dir=\"output/\", prefix=\"building_mask\", unique=True\n)\n</pre> # Save all masks to disk saved_files = sam3.save_masks_batch(     output_dir=\"output/\", prefix=\"building_mask\", unique=True )"},{"location":"examples/sam3_batch_segmentation/#batch-segmentation-for-remote-sensing-imagery-with-sam-3","title":"Batch Segmentation for Remote Sensing Imagery with SAM 3\u00b6","text":"<p>This notebook demonstrates how to do batch segmentation for remote sensing imagery with SAM 3.</p>"},{"location":"examples/sam3_batch_segmentation/#installation","title":"Installation\u00b6","text":"<p>First, make sure you have the required dependencies installed:</p>"},{"location":"examples/sam3_batch_segmentation/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_batch_segmentation/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download sample satellite images covering the University of California, Berkeley, for testing:</p>"},{"location":"examples/sam3_batch_segmentation/#request-access-to-sam3","title":"Request access to SAM3\u00b6","text":"<p>To use SAM3, you need to request access by filling out this form on Hugging Face: https://huggingface.co/facebook/sam3</p> <p>Once you have access, uncomment the following code block and run it.</p>"},{"location":"examples/sam3_batch_segmentation/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>When initializing SAM3, you can choose the backend from \"meta\", or \"transformers\".</p>"},{"location":"examples/sam3_batch_segmentation/#set-the-image-batch","title":"Set the image batch\u00b6","text":""},{"location":"examples/sam3_batch_segmentation/#generate-masks-with-text-prompt","title":"Generate masks with text prompt\u00b6","text":"<p>Generate masks for all images with a text prompt</p>"},{"location":"examples/sam3_batch_segmentation/#show-results","title":"Show results\u00b6","text":""},{"location":"examples/sam3_batch_segmentation/#save-results","title":"Save results\u00b6","text":""},{"location":"examples/sam3_box_prompts/","title":"Sam3 box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\nfrom samgeo.common import raster_to_vector, regularize\n</pre> import leafmap from samgeo import SamGeo3, download_file from samgeo.common import raster_to_vector, regularize In\u00a0[\u00a0]: Copied! <pre>image_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_image.tif\"\nimage = download_file(image_url)\n</pre> image_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_image.tif\" image = download_file(image_url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Satellite image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Satellite image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n</pre> sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-117.5995, 47.6518, -117.5988, 47.652],\n        [-117.5987, 47.6518, -117.5979, 47.652],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-117.5995, 47.6518, -117.5988, 47.652],         [-117.5987, 47.6518, -117.5979, 47.652],     ] In\u00a0[\u00a0]: Copied! <pre>sam.generate_masks_by_boxes_inst(boxes=boxes, box_crs=\"EPSG:4326\")\n</pre> sam.generate_masks_by_boxes_inst(boxes=boxes, box_crs=\"EPSG:4326\") <p>Save the masks to a file.</p> In\u00a0[\u00a0]: Copied! <pre>sam.save_masks(output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.save_masks(output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.5, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.5, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\"\ngeojson = \"building_bboxes.geojson\"\ndownload_file(url, geojson)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\" geojson = \"building_bboxes.geojson\" download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\") m In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate_masks_by_boxes_inst(\n    boxes=geojson,\n    box_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint16\",\n    multimask_output=False,\n)\n</pre> sam.generate_masks_by_boxes_inst(     boxes=geojson,     box_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint16\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\" ) m In\u00a0[\u00a0]: Copied! <pre>output_vector = \"building_vector.geojson\"\nraster_to_vector(output_masks, output_vector)\n</pre> output_vector = \"building_vector.geojson\" raster_to_vector(output_masks, output_vector) In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(output_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(output_vector, output_regularized) In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\n</pre> m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None )"},{"location":"examples/sam3_box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts-and-sam-3","title":"Segmenting remote sensing imagery with box prompts and SAM 3\u00b6","text":"<p>This notebook shows how to generate object masks from box prompts with the Segment Anything Model 3 (SAM 3).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam3_box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam3_box_prompts/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam3_box_prompts/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering Washington State:</p>"},{"location":"examples/sam3_box_prompts/#initialize-sam-3","title":"Initialize SAM 3\u00b6","text":"<p>To use point and box prompts (SAM1-style interactive segmentation), initialize SAM3 with <code>enable_inst_interactivity=True</code>.</p>"},{"location":"examples/sam3_box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> <p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/sam3_box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>generate_masks_by_boxes_inst()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax], ...].</p>"},{"location":"examples/sam3_box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/sam3_box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/sam3_box_prompts/#segment-image-with-box-prompts-from-vector-file","title":"Segment image with box prompts from vector file\u00b6","text":"<p>The <code>generate_masks_by_boxes_inst()</code> method can directly accept a file path to a vector file (GeoJSON, Shapefile, etc.). It will automatically extract bounding boxes from geometries and filter out any boxes outside the image bounds.</p>"},{"location":"examples/sam3_box_prompts/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/sam3_box_prompts/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/sam3_image_segmentation/","title":"Sam3 image segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\n</pre> import leafmap from samgeo import SamGeo3, download_file In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\"\nimage_path = download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\" image_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Satellite image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Satellite image\") m In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import login\n# login()\n</pre> # from huggingface_hub import login # login() In\u00a0[\u00a0]: Copied! <pre>sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True)\n</pre> sam3 = SamGeo3(backend=\"meta\", device=None, checkpoint_path=None, load_from_HF=True) In\u00a0[\u00a0]: Copied! <pre>sam3.set_image(image_path)\n</pre> sam3.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre>sam3.generate_masks(prompt=\"building\")\n</pre> sam3.generate_masks(prompt=\"building\") In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks()\n</pre> sam3.show_masks() In\u00a0[\u00a0]: Copied! <pre># Define boxes in [xmin, ymin, xmax, ymax] format\nboxes = [[-122.2597, 37.8709, -122.2587, 37.8717]]\n\n# Optional: specify which boxes are positive/negative prompts\nbox_labels = [True]  # True=include, False=exclude\n\n# Generate masks\nsam3.generate_masks_by_boxes(boxes, box_labels, box_crs=\"EPSG:4326\")\n</pre> # Define boxes in [xmin, ymin, xmax, ymax] format boxes = [[-122.2597, 37.8709, -122.2587, 37.8717]]  # Optional: specify which boxes are positive/negative prompts box_labels = [True]  # True=include, False=exclude  # Generate masks sam3.generate_masks_by_boxes(boxes, box_labels, box_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>sam3.show_boxes(boxes, box_labels, box_crs=\"EPSG:4326\")\n</pre> sam3.show_boxes(boxes, box_labels, box_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre># Save masks with unique values for each object\n# Since the input image is a GeoTIFF, the output will also be a GeoTIFF\nsam3.save_masks(output=\"building_masks.tif\", unique=True)\n</pre> # Save masks with unique values for each object # Since the input image is a GeoTIFF, the output will also be a GeoTIFF sam3.save_masks(output=\"building_masks.tif\", unique=True) In\u00a0[\u00a0]: Copied! <pre># Save as binary mask (all foreground pixels are 255)\nsam3.save_masks(output=\"building_masks_binary.tif\", unique=False)\n</pre> # Save as binary mask (all foreground pixels are 255) sam3.save_masks(output=\"building_masks_binary.tif\", unique=False) In\u00a0[\u00a0]: Copied! <pre># Save masks and confidence scores\n# Each pixel in the scores image will have the confidence value of its mask\nsam3.save_masks(\n    output=\"building_masks_with_scores.tif\",\n    save_scores=\"building_scores.tif\",\n    unique=True,\n)\n</pre> # Save masks and confidence scores # Each pixel in the scores image will have the confidence value of its mask sam3.save_masks(     output=\"building_masks_with_scores.tif\",     save_scores=\"building_scores.tif\",     unique=True, ) In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks(cmap=\"coolwarm\")\n</pre> sam3.show_masks(cmap=\"coolwarm\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"building_masks.tif\", layer_name=\"Building masks\", visible=False)\nm.add_raster(\n    \"building_scores.tif\",\n    layer_name=\"Building scores\",\n    cmap=\"coolwarm\",\n    opacity=0.8,\n    nodata=0,\n)\nm\n</pre> m.add_raster(\"building_masks.tif\", layer_name=\"Building masks\", visible=False) m.add_raster(     \"building_scores.tif\",     layer_name=\"Building scores\",     cmap=\"coolwarm\",     opacity=0.8,     nodata=0, ) m"},{"location":"examples/sam3_image_segmentation/#sam3-image-segmentation-for-remote-sensing","title":"SAM3 Image Segmentation for Remote Sensing\u00b6","text":"<p>This notebook demonstrates how to use the Segment Anything Model 3 (SAM3) for segmenting remote sensing images using the <code>samgeo3</code> module.</p>"},{"location":"examples/sam3_image_segmentation/#installation","title":"Installation\u00b6","text":"<p>First, make sure you have the required dependencies installed:</p>"},{"location":"examples/sam3_image_segmentation/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_image_segmentation/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering the University of California, Berkeley, for testing:</p>"},{"location":"examples/sam3_image_segmentation/#request-access-to-sam3","title":"Request access to SAM3\u00b6","text":"<p>To use SAM3, you need to request access by filling out this form on Hugging Face: https://huggingface.co/facebook/sam3</p> <p>Once you have access, uncomment the following code block and run it.</p>"},{"location":"examples/sam3_image_segmentation/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>When initializing SAM3, you can choose the backend from \"meta\", or \"transformers\".</p>"},{"location":"examples/sam3_image_segmentation/#set-the-image","title":"Set the image\u00b6","text":"<p>You can set the image by either passing the image path or the image URL.</p>"},{"location":"examples/sam3_image_segmentation/#generate-masks-with-text-prompt","title":"Generate masks with text prompt\u00b6","text":""},{"location":"examples/sam3_image_segmentation/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_image_segmentation/#generate-masks-by-bounding-boxes","title":"Generate masks by bounding boxes\u00b6","text":""},{"location":"examples/sam3_image_segmentation/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_image_segmentation/#save-masks","title":"Save Masks\u00b6","text":"<p>Save the generated masks to a file. If the input is a GeoTIFF, the output will be a GeoTIFF with the same georeferencing. Otherwise, it will be saved as PNG.</p>"},{"location":"examples/sam3_image_segmentation/#save-masks-with-confidence-scores","title":"Save Masks with Confidence Scores\u00b6","text":"<p>You can also save the confidence scores for each mask. The scores indicate the model's confidence for each predicted mask.</p>"},{"location":"examples/sam3_image_segmentation/#visualize-confidence-scores","title":"Visualize Confidence Scores\u00b6","text":"<p>Let's visualize the confidence scores to see which masks have higher confidence:</p>"},{"location":"examples/sam3_image_segmentation_jpg/","title":"Sam3 image segmentation jpg","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre># %pip install transformers==5.0.0rc0\n</pre> # %pip install transformers==5.0.0rc0 In\u00a0[\u00a0]: Copied! <pre>from samgeo import SamGeo3, download_file\nfrom samgeo.common import show_image\n</pre> from samgeo import SamGeo3, download_file from samgeo.common import show_image In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/facebookresearch/sam3/refs/heads/main/assets/images/test_image.jpg\"\nimage_path = download_file(url)\n</pre> url = \"https://raw.githubusercontent.com/facebookresearch/sam3/refs/heads/main/assets/images/test_image.jpg\" image_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>show_image(image_path)\n</pre> show_image(image_path) In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import login\n# login()\n</pre> # from huggingface_hub import login # login() In\u00a0[\u00a0]: Copied! <pre>sam3 = SamGeo3(\n    backend=\"transformers\", device=None, checkpoint_path=None, load_from_HF=True\n)\n</pre> sam3 = SamGeo3(     backend=\"transformers\", device=None, checkpoint_path=None, load_from_HF=True ) In\u00a0[\u00a0]: Copied! <pre>sam3.set_image(image_path)\n</pre> sam3.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre>sam3.generate_masks(prompt=\"person\")\n</pre> sam3.generate_masks(prompt=\"person\") In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks()\n</pre> sam3.show_masks() In\u00a0[\u00a0]: Copied! <pre># Define boxes in [xmin, ymin, xmax, ymax] format\nboxes = [[480, 290, 590, 650]]\n\n# Optional: specify which boxes are positive/negative prompts\nbox_labels = [True]  # True=include, False=exclude\n\n# Generate masks\nsam3.generate_masks_by_boxes(boxes, box_labels)\n</pre> # Define boxes in [xmin, ymin, xmax, ymax] format boxes = [[480, 290, 590, 650]]  # Optional: specify which boxes are positive/negative prompts box_labels = [True]  # True=include, False=exclude  # Generate masks sam3.generate_masks_by_boxes(boxes, box_labels) In\u00a0[\u00a0]: Copied! <pre>sam3.show_boxes(boxes, box_labels)\n</pre> sam3.show_boxes(boxes, box_labels) In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre># Save as binary mask (all foreground pixels are 255)\nsam3.save_masks(output=\"person_masks_binary.png\", unique=False)\n</pre> # Save as binary mask (all foreground pixels are 255) sam3.save_masks(output=\"person_masks_binary.png\", unique=False) In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks(cmap=\"coolwarm\")\n</pre> sam3.show_masks(cmap=\"coolwarm\")"},{"location":"examples/sam3_image_segmentation_jpg/#sam3-image-segmentation-for-jpg-images","title":"SAM3 Image Segmentation for JPG images\u00b6","text":"<p>This notebook demonstrates how to use the Segment Anything Model 3 (SAM3) for segmenting JPG images using the <code>samgeo3</code> module.</p>"},{"location":"examples/sam3_image_segmentation_jpg/#installation","title":"Installation\u00b6","text":"<p>First, make sure you have the required dependencies installed:</p>"},{"location":"examples/sam3_image_segmentation_jpg/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_image_segmentation_jpg/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering the University of California, Berkeley, for testing:</p>"},{"location":"examples/sam3_image_segmentation_jpg/#request-access-to-sam3","title":"Request access to SAM3\u00b6","text":"<p>To use SAM3, you need to request access by filling out this form on Hugging Face: https://huggingface.co/facebook/sam3</p> <p>Once you have access, uncomment the following code block and run it.</p>"},{"location":"examples/sam3_image_segmentation_jpg/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>When initializing SAM3, you can choose the backend from \"meta\", or \"transformers\".</p>"},{"location":"examples/sam3_image_segmentation_jpg/#set-the-image","title":"Set the image\u00b6","text":"<p>You can set the image by either passing the image path or the image URL.</p>"},{"location":"examples/sam3_image_segmentation_jpg/#generate-masks-with-text-prompt","title":"Generate masks with text prompt\u00b6","text":""},{"location":"examples/sam3_image_segmentation_jpg/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_image_segmentation_jpg/#generate-masks-by-bounding-boxes","title":"Generate masks by bounding boxes\u00b6","text":""},{"location":"examples/sam3_image_segmentation_jpg/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_image_segmentation_jpg/#save-masks","title":"Save Masks\u00b6","text":"<p>Save the generated masks to a file. If the input is a GeoTIFF, the output will be a GeoTIFF with the same georeferencing. Otherwise, it will be saved as PNG.</p>"},{"location":"examples/sam3_interactive/","title":"Sam3 interactive","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre># %pip install transformers==5.0.0rc0\n</pre> # %pip install transformers==5.0.0rc0 In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\n</pre> import leafmap from samgeo import SamGeo3, download_file In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\"\nimage_path = download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif\" image_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Satellite image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Satellite image\") m In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import login\n# login()\n</pre> # from huggingface_hub import login # login() In\u00a0[\u00a0]: Copied! <pre>sam3 = SamGeo3(\n    backend=\"transformers\", device=None, checkpoint_path=None, load_from_HF=True\n)\n</pre> sam3 = SamGeo3(     backend=\"transformers\", device=None, checkpoint_path=None, load_from_HF=True ) In\u00a0[\u00a0]: Copied! <pre>sam3.set_image(image_path)\n</pre> sam3.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre>sam3.generate_masks(prompt=\"building\")\n</pre> sam3.generate_masks(prompt=\"building\") In\u00a0[\u00a0]: Copied! <pre>sam3.save_masks(\"masks.tif\")\n</pre> sam3.save_masks(\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam3.show_anns()\n</pre> sam3.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam3.show_masks()\n</pre> sam3.show_masks() In\u00a0[\u00a0]: Copied! <pre>sam3.show_map(height=\"700px\", min_size=10)\n</pre> sam3.show_map(height=\"700px\", min_size=10) <p>Segmentation by text prompts.</p> <p></p> <p>Segmentation by bounding boxes.</p> <p></p>"},{"location":"examples/sam3_interactive/#interactive-segmentation-with-sam3","title":"Interactive Segmentation with SAM3\u00b6","text":"<p>This notebook demonstrates how to segment remote sensing images interactively using the Segment Anything Model 3 (SAM3).</p>"},{"location":"examples/sam3_interactive/#installation","title":"Installation\u00b6","text":"<p>First, make sure you have the required dependencies installed:</p>"},{"location":"examples/sam3_interactive/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_interactive/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering the University of California, Berkeley, for testing:</p>"},{"location":"examples/sam3_interactive/#request-access-to-sam3","title":"Request access to SAM3\u00b6","text":"<p>To use SAM3, you need to request access by filling out this form on Hugging Face: https://huggingface.co/facebook/sam3</p> <p>Once you have access, uncomment the following code block and run it.</p>"},{"location":"examples/sam3_interactive/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>When initializing SAM3, you can choose the backend from \"meta\", or \"transformers\".</p>"},{"location":"examples/sam3_interactive/#set-the-image","title":"Set the image\u00b6","text":"<p>You can set the image by either passing the image path or the image URL.</p>"},{"location":"examples/sam3_interactive/#generate-masks-with-text-prompt","title":"Generate masks with text prompt\u00b6","text":""},{"location":"examples/sam3_interactive/#show-the-results","title":"Show the results\u00b6","text":""},{"location":"examples/sam3_interactive/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Enter a text prompt or draw a rectangle on the map, then click on the Segment button to perform instance segmentation.</p>"},{"location":"examples/sam3_object_tracking/","title":"Sam3 object tracking","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import os\nfrom samgeo import SamGeo3Video, download_file\n</pre> import os from samgeo import SamGeo3Video, download_file In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3Video()\n</pre> sam = SamGeo3Video() In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/basketball.mp4\"\nvideo_path = download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/basketball.mp4\" video_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>sam.set_video(video_path)\n</pre> sam.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>sam.show_video(video_path)\n</pre> sam.show_video(video_path) In\u00a0[\u00a0]: Copied! <pre># Segment all players in the video\nsam.generate_masks(\"player\")\n</pre> # Segment all players in the video sam.generate_masks(\"player\") In\u00a0[\u00a0]: Copied! <pre>player_names = {}\nfor i in range(15):\n    player_names[i] = f\"Player {i}\"\nsam.show_frame(0, axis=\"on\", show_ids=player_names)\n</pre> player_names = {} for i in range(15):     player_names[i] = f\"Player {i}\" sam.show_frame(0, axis=\"on\", show_ids=player_names) In\u00a0[\u00a0]: Copied! <pre># Remove objects and re-propagate\nsam.remove_object(obj_id=[5, 8, 12, 13])\nsam.propagate()\nsam.show_frame(0, show_ids=player_names)\n</pre> # Remove objects and re-propagate sam.remove_object(obj_id=[5, 8, 12, 13]) sam.propagate() sam.show_frame(0, show_ids=player_names) In\u00a0[\u00a0]: Copied! <pre>os.makedirs(\"output\", exist_ok=True)\n\n# Save mask images\nsam.save_masks(\"output/masks\")\n</pre> os.makedirs(\"output\", exist_ok=True)  # Save mask images sam.save_masks(\"output/masks\") In\u00a0[\u00a0]: Copied! <pre># Save video with blended masks\nsam.save_video(\"output/players_segmented.mp4\", fps=60, show_ids=player_names)\n</pre> # Save video with blended masks sam.save_video(\"output/players_segmented.mp4\", fps=60, show_ids=player_names) In\u00a0[\u00a0]: Copied! <pre>sam.show_video(\"output/players_segmented.mp4\")\n</pre> sam.show_video(\"output/players_segmented.mp4\") In\u00a0[\u00a0]: Copied! <pre>sam.close()\n</pre> sam.close() <p>To completely shutdown and free all resources:</p> In\u00a0[\u00a0]: Copied! <pre>sam.shutdown()\n</pre> sam.shutdown()"},{"location":"examples/sam3_object_tracking/#video-segmentation-and-object-tracking-with-sam-3","title":"Video Segmentation and Object Tracking with SAM 3\u00b6","text":"<p>This notebook demonstrates how to use SAM 3 for video segmentation and object tracking.</p>"},{"location":"examples/sam3_object_tracking/#installation","title":"Installation\u00b6","text":"<p>SAM 3 requires CUDA-capable GPU. Install with:</p>"},{"location":"examples/sam3_object_tracking/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_object_tracking/#initialize-video-predictor","title":"Initialize Video Predictor\u00b6","text":"<p>The <code>SamGeo3Video</code> class provides a simplified API for video segmentation. It automatically uses all available GPUs.</p>"},{"location":"examples/sam3_object_tracking/#load-a-video","title":"Load a Video\u00b6","text":"<p>You can load from different sources:</p> <ul> <li>MP4 video file</li> <li>Directory of JPEG frames</li> <li>Directory of GeoTIFFs (for remote sensing time series)</li> </ul>"},{"location":"examples/sam3_object_tracking/#text-prompted-segmentation","title":"Text-Prompted Segmentation\u00b6","text":"<p>Use natural language to describe objects. SAM 3 finds all instances and tracks them.</p>"},{"location":"examples/sam3_object_tracking/#visualize-results","title":"Visualize Results\u00b6","text":"<p>Customize player names:</p>"},{"location":"examples/sam3_object_tracking/#remove-objects","title":"Remove objects\u00b6","text":""},{"location":"examples/sam3_object_tracking/#save-results","title":"Save Results\u00b6","text":"<p>Save masks as images or create an output video.</p>"},{"location":"examples/sam3_object_tracking/#close-session","title":"Close Session\u00b6","text":"<p>Close the session to free GPU resources.</p>"},{"location":"examples/sam3_point_prompts/","title":"Sam3 point prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>from samgeo import SamGeo3, download_file, show_image\n</pre> from samgeo import SamGeo3, download_file, show_image In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/facebookresearch/sam3/refs/heads/main/assets/images/truck.jpg\"\nimage_path = download_file(url)\n</pre> url = \"https://raw.githubusercontent.com/facebookresearch/sam3/refs/heads/main/assets/images/truck.jpg\" image_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>show_image(image_path, axis=\"on\")\n</pre> show_image(image_path, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n</pre> sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image_path)\n</pre> sam.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre># Single foreground point - input as Python list\nsam.generate_masks_by_points([[520, 375]])\n</pre> # Single foreground point - input as Python list sam.generate_masks_by_points([[520, 375]]) In\u00a0[\u00a0]: Copied! <pre>print(sam.masks)\n</pre> print(sam.masks) In\u00a0[\u00a0]: Copied! <pre>print(sam.scores)\n</pre> print(sam.scores) In\u00a0[\u00a0]: Copied! <pre>sam.show_points([[520, 375]], [1])\n</pre> sam.show_points([[520, 375]], [1]) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns()\n</pre> sam.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks()\n</pre> sam.show_masks() In\u00a0[\u00a0]: Copied! <pre>sam.save_masks(\"truck_mask.png\", unique=True)\n</pre> sam.save_masks(\"truck_mask.png\", unique=True) In\u00a0[\u00a0]: Copied! <pre># Two foreground points on the truck\nsam.generate_masks_by_points([[500, 375], [1125, 625]], point_labels=[1, 1])\n</pre> # Two foreground points on the truck sam.generate_masks_by_points([[500, 375], [1125, 625]], point_labels=[1, 1]) In\u00a0[\u00a0]: Copied! <pre>sam.show_points([[500, 375], [1125, 625]], [1, 1])\n</pre> sam.show_points([[500, 375], [1125, 625]], [1, 1]) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns()\n</pre> sam.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks()\n</pre> sam.show_masks() In\u00a0[\u00a0]: Copied! <pre># One foreground point on window, one background point on car body\nsam.generate_masks_by_points(\n    [[500, 375], [1125, 625]], point_labels=[1, 0]  # foreground, background\n)\n</pre> # One foreground point on window, one background point on car body sam.generate_masks_by_points(     [[500, 375], [1125, 625]], point_labels=[1, 0]  # foreground, background ) In\u00a0[\u00a0]: Copied! <pre>sam.show_points([[500, 375], [1125, 625]], [1, 0])\n</pre> sam.show_points([[500, 375], [1125, 625]], [1, 0]) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns()\n</pre> sam.show_anns() In\u00a0[\u00a0]: Copied! <pre># Box around the front wheel\nsam.generate_masks_by_boxes_inst([[425, 600, 700, 875]])\n</pre> # Box around the front wheel sam.generate_masks_by_boxes_inst([[425, 600, 700, 875]]) In\u00a0[\u00a0]: Copied! <pre>sam.show_boxes([[425, 600, 700, 875]])\n</pre> sam.show_boxes([[425, 600, 700, 875]]) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns()\n</pre> sam.show_anns() In\u00a0[\u00a0]: Copied! <pre>boxes = [\n    [75, 275, 1725, 850],  # Whole truck\n    [425, 600, 700, 875],  # Front wheel\n    [1375, 550, 1650, 800],  # Rear wheel\n    [1240, 675, 1400, 750],  # License plate area\n]\n\nsam.generate_masks_by_boxes_inst(boxes)\n</pre> boxes = [     [75, 275, 1725, 850],  # Whole truck     [425, 600, 700, 875],  # Front wheel     [1375, 550, 1650, 800],  # Rear wheel     [1240, 675, 1400, 750],  # License plate area ]  sam.generate_masks_by_boxes_inst(boxes) In\u00a0[\u00a0]: Copied! <pre>sam.show_boxes(boxes)\n</pre> sam.show_boxes(boxes) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns()\n</pre> sam.show_anns() In\u00a0[\u00a0]: Copied! <pre>sam.save_masks(\"truck_boxes_mask.png\", unique=True)\n</pre> sam.save_masks(\"truck_boxes_mask.png\", unique=True) In\u00a0[\u00a0]: Copied! <pre># Using Python lists for input\nmasks, scores, logits = sam.predict_inst(\n    point_coords=[[520, 375]],\n    point_labels=[1],\n    multimask_output=True,\n)\n\nprint(f\"Generated {len(masks)} masks\")\nprint(f\"Scores: {scores}\")\n</pre> # Using Python lists for input masks, scores, logits = sam.predict_inst(     point_coords=[[520, 375]],     point_labels=[1],     multimask_output=True, )  print(f\"Generated {len(masks)} masks\") print(f\"Scores: {scores}\") In\u00a0[\u00a0]: Copied! <pre># Show all masks with point overlays\nsam.show_inst_masks(masks, scores, point_coords=[[520, 375]], point_labels=[1])\n</pre> # Show all masks with point overlays sam.show_inst_masks(masks, scores, point_coords=[[520, 375]], point_labels=[1]) In\u00a0[\u00a0]: Copied! <pre># Box prompt with Python list\nmasks, scores, logits = sam.predict_inst(\n    box=[425, 600, 700, 875],\n    multimask_output=False,\n)\n\nsam.show_inst_masks(masks, scores, box_coords=[425, 600, 700, 875])\n</pre> # Box prompt with Python list masks, scores, logits = sam.predict_inst(     box=[425, 600, 700, 875],     multimask_output=False, )  sam.show_inst_masks(masks, scores, box_coords=[425, 600, 700, 875])"},{"location":"examples/sam3_point_prompts/#instance-segmentation-with-point-prompts-and-sam-3","title":"Instance Segmentation with Point Prompts and SAM 3\u00b6","text":"<p>This notebook demonstrates how to use the Segment Anything Model 3 (SAM3) for interactive instance segmentation using point and box prompts.</p>"},{"location":"examples/sam3_point_prompts/#installation","title":"Installation\u00b6","text":""},{"location":"examples/sam3_point_prompts/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_point_prompts/#download-sample-data","title":"Download Sample Data\u00b6","text":""},{"location":"examples/sam3_point_prompts/#initialize-sam3","title":"Initialize SAM3\u00b6","text":"<p>To use point and box prompts (SAM1-style interactive segmentation), initialize SAM3 with <code>enable_inst_interactivity=True</code>.</p>"},{"location":"examples/sam3_point_prompts/#generate-masks-by-point-prompts","title":"Generate Masks by Point Prompts\u00b6","text":"<p>Select an object by clicking a point on it. Points are input in (x, y) format with labels:</p> <ul> <li>1 = foreground point (include this region)</li> <li>0 = background point (exclude this region)</li> </ul>"},{"location":"examples/sam3_point_prompts/#visualize-point-prompts","title":"Visualize Point Prompts\u00b6","text":""},{"location":"examples/sam3_point_prompts/#show-the-results","title":"Show the Results\u00b6","text":""},{"location":"examples/sam3_point_prompts/#save-masks","title":"Save Masks\u00b6","text":""},{"location":"examples/sam3_point_prompts/#multiple-points-with-labels","title":"Multiple Points with Labels\u00b6","text":"<p>Use multiple points to refine selection. Use label=0 for background points to exclude regions.</p>"},{"location":"examples/sam3_point_prompts/#using-background-points","title":"Using Background Points\u00b6","text":"<p>Add a background point (label=0) to exclude a region from the mask.</p>"},{"location":"examples/sam3_point_prompts/#box-prompts","title":"Box Prompts\u00b6","text":"<p>Use a bounding box in XYXY format (xmin, ymin, xmax, ymax) to select an object.</p>"},{"location":"examples/sam3_point_prompts/#multiple-box-prompts","title":"Multiple Box Prompts\u00b6","text":"<p>Process multiple boxes at once for efficient batch segmentation.</p>"},{"location":"examples/sam3_point_prompts/#low-level-api-predict_inst","title":"Low-Level API: predict_inst\u00b6","text":"<p>For more control, you can use the lower-level <code>predict_inst()</code> method which returns masks, scores, and logits directly. Input points and boxes can be provided as Python lists.</p>"},{"location":"examples/sam3_point_prompts/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated SAM3's interactive instance segmentation capabilities:</p> <p>High-level API (recommended):</p> <ul> <li><code>generate_masks_by_points()</code> - Generate masks from point prompts</li> <li><code>generate_masks_by_boxes_inst()</code> - Generate masks from box prompts</li> <li><code>show_points()</code> / <code>show_boxes()</code> - Visualize prompts</li> <li><code>show_anns()</code> / <code>show_masks()</code> - Visualize results</li> <li><code>save_masks()</code> - Save masks to file</li> </ul> <p>Low-level API:</p> <ul> <li><code>predict_inst()</code> - Direct access to masks, scores, and logits</li> <li><code>show_inst_masks()</code> - Display masks with overlays</li> </ul> <p>Input formats:</p> <ul> <li>Points and boxes can be provided as Python lists or numpy arrays</li> <li>Point labels: 1 = foreground, 0 = background</li> </ul>"},{"location":"examples/sam3_point_prompts_batch/","title":"Sam3 point prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo3, download_file\n</pre> import leafmap from samgeo import SamGeo3, download_file In\u00a0[\u00a0]: Copied! <pre>image_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_image.tif\"\nimage_path = download_file(image_url)\ngeojson_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\ngeojson_path = download_file(geojson_url)\n</pre> image_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_image.tif\" image_path = download_file(image_url) geojson_url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\" geojson_path = download_file(geojson_url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Satellite image\")\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Satellite image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True)\n</pre> sam = SamGeo3(backend=\"meta\", enable_inst_interactivity=True) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image_path)\n</pre> sam.set_image(image_path) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    point_coords_batch = m.user_rois\nelse:\n    point_coords_batch = [\n        [-117.599896, 47.655345],\n        [-117.59992, 47.655167],\n        [-117.599928, 47.654974],\n        [-117.599518, 47.655337],\n    ]\n</pre> if m.user_rois is not None:     point_coords_batch = m.user_rois else:     point_coords_batch = [         [-117.599896, 47.655345],         [-117.59992, 47.655167],         [-117.599928, 47.654974],         [-117.599518, 47.655337],     ] In\u00a0[\u00a0]: Copied! <pre>point_coords_batch\n</pre> point_coords_batch <p>Segment the objects using the point prompts and save the output masks.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate_masks_by_points_patch(\n    point_coords_batch=point_coords_batch,\n    point_crs=\"EPSG:4326\",\n    output=\"masks.tif\",\n    dtype=\"uint8\",\n)\n</pre> sam.generate_masks_by_points_patch(     point_coords_batch=point_coords_batch,     point_crs=\"EPSG:4326\",     output=\"masks.tif\",     dtype=\"uint8\", ) In\u00a0[\u00a0]: Copied! <pre>sam.show_points(point_coords_batch, point_crs=\"EPSG:4326\")\n</pre> sam.show_points(point_coords_batch, point_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"masks.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Image\")\nm.add_circle_markers_from_xy(\n    geojson_path, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Image\") m.add_circle_markers_from_xy(     geojson_path, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8 ) m In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate_masks_by_points_patch(\n    point_coords_batch=geojson_path,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint16\",\n)\n</pre> sam.generate_masks_by_points_patch(     point_coords_batch=geojson_path,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint16\", ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\" ) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map(prompt=\"point\")\n</pre> sam.show_map(prompt=\"point\")"},{"location":"examples/sam3_point_prompts_batch/#segmenting-remote-sensing-imagery-with-point-prompts-and-sam-3","title":"Segmenting remote sensing imagery with point prompts and SAM 3\u00b6","text":"<p>This notebook shows how to generate object masks from point prompts with the Segment Anything Model 3 (SAM 3).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/sam3_point_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/sam3_point_prompts_batch/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/sam3_point_prompts_batch/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>Let's download a sample satellite image covering Washington State:</p>"},{"location":"examples/sam3_point_prompts_batch/#initialize-sam-3","title":"Initialize SAM 3\u00b6","text":"<p>To use point and box prompts (SAM1-style interactive segmentation), initialize SAM3 with <code>enable_inst_interactivity=True</code>.</p>"},{"location":"examples/sam3_point_prompts_batch/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>generate_masks_by_points_patch()</code> method to segment the image with specified point coordinates. You can use the draw tools to add place markers on the map. If no point is added, the default sample points will be used.</p>"},{"location":"examples/sam3_point_prompts_batch/#segment-image-with-a-vector-dataset","title":"Segment image with a vector dataset\u00b6","text":"<p>Alternatively, you can specify a file path or HTTP URL to a vector dataset containing point geometries.</p>"},{"location":"examples/sam3_point_prompts_batch/#interactive-segmentation","title":"Interactive Segmentation\u00b6","text":""},{"location":"examples/sam3_tiled_segmentation/","title":"Sam3 tiled segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo3, common\n</pre> import os import leafmap from samgeo import SamGeo3, common In\u00a0[\u00a0]: Copied! <pre># Download a sample satellite image\nurl = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\"\nimage_path = \"naip_water_train.tif\"\n\nif not os.path.exists(image_path):\n    common.download_file(url, image_path)\n</pre> # Download a sample satellite image url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\" image_path = \"naip_water_train.tif\"  if not os.path.exists(image_path):     common.download_file(url, image_path) In\u00a0[\u00a0]: Copied! <pre>common.print_raster_info(image_path)\n</pre> common.print_raster_info(image_path) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3(backend=\"meta\")\n</pre> sam = SamGeo3(backend=\"meta\") In\u00a0[\u00a0]: Copied! <pre># Output path for the mask\noutput_path = \"segmentation_mask.tif\"\n\n# Run tiled segmentation\nsam.generate_masks_tiled(\n    source=image_path,\n    prompt=\"water\",  # Change prompt based on what you want to segment\n    output=output_path,\n    tile_size=1024,  # Size of each tile (adjust based on GPU memory)\n    overlap=128,  # Overlap between tiles\n    min_size=100,  # Minimum object size in pixels\n    unique=False,  # Create binary mask (0 or 1)\n    dtype=\"int32\",  # Data type for output\n    verbose=True,  # Show progress\n)\n</pre> # Output path for the mask output_path = \"segmentation_mask.tif\"  # Run tiled segmentation sam.generate_masks_tiled(     source=image_path,     prompt=\"water\",  # Change prompt based on what you want to segment     output=output_path,     tile_size=1024,  # Size of each tile (adjust based on GPU memory)     overlap=128,  # Overlap between tiles     min_size=100,  # Minimum object size in pixels     unique=False,  # Create binary mask (0 or 1)     dtype=\"int32\",  # Data type for output     verbose=True,  # Show progress ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image_path, layer_name=\"Original Image\")\nm.add_raster(\n    output_path, nodata=0, opacity=0.8, cmap=\"Blues\", layer_name=\"Segmentation Mask\"\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image_path, layer_name=\"Original Image\") m.add_raster(     output_path, nodata=0, opacity=0.8, cmap=\"Blues\", layer_name=\"Segmentation Mask\" ) m In\u00a0[\u00a0]: Copied! <pre># Convert mask to vector\nvector_path = \"segmentation_mask.gpkg\"\ncommon.raster_to_vector(output_path, vector_path)\n</pre> # Convert mask to vector vector_path = \"segmentation_mask.gpkg\" common.raster_to_vector(output_path, vector_path) In\u00a0[\u00a0]: Copied! <pre>smooth_vector_path = \"segmentation_mask_smooth.gpkg\"\ngdf = common.smooth_vector(vector_path, smooth_vector_path)\n</pre> smooth_vector_path = \"segmentation_mask_smooth.gpkg\" gdf = common.smooth_vector(vector_path, smooth_vector_path) In\u00a0[\u00a0]: Copied! <pre>m.add_gdf(gdf, layer_name=\"Smoothed Vector\", info_mode=None)\nm\n</pre> m.add_gdf(gdf, layer_name=\"Smoothed Vector\", info_mode=None) m In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/sam3_tiled_segmentation/#tiled-segmentation-for-large-geotiff-images-with-sam-3","title":"Tiled Segmentation for Large GeoTIFF Images with SAM 3\u00b6","text":"<p>This notebook demonstrates how to use the sliding window (tiled) approach to segment large GeoTIFF images that would otherwise exceed GPU memory limits.</p>"},{"location":"examples/sam3_tiled_segmentation/#overview","title":"Overview\u00b6","text":"<p>When working with large satellite or aerial imagery, loading the entire image into GPU memory for SAM 3 inference is often not feasible. The <code>generate_masks_tiled()</code> method solves this by:</p> <ol> <li>Dividing the large image into smaller, overlapping tiles</li> <li>Processing each tile independently with SAM 3</li> <li>Merging the results back into a seamless output mask</li> <li>Preserving georeferencing information from the original GeoTIFF</li> </ol>"},{"location":"examples/sam3_tiled_segmentation/#key-parameters","title":"Key Parameters\u00b6","text":"<ul> <li>tile_size: Size of each processing tile (default: 1024 pixels)</li> <li>overlap: Overlap between adjacent tiles (default: 128 pixels) - helps prevent edge artifacts</li> <li>prompt: Text description of objects to segment</li> <li>min_size/max_size: Filter objects by pixel area</li> </ul>"},{"location":"examples/sam3_tiled_segmentation/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required packages:</p>"},{"location":"examples/sam3_tiled_segmentation/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_tiled_segmentation/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>We'll use a sample satellite image for this demonstration. You can replace this with your own large GeoTIFF.</p>"},{"location":"examples/sam3_tiled_segmentation/#check-raster-info","title":"Check Raster Info\u00b6","text":"<p>Let's check the dimensions of our image to understand why tiling might be necessary.</p>"},{"location":"examples/sam3_tiled_segmentation/#initialize-samgeo3","title":"Initialize SamGeo3\u00b6","text":""},{"location":"examples/sam3_tiled_segmentation/#run-tiled-segmentation","title":"Run Tiled Segmentation\u00b6","text":"<p>Now we'll use the <code>generate_masks_tiled()</code> method to process the image. This method:</p> <ol> <li>Reads the image tile by tile</li> <li>Processes each tile with SAM3</li> <li>Merges overlapping regions intelligently</li> <li>Saves the result as a georeferenced GeoTIFF</li> </ol>"},{"location":"examples/sam3_tiled_segmentation/#visualize-results","title":"Visualize Results\u00b6","text":""},{"location":"examples/sam3_tiled_segmentation/#convert-mask-to-vector","title":"Convert Mask to Vector\u00b6","text":"<p>You can convert the raster mask to vector format (GeoPackage, Shapefile, etc.) for further analysis in GIS software.</p>"},{"location":"examples/sam3_tiled_segmentation/#smooth-vector","title":"Smooth Vector\u00b6","text":""},{"location":"examples/sam3_tiled_segmentation/#tips-for-processing-large-images","title":"Tips for Processing Large Images\u00b6","text":"<ol> <li><p>Tile Size: Larger tiles capture more context but require more GPU memory. Start with 512 or 1024 and increase if you have sufficient GPU memory.</p> </li> <li><p>Overlap: Higher overlap (e.g., 128-256) helps prevent artifacts at tile boundaries but increases processing time. Lower overlap (e.g., 64) is faster but may have more edge effects.</p> </li> <li><p>Memory Management: The method automatically clears GPU memory after each tile. If you still encounter memory issues, try reducing the tile_size.</p> </li> <li><p>Data Type: Use <code>int32</code> for images with many objects, <code>int16</code> for up to 65535 objects, or <code>int8</code> for up to 255 objects.</p> </li> <li><p>Filtering: Use <code>min_size</code> and <code>max_size</code> to filter out noise (small objects) or irrelevant large regions.</p> </li> </ol>"},{"location":"examples/sam3_video_masks/","title":"Sam3 video masks","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nfrom samgeo import SamGeo3Video, download_file\n</pre> import os import numpy as np from samgeo import SamGeo3Video, download_file In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\nvideo_path = download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" video_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre># Initialize SAM 3 video predictor\nsam = SamGeo3Video()\n</pre> # Initialize SAM 3 video predictor sam = SamGeo3Video() In\u00a0[\u00a0]: Copied! <pre># Load the video\nsam.set_video(video_path)\n</pre> # Load the video sam.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre># Generate masks using text prompt (simulating external model output)\nsam.generate_masks(\"car\")\n</pre> # Generate masks using text prompt (simulating external model output) sam.generate_masks(\"car\") In\u00a0[\u00a0]: Copied! <pre># Show the segmentation results\nsam.show_frame(0, axis=\"on\")\n</pre> # Show the segmentation results sam.show_frame(0, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre># Extract masks from the first frame\nformatted_outputs = sam._format_outputs()\nframe_0_masks = formatted_outputs.get(0, {})\n\n# Store masks and their object IDs\nexternal_masks = []\noriginal_obj_ids = []\nfor obj_id, mask in frame_0_masks.items():\n    if isinstance(obj_id, int):\n        external_masks.append(np.array(mask))\n        original_obj_ids.append(obj_id)\n\nprint(f\"Extracted {len(external_masks)} masks from frame 0\")\nprint(f\"Object IDs: {original_obj_ids}\")\n</pre> # Extract masks from the first frame formatted_outputs = sam._format_outputs() frame_0_masks = formatted_outputs.get(0, {})  # Store masks and their object IDs external_masks = [] original_obj_ids = [] for obj_id, mask in frame_0_masks.items():     if isinstance(obj_id, int):         external_masks.append(np.array(mask))         original_obj_ids.append(obj_id)  print(f\"Extracted {len(external_masks)} masks from frame 0\") print(f\"Object IDs: {original_obj_ids}\") In\u00a0[\u00a0]: Copied! <pre>sam.init_tracker()\n</pre> sam.init_tracker() In\u00a0[\u00a0]: Copied! <pre># Add masks one by one using add_mask_prompt()\n# Use obj_ids starting from 100 to avoid conflicts with detected objects\nfor i, mask in enumerate(external_masks):\n    sam.add_mask_prompt(mask, obj_id=100 + i, frame_idx=0)\n</pre> # Add masks one by one using add_mask_prompt() # Use obj_ids starting from 100 to avoid conflicts with detected objects for i, mask in enumerate(external_masks):     sam.add_mask_prompt(mask, obj_id=100 + i, frame_idx=0) In\u00a0[\u00a0]: Copied! <pre># Propagate masks through the video\nsam.propagate()\n</pre> # Propagate masks through the video sam.propagate() In\u00a0[\u00a0]: Copied! <pre># Show results\nsam.show_frame(0, axis=\"on\")\n</pre> # Show results sam.show_frame(0, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre># Show multiple frames\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Show multiple frames sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre>sam.init_tracker()\n</pre> sam.init_tracker() In\u00a0[\u00a0]: Copied! <pre># Add all masks at once (IDs will be auto-assigned starting from 100)\nsam.add_masks_prompt(external_masks)\n</pre> # Add all masks at once (IDs will be auto-assigned starting from 100) sam.add_masks_prompt(external_masks) In\u00a0[\u00a0]: Copied! <pre># Propagate\nsam.propagate()\n</pre> # Propagate sam.propagate() In\u00a0[\u00a0]: Copied! <pre># Show results\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Show results sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre>os.makedirs(\"output\", exist_ok=True)\n\n# Save mask images\nsam.save_masks(\"output/external_masks\")\n</pre> os.makedirs(\"output\", exist_ok=True)  # Save mask images sam.save_masks(\"output/external_masks\") In\u00a0[\u00a0]: Copied! <pre># Save video with blended masks\nsam.save_video(\"output/external_tracked.mp4\", fps=25)\n</pre> # Save video with blended masks sam.save_video(\"output/external_tracked.mp4\", fps=25) In\u00a0[\u00a0]: Copied! <pre># Close session to free GPU resources\nsam.close()\n</pre> # Close session to free GPU resources sam.close() In\u00a0[\u00a0]: Copied! <pre>sam.shutdown()\n</pre> sam.shutdown()"},{"location":"examples/sam3_video_masks/#external-mask-tracking-with-sam-3","title":"External Mask Tracking with SAM 3\u00b6","text":"<p>This notebook demonstrates how to use external segmentation masks with SAM 3's tracking capability. This is useful when:</p> <ul> <li>SAM 3's text prompts don't segment the exact objects you need</li> <li>You have masks from another segmentation model (YOLO, Detectron2, GroundingDINO, etc.)</li> <li>You want to use a specialized model for initial segmentation and SAM 3 for tracking</li> </ul>"},{"location":"examples/sam3_video_masks/#workflow","title":"Workflow\u00b6","text":"<ol> <li>Initialize tracker - Call <code>init_tracker()</code> with any text prompt to initialize SAM3's tracker</li> <li>Add external masks - Use <code>add_mask_prompt()</code> to add masks from your external model</li> <li>Propagate - Use SAM 3's tracking to propagate masks through the video</li> </ol>"},{"location":"examples/sam3_video_masks/#installation","title":"Installation\u00b6","text":"<p>SAM 3 requires CUDA-capable GPU. Install with:</p>"},{"location":"examples/sam3_video_masks/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_video_masks/#download-sample-video","title":"Download Sample Video\u00b6","text":""},{"location":"examples/sam3_video_masks/#method-1-using-sam-3s-text-prompt-to-generate-initial-masks","title":"Method 1: Using SAM 3's Text Prompt to Generate Initial Masks\u00b6","text":"<p>In this example, we'll use SAM 3's own text prompt to generate masks, then demonstrate how to use those masks with the <code>add_mask_prompt()</code> method. In a real workflow, you would replace this with masks from your preferred external model.</p>"},{"location":"examples/sam3_video_masks/#step-1-generate-initial-masks-using-text-prompt","title":"Step 1: Generate initial masks using text prompt\u00b6","text":"<p>This simulates getting masks from an external model. In practice, you would use your own segmentation model here.</p>"},{"location":"examples/sam3_video_masks/#step-2-extract-masks-from-frame-0","title":"Step 2: Extract masks from frame 0\u00b6","text":"<p>Extract the binary masks that we'll use to demonstrate the <code>add_mask_prompt()</code> method.</p>"},{"location":"examples/sam3_video_masks/#step-3-add-external-masks-for-tracking","title":"Step 3: Add external masks for tracking\u00b6","text":"<p>Now add the extracted masks using <code>add_mask_prompt()</code>. Use new object IDs (starting from 100) to avoid conflicts with the text-prompt detected objects.</p> <p>First, let's initialize the tracker.</p>"},{"location":"examples/sam3_video_masks/#method-2-using-add_masks_prompt-for-multiple-masks","title":"Method 2: Using add_masks_prompt() for Multiple Masks\u00b6","text":"<p>For convenience, you can add multiple masks at once using <code>add_masks_prompt()</code>.</p> <p>Note: Object IDs are auto-assigned starting from 100 to avoid conflicts.</p>"},{"location":"examples/sam3_video_masks/#save-results","title":"Save Results\u00b6","text":""},{"location":"examples/sam3_video_masks/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"examples/sam3_video_masks/#summary","title":"Summary\u00b6","text":"<p>The <code>add_mask_prompt()</code> and <code>add_masks_prompt()</code> methods allow you to:</p> <ol> <li>Use any segmentation model - Not limited to SAM 3's text prompts</li> <li>Get accurate initial segmentation - Use specialized models for your specific objects</li> <li>Leverage SAM 3's tracking - Powerful temporal tracking through the video</li> <li>Selective tracking - Choose which objects to track</li> </ol>"},{"location":"examples/sam3_video_masks/#important-workflow-notes","title":"Important Workflow Notes\u00b6","text":"<ol> <li>Initialize tracker first: Always call <code>init_tracker()</code> before <code>add_mask_prompt()</code></li> <li>Don't reset: Do not call <code>reset()</code> before adding external masks</li> <li>Use unique IDs: Use <code>obj_id &gt;= 100</code> to avoid conflicts with text-detected objects</li> </ol>"},{"location":"examples/sam3_video_masks/#api-reference","title":"API Reference\u00b6","text":"<ul> <li><code>add_mask_prompt(mask, obj_id, frame_idx=0, num_points=5)</code> - Add a single mask</li> <li><code>add_masks_prompt(masks, obj_ids=None, frame_idx=0)</code> - Add multiple masks at once</li> </ul>"},{"location":"examples/sam3_video_masks/#mask-requirements","title":"Mask Requirements\u00b6","text":"<ul> <li>Binary mask array of shape <code>(H, W)</code></li> <li>Values: <code>True/1</code> for object, <code>False/0</code> for background</li> <li>Automatically resized to match video frame dimensions</li> </ul>"},{"location":"examples/sam3_video_prompts/","title":"Sam3 video prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import os\nfrom samgeo import SamGeo3Video, download_file\n</pre> import os from samgeo import SamGeo3Video, download_file In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3Video()\n</pre> sam = SamGeo3Video() In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\nvideo_path = download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" video_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>sam.set_video(video_path)\n</pre> sam.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>sam.show_video(video_path)\n</pre> sam.show_video(video_path) In\u00a0[\u00a0]: Copied! <pre>sam.init_tracker()\n</pre> sam.init_tracker() In\u00a0[\u00a0]: Copied! <pre>sam.show_frame(0, axis=\"on\")\n</pre> sam.show_frame(0, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre>sam.add_point_prompts([[300, 200]], [1], obj_id=1, frame_idx=0)\n</pre> sam.add_point_prompts([[300, 200]], [1], obj_id=1, frame_idx=0) In\u00a0[\u00a0]: Copied! <pre>sam.add_point_prompts([[420, 200]], [1], obj_id=2, frame_idx=0)\n</pre> sam.add_point_prompts([[420, 200]], [1], obj_id=2, frame_idx=0) In\u00a0[\u00a0]: Copied! <pre>sam.propagate()\n</pre> sam.propagate() In\u00a0[\u00a0]: Copied! <pre># Show the first frame with masks\nsam.show_frame(0, axis=\"on\")\n</pre> # Show the first frame with masks sam.show_frame(0, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre># Show multiple frames in a grid\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Show multiple frames in a grid sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre># Remove object 2 and re-propagate\nsam.remove_object(2)\nsam.propagate()\nsam.show_frame(0)\n</pre> # Remove object 2 and re-propagate sam.remove_object(2) sam.propagate() sam.show_frame(0) In\u00a0[\u00a0]: Copied! <pre># Refine to segment only the windshield (not the rest of the car)\nsam.add_point_prompts(\n    points=[[335, 195], [335, 220]],  # positive: windshield, negative: car body\n    labels=[1, 0],  # positive, negative\n    obj_id=1,\n    frame_idx=0,\n)\nsam.propagate()\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Refine to segment only the windshield (not the rest of the car) sam.add_point_prompts(     points=[[335, 195], [335, 220]],  # positive: windshield, negative: car body     labels=[1, 0],  # positive, negative     obj_id=1,     frame_idx=0, ) sam.propagate() sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre>os.makedirs(\"output\", exist_ok=True)\n\n# Save mask images\nsam.save_masks(\"output/masks\")\n</pre> os.makedirs(\"output\", exist_ok=True)  # Save mask images sam.save_masks(\"output/masks\") In\u00a0[\u00a0]: Copied! <pre># Save video with blended masks\nsam.save_video(\"output/segmented.mp4\", fps=25)\n</pre> # Save video with blended masks sam.save_video(\"output/segmented.mp4\", fps=25) In\u00a0[\u00a0]: Copied! <pre>sam.close()\n</pre> sam.close() <p>To completely shutdown and free all resources:</p> In\u00a0[\u00a0]: Copied! <pre>sam.shutdown()\n</pre> sam.shutdown()"},{"location":"examples/sam3_video_prompts/#video-segmentation-with-point-prompts-and-sam-3","title":"Video Segmentation with Point Prompts and SAM 3\u00b6","text":"<p>This notebook demonstrates how to use SAM 3 for video segmentation and tracking. You will learn how to install the required libraries, load videos from different sources, apply point prompts for segmentation, and visualize the results step by step.</p>"},{"location":"examples/sam3_video_prompts/#installation","title":"Installation\u00b6","text":"<p>SAM 3 requires CUDA-capable GPU. Install with:</p>"},{"location":"examples/sam3_video_prompts/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_video_prompts/#initialize-video-predictor","title":"Initialize Video Predictor\u00b6","text":"<p>The <code>SamGeo3Video</code> class provides a simplified API for video segmentation. It automatically uses all available GPUs.</p>"},{"location":"examples/sam3_video_prompts/#load-a-video","title":"Load a Video\u00b6","text":"<p>You can load from different sources:</p> <ul> <li>MP4 video file</li> <li>Directory of JPEG frames</li> <li>Directory of GeoTIFFs (for remote sensing time series)</li> </ul>"},{"location":"examples/sam3_video_prompts/#point-prompted-segmentation","title":"Point-Prompted Segmentation\u00b6","text":"<p>First, let's initialize the tracker.</p>"},{"location":"examples/sam3_video_prompts/#add-point-prompts","title":"Add point prompts\u00b6","text":""},{"location":"examples/sam3_video_prompts/#visualize-results","title":"Visualize Results\u00b6","text":""},{"location":"examples/sam3_video_prompts/#remove-objects","title":"Remove Objects\u00b6","text":"<p>Remove specific objects by ID and re-propagate.</p>"},{"location":"examples/sam3_video_prompts/#refine-with-multiple-points","title":"Refine with Multiple Points\u00b6","text":"<p>Use positive and negative points to refine the mask.</p>"},{"location":"examples/sam3_video_prompts/#save-results","title":"Save Results\u00b6","text":"<p>Save masks as images or create an output video.</p>"},{"location":"examples/sam3_video_prompts/#close-session","title":"Close Session\u00b6","text":"<p>Close the session to free GPU resources.</p>"},{"location":"examples/sam3_video_segmentation/","title":"Sam3 video segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"segment-geospatial[samgeo3]\"\n</pre> # %pip install \"segment-geospatial[samgeo3]\" In\u00a0[\u00a0]: Copied! <pre>import os\nfrom samgeo import SamGeo3Video, download_file\n</pre> import os from samgeo import SamGeo3Video, download_file In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo3Video()\n</pre> sam = SamGeo3Video() In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\nvideo_path = download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" video_path = download_file(url) In\u00a0[\u00a0]: Copied! <pre>sam.set_video(video_path)\n</pre> sam.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>sam.show_video(video_path)\n</pre> sam.show_video(video_path) In\u00a0[\u00a0]: Copied! <pre># Segment all car in the video\nsam.generate_masks(\"car\")\n</pre> # Segment all car in the video sam.generate_masks(\"car\") In\u00a0[\u00a0]: Copied! <pre># Show the first frame with masks\nsam.show_frame(0, axis=\"on\")\n</pre> # Show the first frame with masks sam.show_frame(0, axis=\"on\") In\u00a0[\u00a0]: Copied! <pre># Show multiple frames in a grid\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Show multiple frames in a grid sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre># Remove object 2 and re-propagate\nsam.remove_object(2)\nsam.propagate()\nsam.show_frame(0)\n</pre> # Remove object 2 and re-propagate sam.remove_object(2) sam.propagate() sam.show_frame(0) In\u00a0[\u00a0]: Copied! <pre># Add back object 2 with a positive point click\nsam.add_point_prompts(\n    points=[[335, 203]],  # [x, y] coordinates\n    labels=[1],  # 1=positive, 0=negative\n    obj_id=2,\n    frame_idx=0,\n)\nsam.propagate()\nsam.show_frame(0)\n</pre> # Add back object 2 with a positive point click sam.add_point_prompts(     points=[[335, 203]],  # [x, y] coordinates     labels=[1],  # 1=positive, 0=negative     obj_id=2,     frame_idx=0, ) sam.propagate() sam.show_frame(0) In\u00a0[\u00a0]: Copied! <pre># Refine to segment only the shirt (not pants)\nsam.add_point_prompts(\n    points=[[335, 195], [335, 220]],  # detect windshield, not the car\n    labels=[1, 0],  # positive, negative\n    obj_id=2,\n    frame_idx=0,\n)\nsam.propagate()\nsam.show_frames(frame_stride=20, ncols=3)\n</pre> # Refine to segment only the shirt (not pants) sam.add_point_prompts(     points=[[335, 195], [335, 220]],  # detect windshield, not the car     labels=[1, 0],  # positive, negative     obj_id=2,     frame_idx=0, ) sam.propagate() sam.show_frames(frame_stride=20, ncols=3) In\u00a0[\u00a0]: Copied! <pre>os.makedirs(\"output\", exist_ok=True)\n\n# Save mask images\nsam.save_masks(\"output/masks\")\n</pre> os.makedirs(\"output\", exist_ok=True)  # Save mask images sam.save_masks(\"output/masks\") In\u00a0[\u00a0]: Copied! <pre># Save video with blended masks\nsam.save_video(\"output/segmented.mp4\", fps=25)\n</pre> # Save video with blended masks sam.save_video(\"output/segmented.mp4\", fps=25) In\u00a0[\u00a0]: Copied! <pre>sam.close()\n</pre> sam.close() <p>To completely shutdown and free all resources:</p> In\u00a0[\u00a0]: Copied! <pre>sam.shutdown()\n</pre> sam.shutdown()"},{"location":"examples/sam3_video_segmentation/#video-segmentation-with-sam-3","title":"Video Segmentation with SAM 3\u00b6","text":"<p>This notebook demonstrates how to use SAM 3 for video segmentation and tracking. SAM 3 provides:</p> <ul> <li>Text prompts: Segment objects using natural language (e.g., \"person\", \"car\")</li> <li>Point prompts: Add clicks to segment and refine objects</li> <li>Object tracking: Track segmented objects across all video frames</li> <li>Time series support: Process GeoTIFF time series with georeferencing</li> </ul>"},{"location":"examples/sam3_video_segmentation/#installation","title":"Installation\u00b6","text":"<p>SAM 3 requires CUDA-capable GPU. Install with:</p>"},{"location":"examples/sam3_video_segmentation/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/sam3_video_segmentation/#initialize-video-predictor","title":"Initialize Video Predictor\u00b6","text":"<p>The <code>SamGeo3Video</code> class provides a simplified API for video segmentation. It automatically uses all available GPUs.</p>"},{"location":"examples/sam3_video_segmentation/#load-a-video","title":"Load a Video\u00b6","text":"<p>You can load from different sources:</p> <ul> <li>MP4 video file</li> <li>Directory of JPEG frames</li> <li>Directory of GeoTIFFs (for remote sensing time series)</li> </ul>"},{"location":"examples/sam3_video_segmentation/#text-prompted-segmentation","title":"Text-Prompted Segmentation\u00b6","text":"<p>Use natural language to describe objects. SAM 3 finds all instances and tracks them.</p>"},{"location":"examples/sam3_video_segmentation/#visualize-results","title":"Visualize Results\u00b6","text":""},{"location":"examples/sam3_video_segmentation/#remove-objects","title":"Remove Objects\u00b6","text":"<p>Remove specific objects by ID and re-propagate.</p>"},{"location":"examples/sam3_video_segmentation/#point-prompts","title":"Point Prompts\u00b6","text":"<p>Add objects back or refine segmentation using point prompts.</p>"},{"location":"examples/sam3_video_segmentation/#refine-with-multiple-points","title":"Refine with Multiple Points\u00b6","text":"<p>Use positive and negative points to refine the mask.</p>"},{"location":"examples/sam3_video_segmentation/#save-results","title":"Save Results\u00b6","text":"<p>Save masks as images or create an output video.</p>"},{"location":"examples/sam3_video_segmentation/#close-session","title":"Close Session\u00b6","text":"<p>Close the session to free GPU resources.</p>"},{"location":"examples/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor\nfrom samgeo.common import tms_to_geotiff\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor from samgeo.common import tms_to_geotiff from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.common import tms_to_geotiff\n</pre> import leafmap from samgeo import SamGeo from samgeo.common import tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/text_swimming_pools/","title":"Text swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.common import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.common import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/text_swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/text_swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/tree_mapping/","title":"Tree mapping","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>geojson = (\n    \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_boxes.geojson\"\n)\n</pre> geojson = (     \"https://github.com/opengeos/datasets/releases/download/samgeo/tree_boxes.geojson\" ) <p>Display the bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(\n    geojson,\n    style=style,\n    zoom_to_layer=True,\n    layer_name=\"Bounding boxes\",\n    info_mode=None,\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(     geojson,     style=style,     zoom_to_layer=True,     layer_name=\"Bounding boxes\",     info_mode=None, ) m In\u00a0[\u00a0]: Copied! <pre>output_masks = \"mask2.tif\"\nsam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=output_masks, dtype=\"uint8\")\n</pre> output_masks = \"mask2.tif\" sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=output_masks, dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(output_masks, nodata=0, opacity=0.5, layer_name=\"Tree masks\")\n</pre> m.add_raster(output_masks, nodata=0, opacity=0.5, layer_name=\"Tree masks\") In\u00a0[\u00a0]: Copied! <pre>out_image = \"tree_masks.tif\"\nout_vector = \"tree_vector.geojson\"\narray, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> out_image = \"tree_masks.tif\" out_vector = \"tree_vector.geojson\" array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(\n    out_image, colormap=\"tab20\", nodata=0, opacity=0.7, layer_name=\"Tree masks\"\n)\nm.add_vector(out_vector, style=style, zoom_to_layer=True, layer_name=\"Tree vector\")\nm.add_vector(\n    geojson,\n    style={\"color\": \"blue\", \"fillOpacity\": 0},\n    layer_name=\"Bounding boxes\",\n    info_mode=None,\n)\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(     out_image, colormap=\"tab20\", nodata=0, opacity=0.7, layer_name=\"Tree masks\" ) m.add_vector(out_vector, style=style, zoom_to_layer=True, layer_name=\"Tree vector\") m.add_vector(     geojson,     style={\"color\": \"blue\", \"fillOpacity\": 0},     layer_name=\"Bounding boxes\",     info_mode=None, ) m.add_layer_manager() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.split_map(\n    out_image,\n    image,\n    left_label=\"Tree masks\",\n    right_label=\"Aerial imagery\",\n    left_args={\"colormap\": \"tab20\", \"nodata\": 0, \"opacity\": 0.7},\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.split_map(     out_image,     image,     left_label=\"Tree masks\",     right_label=\"Aerial imagery\",     left_args={\"colormap\": \"tab20\", \"nodata\": 0, \"opacity\": 0.7}, ) m <p></p>"},{"location":"examples/tree_mapping/#tree-mapping-with-samgeo-and-segment-anything-model-2-sam-2","title":"Tree Mapping with SAMGeo and Segment Anything Model 2 (SAM 2)\u00b6","text":"<p>This notebook shows how to segment trees from aerial imagery with the Segment Anything Model 2 (SAM 2).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/tree_mapping/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/tree_mapping/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/tree_mapping/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/tree_mapping/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"examples/tree_mapping/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/tree_mapping/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/tree_mapping/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/tree_mapping/#use-an-existing-vector-dataset-as-box-prompts","title":"Use an existing vector dataset as box prompts\u00b6","text":"<p>You can also use an existing vector dataset as box prompts. The following example uses an existing dataset of tree bounding boxes from GitHub.</p>"},{"location":"examples/tree_mapping/#segment-trees-with-box-prompts","title":"Segment trees with box prompts\u00b6","text":"<p>Segment trees using the bounding boxes from the vector dataset.</p>"},{"location":"examples/tree_mapping/#post-processing","title":"Post-processing\u00b6","text":"<p>You can use the <code>region_groups()</code> method to clean up the segmentation results, such as removing small regions, and filling holes. In addition, you can compute geometric properties of the regions, such as area, perimeter, eccentricity, and solidity.</p>"},{"location":"examples/tree_mapping/#display-the-cleaned-masks","title":"Display the cleaned masks\u00b6","text":""},{"location":"examples/tree_mapping/#create-a-split-map","title":"Create a split map\u00b6","text":""},{"location":"workshops/AIforGood_2025/","title":"AIforGood 2025","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install segment-geospatial groundingdino-py\n</pre> %pip install segment-geospatial groundingdino-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2, regularize\n</pre> import leafmap from samgeo import SamGeo2, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>Important note: The code is provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.</p> <p>Alternatively, you can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    point_coords_batch = m.user_rois\nelse:\n    point_coords_batch = [\n        [-117.599896, 47.655345],\n        [-117.59992, 47.655167],\n        [-117.599928, 47.654974],\n        [-117.599518, 47.655337],\n    ]\n</pre> if m.user_rois is not None:     point_coords_batch = m.user_rois else:     point_coords_batch = [         [-117.599896, 47.655345],         [-117.59992, 47.655167],         [-117.599928, 47.654974],         [-117.599518, 47.655337],     ] <p>Segment the objects using the point prompts and save the output masks.</p> In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=point_coords_batch,\n    point_crs=\"EPSG:4326\",\n    output=\"mask.tif\",\n    dtype=\"uint8\",\n)\n</pre> sam.predict_by_points(     point_coords_batch=point_coords_batch,     point_crs=\"EPSG:4326\",     output=\"mask.tif\",     dtype=\"uint8\", ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.7, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n</pre> geojson = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\" <p>Display the vector dataawr on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nm.add_circle_markers_from_xy(\n    geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") m.add_circle_markers_from_xy(     geojson, radius=3, color=\"red\", fill_color=\"yellow\", fill_opacity=0.8 ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_by_points(\n    point_coords_batch=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict_by_points(     point_coords_batch=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.7, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>out_vector = \"building_vector.geojson\"\nout_image = \"buildings.tif\"\n</pre> out_vector = \"building_vector.geojson\" out_image = \"buildings.tif\" In\u00a0[\u00a0]: Copied! <pre>array, gdf = sam.region_groups(\n    output_masks, min_size=200, out_vector=out_vector, out_image=out_image\n)\n</pre> array, gdf = sam.region_groups(     output_masks, min_size=200, out_vector=out_vector, out_image=out_image ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() <p></p> In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(out_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(out_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\")\nm.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_raster(out_image, cmap=\"tab20\", opacity=0.7, nodata=0, layer_name=\"Buildings\") m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2, raster_to_vector, regularize\n</pre> import leafmap from samgeo import SamGeo2, raster_to_vector, regularize In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\")\nm.add_basemap(\"Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.653287, -117.588070], zoom=16, height=\"800px\") m.add_basemap(\"Satellite\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-117.6029, 47.65, -117.5936, 47.6563]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-117.6029, 47.65, -117.5936, 47.6563] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"satellite.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    automatic=False,\n)\n</pre> sam = SamGeo2(     model_id=\"sam2-hiera-large\",     automatic=False, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-117.5995, 47.6518, -117.5988, 47.652],\n        [-117.5987, 47.6518, -117.5979, 47.652],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-117.5995, 47.6518, -117.5988, 47.652],         [-117.5987, 47.6518, -117.5979, 47.652],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\"\ngeojson = \"building_bboxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/samgeo/building_bboxes.geojson\" geojson = \"building_bboxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector dataset on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bboxes\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_masks = \"building_masks.tif\"\n</pre> output_masks = \"building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson,\n    point_crs=\"EPSG:4326\",\n    output=output_masks,\n    dtype=\"uint8\",\n    multimask_output=False,\n)\n</pre> sam.predict(     boxes=geojson,     point_crs=\"EPSG:4326\",     output=output_masks,     dtype=\"uint8\",     multimask_output=False, ) <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\"\n)\nm\n</pre> m.add_raster(     output_masks, cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Building masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>output_vector = \"building_vector.geojson\"\nraster_to_vector(output_masks, output_vector)\n</pre> output_vector = \"building_vector.geojson\" raster_to_vector(output_masks, output_vector) In\u00a0[\u00a0]: Copied! <pre>output_regularized = \"building_regularized.geojson\"\nregularize(output_vector, output_regularized)\n</pre> output_regularized = \"building_regularized.geojson\" regularize(output_vector, output_regularized) <p>Display the regularized building footprints on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\n    output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None\n)\n</pre> m.add_vector(     output_regularized, style=style, layer_name=\"Building regularized\", info_mode=None ) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] <p>Download the image within the selected region using <code>map_tiles_to_geotiff()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\nleafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True\n)\n</pre> image = \"Image.tif\" leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM(model_type=\"sam2-hiera-large\")\n</pre> sam = LangSAM(model_type=\"sam2-hiera-large\") In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam.region_groups(\n    image=\"trees.tif\",\n    min_size=100,\n    out_csv=\"objects.csv\",\n    out_image=\"objects.tif\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam.region_groups(     image=\"trees.tif\",     min_size=100,     out_csv=\"objects.csv\",     out_image=\"objects.tif\",     out_vector=\"objects.gpkg\", ) <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"objects.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"objects.gpkg\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\"\nleafmap.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/landsat_ts.zip\" leafmap.download_file(url) In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>video_path = \"landsat_ts\"\npredictor.set_video(video_path)\n</pre> video_path = \"landsat_ts\" predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[1582, 933], [1287, 905], [1473, 998]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[1582, 933], [1287, 905], [1473, 998]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> <p>Althernatively, prompts can be provided in lon/lat coordinates. The model will automatically convert the lon/lat coordinates to pixel coordinates when the <code>point_crs</code> parameter is set to the coordinate reference system of the lon/lat coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],\n        \"labels\": [1, 1, 1],\n        \"frame_idx\": 0,\n    },\n}\npredictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\")\n</pre> prompts = {     1: {         \"points\": [[-74.3713, -8.5218], [-74.2973, -8.5306], [-74.3230, -8.5495]],         \"labels\": [1, 1, 1],         \"frame_idx\": 0,     }, } predictor.show_prompts(prompts, frame_idx=0, point_crs=\"EPSG:4326\") In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video()\n</pre> predictor.predict_video() In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments(\"segments\")\n</pre> predictor.save_video_segments(\"segments\") <p>To save the results as blended images and MP4 video:</p> In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\n    \"blended\", fps=5, output_video=\"segments_blended.mp4\"\n)\n</pre> predictor.save_video_segments_blended(     \"blended\", fps=5, output_video=\"segments_blended.mp4\" ) <p></p> <p>Preview the video.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Video\n\nVideo(\"segments_blended.mp4\", embed=True, width=600, height=400)\n</pre> from IPython.display import Video  Video(\"segments_blended.mp4\", embed=True, width=600, height=400) In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo2\n</pre> import leafmap from samgeo import SamGeo2 In\u00a0[\u00a0]: Copied! <pre>predictor = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    video=True,\n)\n</pre> predictor = SamGeo2(     model_id=\"sam2-hiera-large\",     video=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/videos/cars.mp4\" In\u00a0[\u00a0]: Copied! <pre>video_path = url\npredictor.set_video(video_path)\n</pre> video_path = url predictor.set_video(video_path) In\u00a0[\u00a0]: Copied! <pre>predictor.show_images()\n</pre> predictor.show_images() In\u00a0[\u00a0]: Copied! <pre>prompts = {\n    1: {\n        \"points\": [[335, 203]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n    2: {\n        \"points\": [[420, 201]],\n        \"labels\": [1],\n        \"frame_idx\": 0,\n    },\n}\n</pre> prompts = {     1: {         \"points\": [[335, 203]],         \"labels\": [1],         \"frame_idx\": 0,     },     2: {         \"points\": [[420, 201]],         \"labels\": [1],         \"frame_idx\": 0,     }, } In\u00a0[\u00a0]: Copied! <pre>predictor.show_prompts(prompts, frame_idx=0)\n</pre> predictor.show_prompts(prompts, frame_idx=0) <p></p> In\u00a0[\u00a0]: Copied! <pre>predictor.predict_video(prompts)\n</pre> predictor.predict_video(prompts) In\u00a0[\u00a0]: Copied! <pre>predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25)\n</pre> predictor.save_video_segments_blended(\"cars\", output_video=\"cars_blended.mp4\", fps=25) <p></p> <p>Preview the video.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Video\n\nVideo(\"cars_blended.mp4\", embed=True, width=600, height=400)\n</pre> from IPython.display import Video  Video(\"cars_blended.mp4\", embed=True, width=600, height=400)"},{"location":"workshops/AIforGood_2025/#ai-for-good-workshop-2025","title":"AI for Good Workshop 2025\u00b6","text":"<p>Join us for the AI for Good Workshop 2025, part of the UN's AI for Good workshop series! This workshop will take place online on February 5, 2025, from 9:00 AM to 10:30 AM EST. It is free and open to the public. Please register using this link: Mastering Remote Sensing Image Segmentation with AI: A Hands-On Workshop with the Segment Anything Model.</p>"},{"location":"workshops/AIforGood_2025/#overview","title":"Overview\u00b6","text":"<p>Built upon Meta\u2019s Segment Anything Model (SAM), the SAMGeo Python package brings advanced segmentation capabilities to geospatial data. This hands-on workshop is tailored for geospatial enthusiasts, researchers, and professionals eager to unlock the potential of GeoAI in their projects.</p> <p>Participants will explore how to leverage SAMGeo for accurate and efficient image segmentation of satellite and aerial imagery. The workshop includes step-by-step demonstrations and practical exercises covering:</p> <ul> <li>Introduction to SAM and SAMGeo: Learn the architecture and functionality of SAM and its transformative applications in geospatial analysis.</li> <li>Data Preparation: Prepare geospatial datasets with multi-spectral channels for segmentation tasks.</li> <li>Hands-On with SAMGeo: Leverage SAMGeo to segment geospatial features (e.g., buildings, trees, water bodies) using prompts such as point coordinates, bounding boxes, and text.</li> <li>Postprocessing Techniques: Calculate geometric properties of segmented features, filter results, and extract meaningful insights.</li> <li>Data Visualization: Visualize object masks and segmented features in standard geospatial formats for analysis and reporting.</li> </ul> <p>By the end of the workshop, participants will gain practical experience applying SAMGeo to real-world geospatial challenges and leave equipped with new tools to elevate their geospatial data workflows.</p>"},{"location":"workshops/AIforGood_2025/#target-audience","title":"Target audience\u00b6","text":"<p>This workshop is ideal for geospatial data scientists, remote sensing analysts, researchers, and anyone interested in applying AI to geospatial data.</p>"},{"location":"workshops/AIforGood_2025/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>A Google Colab account</li> <li>Basic understanding of Python programming and geospatial data concepts is recommended</li> </ul>"},{"location":"workshops/AIforGood_2025/#recording","title":"Recording\u00b6","text":"<p>The recording of the workshop is available on YouTube: https://www.youtube.com/watch?v=pTlIIr-ZS4s</p>"},{"location":"workshops/AIforGood_2025/#introduction-to-sam-and-samgeo","title":"Introduction to SAM and SAMGeo\u00b6","text":"<p>The Segment Anything Model (SAM), introduced by Meta AI in April 2023, represents a significant advancement in computer vision, particularly in the field of image segmentation. Designed as a promptable segmentation model, SAM is capable of generating accurate segmentation masks based on various prompts, such as points, bounding boxes, or textual inputs. A notable feature of SAM is its zero-shot transfer ability, allowing it to adapt to new image distributions and tasks without additional training. This adaptability is largely attributed to its training on the extensive SA-1B dataset, which comprises over 1 billion segmentation masks across 11 million images.</p> <p>Building upon the foundation laid by SAM, Meta AI released Segment Anything Model 2 (SAM 2) in August 2024. SAM 2 extends the capabilities of its predecessor by introducing real-time, promptable object segmentation in both images and videos. This unified model achieves state-of-the-art performance, enabling fast and precise selection of any object in any visual context. Key enhancements in SAM 2 include improved accuracy and processing speed, advanced prompting techniques, and the ability to handle video segmentation tasks seamlessly.</p> <p>Building on the success of SAM and SAM 2, the SAMGeo Python package extends these capabilities to geospatial data. SAMGeo empowers users to perform advanced image segmentation tasks on satellite and aerial imagery, enabling the extraction of valuable insights from geospatial datasets. By leveraging the power of SAMGeo, geospatial professionals can streamline their workflows, enhance data analysis, and unlock new possibilities in remote sensing applications.</p> <p>For more information on SAM and SAMGeo, please check out the slides from here: https://bit.ly/aiforgood-samgeo.</p>"},{"location":"workshops/AIforGood_2025/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/AIforGood_2025/#install-the-required-packages-locally","title":"Install the required packages locally\u00b6","text":"<p>If you are running this notebook locally, you can install the required packages using the following commands:</p> <pre>conda create -n sam python=3.12\nconda activate sam\nconda install -c conda-forge mamba\nmamba install -c conda-forge segment-geospatial groundingdino-py gdal\n</pre>"},{"location":"workshops/AIforGood_2025/#use-google-colab","title":"Use Google Colab\u00b6","text":"<p>If you are using Google Colab, make sure you use GPU runtime for this notebook. Go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator. Then you can run the following cell to install the required packages.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-point-prompts","title":"Image segmentation with point prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using point prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"workshops/AIforGood_2025/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict_by_points()</code> method to segment the image with specified point coordinates. You can use the draw tools to add place markers on the map. If no point is added, the default sample points will be used.</p>"},{"location":"workshops/AIforGood_2025/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/AIforGood_2025/#use-an-existing-vector-dataset-as-point-prompts","title":"Use an existing vector dataset as point prompts\u00b6","text":"<p>Alternatively, you can specify a file path or HTTP URL to a vector dataset containing point geometries.</p>"},{"location":"workshops/AIforGood_2025/#segment-image-with-a-vector-dataset","title":"Segment image with a vector dataset\u00b6","text":"<p>Segment the image using the specified file path to the vector dataset.</p>"},{"location":"workshops/AIforGood_2025/#clean-up-the-result","title":"Clean up the result\u00b6","text":"<p>Remove small objects from the segmented masks, fill holes, and compute geometric properties.</p>"},{"location":"workshops/AIforGood_2025/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"workshops/AIforGood_2025/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Place markers on the map to segment the objects interactively.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-box-prompts","title":"Image segmentation with box prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using box prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map. If no geometry is drawn, the default bounding box will be used.</p>"},{"location":"workshops/AIforGood_2025/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Set <code>automatic=False</code> to enable the <code>SAM2ImagePredictor</code>.</p>"},{"location":"workshops/AIforGood_2025/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/AIforGood_2025/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/AIforGood_2025/#use-an-existing-vector-dataset-as-box-prompts","title":"Use an existing vector dataset as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector dataset. Let's download a sample vector dataset from GitHub.</p>"},{"location":"workshops/AIforGood_2025/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"workshops/AIforGood_2025/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":"<p>Convert the segmented masks to a vector format.</p>"},{"location":"workshops/AIforGood_2025/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":"<p>Regularize the building footprints using the <code>regularize()</code> method.</p>"},{"location":"workshops/AIforGood_2025/#image-segmentation-with-text-prompts","title":"Image segmentation with text prompts\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from remote sensing imagery using text prompts with the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":"<p>Create an interactive map using leafmap.</p>"},{"location":"workshops/AIforGood_2025/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map.</p>"},{"location":"workshops/AIforGood_2025/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/AIforGood_2025/#specify-text-prompts","title":"Specify text prompts\u00b6","text":"<p>Specify the text prompt to segment the objects in the image. The text prompt can be a single word or a phrase that describes the object you want to segment.</p>"},{"location":"workshops/AIforGood_2025/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/AIforGood_2025/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/AIforGood_2025/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"workshops/AIforGood_2025/#timeseries-images-segmentation","title":"Timeseries images segmentation\u00b6","text":""},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>For now, SamGeo2 supports remote sensing data in the form of RGB images, 8-bit integer. Make sure all images are in the same width and height. Let's download a sample timeseries dataset from GitHub.</p>"},{"location":"workshops/AIforGood_2025/#initialize-the-model","title":"Initialize the model\u00b6","text":"<p>Initialize the SamGeo2 class with the model ID and set the <code>video</code> parameter to <code>True</code>.</p>"},{"location":"workshops/AIforGood_2025/#specify-the-input-data","title":"Specify the input data\u00b6","text":"<p>Point to the directory containing the images or the video file.</p>"},{"location":"workshops/AIforGood_2025/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":"<p>The prompts can be points and boxes. The points are represented as a list of tuples, where each tuple contains the x and y coordinates of the point. The boxes are represented as a list of tuples, where each tuple contains the x, y, width, and height of the box.</p>"},{"location":"workshops/AIforGood_2025/#segment-objects","title":"Segment objects\u00b6","text":"<p>Segment the objects from the video or timeseries images.</p>"},{"location":"workshops/AIforGood_2025/#save-results","title":"Save results\u00b6","text":"<p>To save the results as gray-scale GeoTIFFs with the same georeference as the input images:</p>"},{"location":"workshops/AIforGood_2025/#video-segmentation","title":"Video segmentation\u00b6","text":"<p>In this section, we will demonstrate how to segment objects from a video using the Segment Anything Model 2 (SAM 2).</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the required libraries, including leafmap and samgeo.</p>"},{"location":"workshops/AIforGood_2025/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#specify-the-input-data","title":"Specify the input data\u00b6","text":""},{"location":"workshops/AIforGood_2025/#specify-the-input-prompts","title":"Specify the input prompts\u00b6","text":""},{"location":"workshops/AIforGood_2025/#segment-objects","title":"Segment objects\u00b6","text":""},{"location":"workshops/AIforGood_2025/#save-results","title":"Save results\u00b6","text":""},{"location":"workshops/IPPN_2024/","title":"IPPN 2024","text":"<p>Open Source Pipeline for UAS and satellite based High Throughput Phenotyping Applications - Part 1</p> <p>This notebook is designed for workshop presented at the International Plant Phenotyping Network (IPPN) conference on October 7, 2024. Click the Open in Colab button above to run this notebook interactively in the cloud. For Part 2 of the workshop, please click here.</p> <ul> <li>Registration: https://www.plant-phenotyping.org/index.php?index=935</li> <li>Notebook: https://samgeo.gishub.org/workshops/IPPN_2024</li> <li>Earth Engine: https://earthengine.google.com</li> <li>Geemap: https://geemap.org</li> <li>Leafmap: https://leafmap.org</li> <li>Samgeo: https://samgeo.gishub.org</li> <li>Data to Science (D2S): https://ps2.d2s.org</li> <li>D2S Python API: https://py.d2s.org</li> </ul> In\u00a0[\u00a0]: Copied! <pre># %pip install -U \"leafmap[raster]\" segment-geospatial d2spy\n</pre> # %pip install -U \"leafmap[raster]\" segment-geospatial d2spy In\u00a0[\u00a0]: Copied! <pre># %pip install numpy==1.26.4\n</pre> # %pip install numpy==1.26.4 In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n</pre> m = leafmap.Map() <p>To display it in a Jupyter notebook, simply ask for the object representation:</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m <p>To customize the map, you can specify various keyword arguments, such as <code>center</code> ([lat, lon]), <code>zoom</code>, <code>width</code>, and <code>height</code>. The default <code>width</code> is <code>100%</code>, which takes up the entire cell width of the Jupyter notebook. The <code>height</code> argument accepts a number or a string. If a number is provided, it represents the height of the map in pixels. If a string is provided, the string must be in the format of a number followed by <code>px</code>, e.g., <code>600px</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\")\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\") m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(basemap=\"Esri.WorldImagery\")\nm\n</pre> m = leafmap.Map(basemap=\"Esri.WorldImagery\") m <p>You can add as many basemaps as you like to the map. For example, the following code adds the <code>OpenTopoMap</code> basemap to the map above:</p> In\u00a0[\u00a0]: Copied! <pre>m.add_basemap(\"OpenTopoMap\")\n</pre> m.add_basemap(\"OpenTopoMap\") <p>You can also add an XYZ tile layer to the map.</p> In\u00a0[\u00a0]: Copied! <pre>basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\"\nm.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\")\n</pre> basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\" m.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\") <p>You can also change basemaps interactively using the basemap GUI.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap_gui()\nm\n</pre> m = leafmap.Map() m.add_basemap_gui() m In\u00a0[\u00a0]: Copied! <pre>from d2spy.workspace import Workspace\n\n# Replace with URL to a D2S instance\nd2s_url = \"https://ps2.d2s.org\"\n\n# Login and connect to workspace with your email address\nworkspace = Workspace.connect(d2s_url, \"workshop@d2s.org\")\n</pre> from d2spy.workspace import Workspace  # Replace with URL to a D2S instance d2s_url = \"https://ps2.d2s.org\"  # Login and connect to workspace with your email address workspace = Workspace.connect(d2s_url, \"workshop@d2s.org\") In\u00a0[\u00a0]: Copied! <pre># Check for API key\napi_key = workspace.api_key\nif not api_key:\n    print(\n        \"No API key. Please request one from the D2S profile page and re-run this cell.\"\n    )\n</pre> # Check for API key api_key = workspace.api_key if not api_key:     print(         \"No API key. Please request one from the D2S profile page and re-run this cell.\"     ) In\u00a0[\u00a0]: Copied! <pre>import os\nfrom datetime import date\n\nos.environ[\"D2S_API_KEY\"] = api_key\nos.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\"\n</pre> import os from datetime import date  os.environ[\"D2S_API_KEY\"] = api_key os.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\" In\u00a0[\u00a0]: Copied! <pre># Get list of all your projects\nprojects = workspace.get_projects()\nfor project in projects:\n    print(project)\n</pre> # Get list of all your projects projects = workspace.get_projects() for project in projects:     print(project) <p>The <code>projects</code> variable is a <code>ProjectCollection</code>. The collection can be filtered by either the project descriptions or titles using the methods <code>filter_by_title</code> or <code>filter_by_name</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title\nfiltered_projects = projects.filter_by_title(\"Citrus Orchard\")\nprint(filtered_projects)\n</pre> # Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title filtered_projects = projects.filter_by_title(\"Citrus Orchard\") print(filtered_projects) <p>Now you can choose a specific project to work with. In this case, the filtered projects returned only one project, so we will use that project.</p> In\u00a0[\u00a0]: Copied! <pre>project = filtered_projects[0]\n</pre> project = filtered_projects[0] <p><code>get_project_boundary</code> method of the <code>Project</code> class will retrieve a GeoJSON object of the project boundary.</p> In\u00a0[\u00a0]: Copied! <pre># Get project boundary as Python dictionary in GeoJSON structure\nproject_boundary = project.get_project_boundary()\nproject_boundary\n</pre> # Get project boundary as Python dictionary in GeoJSON structure project_boundary = project.get_project_boundary() project_boundary In\u00a0[\u00a0]: Copied! <pre># Get list of all flights for a project\nflights = project.get_flights()\n# Print first flight object (if one exists)\nfor flight in flights:\n    print(flight)\n</pre> # Get list of all flights for a project flights = project.get_flights() # Print first flight object (if one exists) for flight in flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only flights from June 2022\nfiltered_flights = flights.filter_by_date(\n    start_date=date(2022, 6, 1), end_date=date(2022, 7, 1)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> # Example of creating new collection of only flights from June 2022 filtered_flights = flights.filter_by_date(     start_date=date(2022, 6, 1), end_date=date(2022, 7, 1) ) for flight in filtered_flights:     print(flight) <p>Now, we can choose a flight from the filtered flight. Let's choose the flight on June 9, 2022.</p> In\u00a0[\u00a0]: Copied! <pre>flight = filtered_flights[0]\nflight\n</pre> flight = filtered_flights[0] flight In\u00a0[\u00a0]: Copied! <pre># Get list of data products from a flight\ndata_products = flight.get_data_products()\n\nfor data_product in data_products:\n    print(data_product)\n</pre> # Get list of data products from a flight data_products = flight.get_data_products()  for data_product in data_products:     print(data_product) <p>The <code>data_products</code> variable is a <code>DataProductCollection</code>. The collection can be filtered by data type using the method <code>filter_by_data_type</code>. This method will return all data products that match the requested data type.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"ortho\" data type\northo_data_products = data_products.filter_by_data_type(\"ortho\")\nprint(ortho_data_products)\n</pre> # Example of creating new collection of data products with the \"ortho\" data type ortho_data_products = data_products.filter_by_data_type(\"ortho\") print(ortho_data_products) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"HYBRID\", show=False)\northo_data = ortho_data_products[0]\northo_url_202206 = ortho_data.url\northo_url_202206 = leafmap.d2s_tile(ortho_url_202206)\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_basemap(\"HYBRID\", show=False) ortho_data = ortho_data_products[0] ortho_url_202206 = ortho_data.url ortho_url_202206 = leafmap.d2s_tile(ortho_url_202206) m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"dsm\" data type\ndsm_data_products = data_products.filter_by_data_type(\"dsm\")\nprint(dsm_data_products)\n</pre> # Example of creating new collection of data products with the \"dsm\" data type dsm_data_products = data_products.filter_by_data_type(\"dsm\") print(dsm_data_products) In\u00a0[\u00a0]: Copied! <pre>dsm_data = dsm_data_products[0]\ndsm_url_202206 = dsm_data.url\ndsm_url_202206 = leafmap.d2s_tile(dsm_url_202206)\nm.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\")\n</pre> dsm_data = dsm_data_products[0] dsm_url_202206 = dsm_data.url dsm_url_202206 = leafmap.d2s_tile(dsm_url_202206) m.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(dsm_url_202206)\n</pre> leafmap.cog_stats(dsm_url_202206) <p>Add a colorbar to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"chm\" data type\nchm_data_products = data_products.filter_by_data_type(\"chm\")\nprint(chm_data_products)\n</pre> # Example of creating new collection of data products with the \"chm\" data type chm_data_products = data_products.filter_by_data_type(\"chm\") print(chm_data_products) In\u00a0[\u00a0]: Copied! <pre>chm_data = chm_data_products[0]\nchm_url_202206 = chm_data.url\nchm_url_202206 = leafmap.d2s_tile(chm_url_202206)\nm.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\")\n</pre> chm_data = chm_data_products[0] chm_url_202206 = chm_data.url chm_url_202206 = leafmap.d2s_tile(chm_url_202206) m.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(chm_url_202206)\n</pre> leafmap.cog_stats(chm_url_202206) In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\") m <p>Add the project boundary to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(project_boundary, layer_name=\"Project Boundary\")\n</pre> m.add_geojson(project_boundary, layer_name=\"Project Boundary\") <p>Add tree boundaries to the map.</p> In\u00a0[\u00a0]: Copied! <pre>map_layers = project.get_map_layers()\ntree_boundaries = map_layers[0]\n</pre> map_layers = project.get_map_layers() tree_boundaries = map_layers[0] In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\n</pre> m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") In\u00a0[\u00a0]: Copied! <pre>filtered_flights = flights.filter_by_date(\n    start_date=date(2022, 12, 1), end_date=date(2022, 12, 31)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> filtered_flights = flights.filter_by_date(     start_date=date(2022, 12, 1), end_date=date(2022, 12, 31) ) for flight in filtered_flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre>flight_202212 = filtered_flights[0]\ndata_products = flight_202212.get_data_products()\northo_data_products = data_products.filter_by_data_type(\"ortho\")\northo_data = ortho_data_products[0]\northo_url_202212 = ortho_data.url\northo_url_202212 = leafmap.d2s_tile(ortho_url_202212)\n</pre> flight_202212 = filtered_flights[0] data_products = flight_202212.get_data_products() ortho_data_products = data_products.filter_by_data_type(\"ortho\") ortho_data = ortho_data_products[0] ortho_url_202212 = ortho_data.url ortho_url_202212 = leafmap.d2s_tile(ortho_url_202212) In\u00a0[\u00a0]: Copied! <pre>from ipyleaflet import TileLayer\n\nm = leafmap.Map()\nleft_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\"\n)\nright_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\"\n)\nm.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\")\nm.set_center(-97.955281, 26.165595, 18)\nm\n</pre> from ipyleaflet import TileLayer  m = leafmap.Map() left_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\" ) right_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\" ) m.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\") m.set_center(-97.955281, 26.165595, 18) m In\u00a0[\u00a0]: Copied! <pre>import rioxarray as rxr\n</pre> import rioxarray as rxr In\u00a0[\u00a0]: Copied! <pre>data = rxr.open_rasterio(ortho_url_202206)\ndata\n</pre> data = rxr.open_rasterio(ortho_url_202206) data In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m <p>Draw an area of interest (AOI) on the map. If an AOI is not provided, a default AOI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-97.956252, 26.165315, -97.954992, 26.165883] In\u00a0[\u00a0]: Copied! <pre>geojson = leafmap.bbox_to_geojson(bbox)\ngdf = leafmap.geojson_to_gdf(geojson)\nm.add_gdf(gdf, layer_name=\"AOI\", info_mode=None)\n</pre> geojson = leafmap.bbox_to_geojson(bbox) gdf = leafmap.geojson_to_gdf(geojson) m.add_gdf(gdf, layer_name=\"AOI\", info_mode=None) In\u00a0[\u00a0]: Copied! <pre>crs = data.rio.crs.to_string()\nprint(crs)\n</pre> crs = data.rio.crs.to_string() print(crs) In\u00a0[\u00a0]: Copied! <pre>gdf = gdf.to_crs(crs)\nprint(gdf.crs)\n</pre> gdf = gdf.to_crs(crs) print(gdf.crs) <p>Resample the ortho imagery from 1 cm to 10 cm resolution.</p> In\u00a0[\u00a0]: Copied! <pre>resampled_data = data.rio.reproject(crs, resolution=(0.1, 0.1))\nresampled_data.shape\n</pre> resampled_data = data.rio.reproject(crs, resolution=(0.1, 0.1)) resampled_data.shape <p>Clip the ortho image to the AOI.</p> In\u00a0[\u00a0]: Copied! <pre>clipped_data = resampled_data.rio.clip(gdf.geometry, gdf.crs)\nclipped_data.shape\n</pre> clipped_data = resampled_data.rio.clip(gdf.geometry, gdf.crs) clipped_data.shape <p>Save the clipped ortho image to a GeoTIFF file.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"ortho_image_202206.tif\"\nclipped_data.sel(band=[1, 2, 3]).rio.to_raster(image)\n</pre> image = \"ortho_image_202206.tif\" clipped_data.sel(band=[1, 2, 3]).rio.to_raster(image) <p>Read the CHM dataset from D2S as a DataArray.</p> In\u00a0[\u00a0]: Copied! <pre>chm_data = rxr.open_rasterio(chm_url_202206)\nchm_data\n</pre> chm_data = rxr.open_rasterio(chm_url_202206) chm_data In\u00a0[\u00a0]: Copied! <pre>resampled_chm_data = chm_data.rio.reproject_match(resampled_data)\nresampled_chm_data.shape\n</pre> resampled_chm_data = chm_data.rio.reproject_match(resampled_data) resampled_chm_data.shape In\u00a0[\u00a0]: Copied! <pre>clipped_chm_data = resampled_chm_data.rio.clip(gdf.geometry, gdf.crs)\nclipped_chm_data.shape\n</pre> clipped_chm_data = resampled_chm_data.rio.clip(gdf.geometry, gdf.crs) clipped_chm_data.shape In\u00a0[\u00a0]: Copied! <pre>chm_image = \"chm_202206.tif\"\nclipped_chm_data.sel(band=[1]).rio.to_raster(chm_image)\n</pre> chm_image = \"chm_202206.tif\" clipped_chm_data.sel(band=[1]).rio.to_raster(chm_image) <p>Visualize the clipped ortho image.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"Ortho Image 202206\")\nm.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"Ortho Image 202206\") m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo import SamGeo, SamGeo2\n</pre> from samgeo import SamGeo, SamGeo2 In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True)\n</pre> sam2 = SamGeo2(model_id=\"sam2-hiera-large\", automatic=True) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image)\n</pre> sam2.generate(image) In\u00a0[\u00a0]: Copied! <pre>sam2.save_masks(output=\"masks.tif\")\n</pre> sam2.save_masks(output=\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"binary_r\")\n</pre> sam2.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Drone Imagery\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Drone Imagery\",     label2=\"Image Segmentation\", ) <p>Add segmentation result to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7)\nm\n</pre> m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7) m <p>Convert the object masks to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.gpkg\", layer_name=\"Objects\")\n</pre> m.add_vector(\"masks.gpkg\", layer_name=\"Objects\") In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    apply_postprocessing=False,\n    points_per_side=64,\n    points_per_batch=128,\n    pred_iou_thresh=0.7,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.7,\n    crop_n_layers=1,\n    box_nms_thresh=0.7,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=25,\n    use_m2m=True,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     apply_postprocessing=False,     points_per_side=64,     points_per_batch=128,     pred_iou_thresh=0.7,     stability_score_thresh=0.92,     stability_score_offset=0.7,     crop_n_layers=1,     box_nms_thresh=0.7,     crop_n_points_downscale_factor=2,     min_mask_region_area=25,     use_m2m=True, ) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(image, output=\"masks2.tif\")\n</pre> sam2.generate(image, output=\"masks2.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations2.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations2.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Remove small objects.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam2.region_groups(\n    \"masks2.tif\",\n    connectivity=1,\n    min_size=10,\n    max_size=2000,\n    intensity_image=\"chm_202206.tif\",\n    out_image=\"objects.tif\",\n    out_csv=\"objects.csv\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam2.region_groups(     \"masks2.tif\",     connectivity=1,     min_size=10,     max_size=2000,     intensity_image=\"chm_202206.tif\",     out_image=\"objects.tif\",     out_csv=\"objects.csv\",     out_vector=\"objects.gpkg\", ) In\u00a0[\u00a0]: Copied! <pre>gdf = leafmap.geojson_to_gdf(tree_boundaries)\ngdf.head()\n</pre> gdf = leafmap.geojson_to_gdf(tree_boundaries) gdf.head() In\u00a0[\u00a0]: Copied! <pre>geojson = \"tree_boundaries.geojson\"\ngdf.to_file(geojson)\n</pre> geojson = \"tree_boundaries.geojson\" gdf.to_file(geojson) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(image, layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(image, layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\"\n)\n</pre> sam.predict(     boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\" ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\"\n)\nm\n</pre> m.add_raster(     \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\" ) m <p></p>"},{"location":"workshops/IPPN_2024/#introduction","title":"Introduction\u00b6","text":"<p>Recent advances in sensor technology have revolutionized the assessment of crop health by providing fine spatial and high temporal resolutions at affordable costs. As plant scientists gain access to increasingly larger volumes of Unmanned Aerial Systems (UAS) and satellite High Throughput Phenotyping (HTP) data, there is a growing need to extract biologically informative and quantitative phenotypic information from the vast amount of freely available geospatial data. However, the lack of specialized software packages tailored for processing such data makes it challenging to develop transdisciplinary research collaboration around these data. This workshop aims to bridge the gap between big data and agricultural research scientists by providing training on an open-source online platform for managing big UAS HTP data known as Data to Science. Additionally, attendees will be introduced to powerful Python packages, namely leafmap and Leafmap, designed for the seamless integration and analysis of UAS and satellite images in various agricultural applications. By participating in this workshop, attendees will acquire the skills necessary to efficiently search, visualize, and analyze geospatial data within a Jupyter environment, even with minimal coding experience. The workshop provides a hands-on learning experience through practical examples and interactive exercises, enabling participants to enhance their proficiency and gain valuable insights into leveraging geospatial data for agricultural research purposes.</p>"},{"location":"workshops/IPPN_2024/#agenda","title":"Agenda\u00b6","text":"<p>The main topics to be covered in this workshop include:</p> <ul> <li>Create interactive maps using leafmap</li> <li>Visualize drone imagery from D2S</li> <li>Segment drone imagery using samgeo</li> <li>Calculate zonal statistics from drone imagery</li> <li>Visualize Earth Engine data</li> <li>Create timelapse animations</li> </ul>"},{"location":"workshops/IPPN_2024/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/IPPN_2024/#change-colab-dark-theme","title":"Change Colab dark theme\u00b6","text":"<p>Currently, ipywidgets does not work well with Colab dark theme. Some of the leafmap widgets may not display properly in Colab dark theme.It is recommended that you change Colab to the light theme.</p> <p></p>"},{"location":"workshops/IPPN_2024/#change-runtime-type-to-gpu","title":"Change runtime type to GPU\u00b6","text":"<p>To speed up the processing, you can change the Colab runtime type to GPU. Go to the \"Runtime\" menu, select \"Change runtime type\", and choose \"T4 GPU\" from the \"Hardware accelerator\" dropdown menu.</p> <p></p>"},{"location":"workshops/IPPN_2024/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following code to install the required packages.</p>"},{"location":"workshops/IPPN_2024/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the necessary libraries for this workshop.</p>"},{"location":"workshops/IPPN_2024/#creating-interactive-maps","title":"Creating interactive maps\u00b6","text":"<p>Let's create an interactive map using the <code>ipyleaflet</code> plotting backend. The <code>leafmap.Map</code> class inherits the <code>ipyleaflet.Map</code> class. Therefore, you can use the same syntax to create an interactive map as you would with <code>ipyleaflet.Map</code>.</p>"},{"location":"workshops/IPPN_2024/#adding-basemaps","title":"Adding basemaps\u00b6","text":"<p>There are several ways to add basemaps to a map. You can specify the basemap to use in the <code>basemap</code> keyword argument when creating the map. Alternatively, you can add basemap layers to the map using the <code>add_basemap</code> method. leafmap has hundreds of built-in basemaps available that can be easily added to the map with only one line of code.</p> <p>Create a map by specifying the basemap to use as follows. For example, the <code>Esri.WorldImagery</code> basemap represents the Esri world imagery basemap.</p>"},{"location":"workshops/IPPN_2024/#visualizing-drone-imagery-from-d2s","title":"Visualizing Drone Imagery from D2S\u00b6","text":"<p>The Data to Science (D2S) platform (https://ps2.d2s.org) hosts a large collection of drone imagery that can be accessed through the D2S API (https://py.d2s.org). To visualize drone imagery from D2S, you need to sign up for a free account on the D2S platform and obtain an API key.</p>"},{"location":"workshops/IPPN_2024/#login-to-d2s","title":"Login to D2S\u00b6","text":"<p>Login and connect to your D2S workspace in one go using the d2spy.</p>"},{"location":"workshops/IPPN_2024/#choose-a-project-to-work-with","title":"Choose a project to work with\u00b6","text":"<p>The Workspace <code>get_projects</code> method will retrieve a collection of the projects your account can currently access on the D2S instance.</p>"},{"location":"workshops/IPPN_2024/#get-the-project-boundary","title":"Get the project boundary\u00b6","text":""},{"location":"workshops/IPPN_2024/#get-project-flights","title":"Get project flights\u00b6","text":"<p>The <code>Project</code> <code>get_flights</code> method will retrieve a list of flights associated with the project.</p>"},{"location":"workshops/IPPN_2024/#filter-flights-by-date","title":"Filter flights by date\u00b6","text":"<p>The <code>flights</code> variable is a <code>FlightCollection</code>. The collection can be filtered by the acquisition date using the method <code>filter_by_date</code>. This method will return all flights with an acquisition date between the provided start and end dates.</p>"},{"location":"workshops/IPPN_2024/#get-data-products","title":"Get data products\u00b6","text":"<p>The Flight <code>get_data_products</code> method will retrieve a list of data products associated with the flight.</p>"},{"location":"workshops/IPPN_2024/#visualize-ortho-imagery","title":"Visualize ortho imagery\u00b6","text":"<p>Now we can grab the ortho URL to display it using leafmap.</p>"},{"location":"workshops/IPPN_2024/#visualize-dsm","title":"Visualize DSM\u00b6","text":"<p>Similarly, you can visualize the Digital Surface Model (DSM) from D2S using the code below.</p>"},{"location":"workshops/IPPN_2024/#visualize-chm","title":"Visualize CHM\u00b6","text":"<p>Similarly, you can visualize the Canopy Height Model (CHM) from D2S using the code below.</p>"},{"location":"workshops/IPPN_2024/#get-another-flight","title":"Get another flight\u00b6","text":"<p>Retrieve the Ortho data product for the December 2022 flight.</p>"},{"location":"workshops/IPPN_2024/#compare-two-ortho-images","title":"Compare two ortho images\u00b6","text":"<p>Create a split map for comparing the 2022 and 2024 ortho images.</p>"},{"location":"workshops/IPPN_2024/#download-data-from-d2s","title":"Download data from D2S\u00b6","text":"<p>Read the ortho image from D2S as a DataArray.</p>"},{"location":"workshops/IPPN_2024/#segmenting-drone-imagery-using-samgeo","title":"Segmenting Drone Imagery using Samgeo\u00b6","text":""},{"location":"workshops/IPPN_2024/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/IPPN_2024/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/IPPN_2024/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/IPPN_2024/#using-box-prompts","title":"Using box prompts\u00b6","text":""},{"location":"workshops/cn_workshop/","title":"Cn workshop","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/cn_workshop/#samgeo-workshop","title":"SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the \u7b2c\u4e03\u5c4a\u5730\u7403\u7a7a\u95f4\u5927\u6570\u636e\u4e0e\u4e91\u8ba1\u7b97\u524d\u6cbf\u4f1a\u8bae\u4e0e\u96c6\u4e2d\u5b66\u4e60.</p>"},{"location":"workshops/cn_workshop/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/cn_workshop/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/cn_workshop/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/cn_workshop/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/cn_workshop/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/cn_workshop/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/cn_workshop/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/cn_workshop/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/cn_workshop/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"workshops/purdue/","title":"Purdue","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/purdue/#purdue-samgeo-workshop","title":"Purdue SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the Purdue GIS Day 2023.</p> <ul> <li>Slides: https://bit.ly/purdue-samgeo</li> <li>Notebook: https://samgeo.gishub.org/workshops/purdue</li> </ul>"},{"location":"workshops/purdue/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/purdue/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/purdue/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/purdue/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/purdue/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/purdue/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/purdue/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/purdue/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/purdue/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/purdue/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/purdue/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""}]}